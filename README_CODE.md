# NLPæŠ€æœ¯å®ç°ä»£ç åº“

> ğŸš€ **æ¨¡å—åŒ–çš„NLPæŠ€æœ¯å®ç°ï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾åƒã€å›¾ç»“æ„çš„å¤šæ¨¡æ€åˆ†æ**

## ğŸ“– é¡¹ç›®æ¦‚è¿°

æœ¬ä»£ç åº“æä¾›äº†NLPç ”ç©¶ä¸­å¸¸ç”¨æŠ€æœ¯çš„ç»Ÿä¸€å®ç°ï¼Œé‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒå¤šç§æ¨¡å‹å˜ä½“ã€‚ä¸»è¦ç‰¹ç‚¹ï¼š

- ğŸ”„ **ç»Ÿä¸€æ¥å£**ï¼šç›¸ä¼¼æ¶æ„çš„æ¨¡å‹ä½¿ç”¨ç»Ÿä¸€ä»£ç ï¼Œé€šè¿‡é…ç½®åˆ‡æ¢
- ğŸ–¼ï¸ **å¤šæ¨¡æ€æ”¯æŒ**ï¼šæ–‡æœ¬+å›¾åƒ+å›¾ç»“æ„çš„è”åˆå»ºæ¨¡
- ğŸŒ **å¤šè¯­è¨€**ï¼šä¸­è‹±æ–‡åŒè¯­æ”¯æŒ
- ğŸ“Š **å›¾ç¥ç»ç½‘ç»œ**ï¼šç¤¾äº¤ç½‘ç»œåˆ†æå’Œå›¾ç»“æ„å­¦ä¹ 
- ğŸ¤— **é¢„è®­ç»ƒæ¨¡å‹**ï¼šé›†æˆä¸»æµé¢„è®­ç»ƒæ¨¡å‹
- âš¡ **é«˜æ•ˆè®­ç»ƒ**ï¼šæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒå’Œå‚æ•°é«˜æ•ˆå¾®è°ƒ

## ğŸ—‚ï¸ ç›®å½•ç»“æ„

```
nlp-learning-research-guide/
â”œâ”€â”€ README.md                           # é¡¹ç›®æ€»ä½“è¯´æ˜
â”œâ”€â”€ requirements.txt                    # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ environment.yml                     # condaç¯å¢ƒé…ç½®
â”œâ”€â”€ setup.py                           # åŒ…å®‰è£…é…ç½®
â”œâ”€â”€ config/                            # ğŸ”§ é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_configs.yaml             # æ¨¡å‹é…ç½®(æ”¯æŒå¤šç§å˜ä½“)
â”‚   â”œâ”€â”€ training_configs.yaml          # è®­ç»ƒè¶…å‚æ•°é…ç½®
â”‚   â”œâ”€â”€ data_configs.yaml              # æ•°æ®å¤„ç†é…ç½®
â”‚   â””â”€â”€ supported_models.yaml          # æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
â”œâ”€â”€ data/                              # ğŸ“š MR2æ•°æ®é›†ç›®å½•
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ processed/                     # å¤„ç†åçš„æ•°æ®
â”‚   â”‚   â”œâ”€â”€ train_features.pkl
â”‚   â”‚   â”œâ”€â”€ val_features.pkl
â”‚   â”‚   â””â”€â”€ test_features.pkl
â”‚   â”œâ”€â”€ dataset_items_train.json      # è®­ç»ƒé›†æ•°æ®é¡¹
â”‚   â”œâ”€â”€ dataset_items_val.json        # éªŒè¯é›†æ•°æ®é¡¹
â”‚   â”œâ”€â”€ dataset_items_test.json       # æµ‹è¯•é›†æ•°æ®é¡¹
â”‚   â””â”€â”€ README.md                     # æ•°æ®é›†è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ preprocessing/                      # ğŸ“ MR2æ•°æ®é¢„å¤„ç†
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ text_processing.py             # MR2æ–‡æœ¬å¤„ç†(ä¸­è‹±æ–‡åˆ†è¯ã€æ¸…æ´—ç­‰)
â”‚   â”œâ”€â”€ image_processing.py            # MR2å›¾åƒé¢„å¤„ç†å’Œç‰¹å¾æå–
â”‚   â”œâ”€â”€ graph_construction.py          # MR2ç¤¾äº¤å›¾æ„å»ºå’Œå›¾ç‰¹å¾å·¥ç¨‹
â”‚   â””â”€â”€ data_augmentation.py           # MR2æ•°æ®å¢å¼ºç­–ç•¥
â”œâ”€â”€ datasets/                          # ğŸ“š MR2æ•°æ®é›†åŠ è½½
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mr2_dataset.py                 # MR2æ•°æ®é›†PyTorchç±»
â”‚   â””â”€â”€ data_loaders.py                # MR2æ•°æ®åŠ è½½å™¨é…ç½®
â”œâ”€â”€ models/                            # ğŸ¤– æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ traditional/                   # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ml_classifiers.py          # SVM/NB/RF/LRç­‰ç»Ÿä¸€æ¥å£
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py     # ç‰¹å¾å·¥ç¨‹(TF-IDF/N-gram/ç»Ÿè®¡ç‰¹å¾)
â”‚   â”‚   â””â”€â”€ ensemble_methods.py        # é›†æˆå­¦ä¹ æ–¹æ³•
â”‚   â”œâ”€â”€ neural_networks/               # åŸºç¡€ç¥ç»ç½‘ç»œ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ cnn_models.py              # CNNæ¶æ„(TextCNN/ImageCNN)
â”‚   â”‚   â”œâ”€â”€ rnn_models.py              # RNN/LSTM/GRU/BiLSTM
â”‚   â”‚   â”œâ”€â”€ attention_models.py        # æ³¨æ„åŠ›æœºåˆ¶(Self-Attention/Cross-Attention)
â”‚   â”‚   â”œâ”€â”€ transformer_base.py        # åŸºç¡€Transformerå®ç°
â”‚   â”‚   â””â”€â”€ hybrid_models.py           # æ··åˆç¥ç»ç½‘ç»œæ¶æ„
â”‚   â”œâ”€â”€ pretrained/                    # ğŸ¤— é¢„è®­ç»ƒæ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ encoder_models.py          # BERTç±»(BERT/RoBERTa/ALBERT/ELECTRA/DeBERTa)
â”‚   â”‚   â”œâ”€â”€ decoder_models.py          # GPTç±»(GPT/GPT-2/GPT-Neo)
â”‚   â”‚   â”œâ”€â”€ encoder_decoder_models.py  # T5ç±»(T5/mT5/UmT5/BART)
â”‚   â”‚   â”œâ”€â”€ chinese_models.py          # ä¸­æ–‡æ¨¡å‹(Chinese-BERT/MacBERT/ERNIE)
â”‚   â”‚   â”œâ”€â”€ multilingual_models.py     # å¤šè¯­è¨€æ¨¡å‹(mBERT/XLM-R/RemBERT)
â”‚   â”‚   â””â”€â”€ model_adapters.py          # æ¨¡å‹é€‚é…å™¨å’ŒåŒ…è£…å™¨
â”‚   â”œâ”€â”€ multimodal/                    # ğŸ–¼ï¸ğŸ”¤ å¤šæ¨¡æ€æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ vision_language_models.py  # CLIP/BLIP/ALBEF/FLAVAç­‰
â”‚   â”‚   â”œâ”€â”€ fusion_strategies.py       # èåˆæ–¹æ³•(Early/Late/Attentionèåˆ)
â”‚   â”‚   â”œâ”€â”€ chinese_multimodal.py      # ä¸­æ–‡å¤šæ¨¡æ€(Chinese-CLIP/Wenlan)
â”‚   â”‚   â”œâ”€â”€ social_media_models.py     # ç¤¾äº¤åª’ä½“ç‰¹åŒ–æ¨¡å‹
â”‚   â”‚   â””â”€â”€ cross_modal_attention.py   # è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶
â”‚   â”œâ”€â”€ graph_neural_networks/         # ğŸ“Š å›¾ç¥ç»ç½‘ç»œ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ basic_gnn_layers.py        # GCN/GAT/GraphSAGE/GINå±‚
â”‚   â”‚   â”œâ”€â”€ advanced_gnn_models.py     # Graph Transformer/GraphBERT
â”‚   â”‚   â”œâ”€â”€ heterogeneous_gnn.py       # å¼‚æ„å›¾ç¥ç»ç½‘ç»œ(HAN/HGT)
â”‚   â”‚   â”œâ”€â”€ temporal_gnn.py            # æ—¶åºå›¾ç¥ç»ç½‘ç»œ(TGCN/EvolveGCN)
â”‚   â”‚   â”œâ”€â”€ multimodal_gnn.py          # å¤šæ¨¡æ€å›¾ç¥ç»ç½‘ç»œ
â”‚   â”‚   â””â”€â”€ graph_pooling.py           # å›¾æ± åŒ–å±‚(GlobalPool/SAGPool)
â”‚   â””â”€â”€ llms/                          # ğŸš€ å¤§è¯­è¨€æ¨¡å‹
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ open_source_llms.py        # å¼€æºLLM(LLaMA/ChatGLM/Baichuan/Qwen)
â”‚       â”œâ”€â”€ multimodal_llms.py         # å¤šæ¨¡æ€LLM(LLaVA/BLIP-2/InstructBLIP)
â”‚       â”œâ”€â”€ prompt_engineering.py      # æç¤ºå·¥ç¨‹å’Œæ¨¡æ¿è®¾è®¡
â”‚       â””â”€â”€ few_shot_learning.py       # å°‘æ ·æœ¬å­¦ä¹ ç­–ç•¥
â”œâ”€â”€ embeddings/                        # ğŸ“ åµŒå…¥æ–¹æ³•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ word_embeddings.py             # Word2Vec/GloVe/FastText
â”‚   â”œâ”€â”€ sentence_embeddings.py         # SentenceBERT/SimCSE/E5ç­‰
â”‚   â”œâ”€â”€ image_embeddings.py            # ResNet/ViT/CLIPå›¾åƒç‰¹å¾
â”‚   â”œâ”€â”€ multimodal_embeddings.py       # å¤šæ¨¡æ€åµŒå…¥å¯¹é½
â”‚   â””â”€â”€ graph_embeddings.py            # Node2Vec/DeepWalk/GraphSAINT
â”œâ”€â”€ rag/                               # ğŸ” RAGæ£€ç´¢å¢å¼ºç”Ÿæˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ retrievers.py                  # å¯†é›†æ£€ç´¢å™¨(DPR/ColBERT/E5)
â”‚   â”œâ”€â”€ generators.py                  # ç”Ÿæˆå™¨(T5/BART/LLaMA)
â”‚   â”œâ”€â”€ vector_stores.py               # å‘é‡æ•°æ®åº“(Faiss/Chroma/Weaviate)
â”‚   â”œâ”€â”€ multimodal_rag.py              # å¤šæ¨¡æ€RAGç³»ç»Ÿ
â”‚   â””â”€â”€ verification_rag.py            # äº‹å®éªŒè¯RAGç³»ç»Ÿ
â”œâ”€â”€ training/                          # ğŸ‹ï¸ è®­ç»ƒæ¡†æ¶
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_trainer.py                # åŸºç¡€è®­ç»ƒå™¨æŠ½è±¡ç±»
â”‚   â”œâ”€â”€ classification_trainer.py      # åˆ†ç±»ä»»åŠ¡è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ multimodal_trainer.py          # å¤šæ¨¡æ€è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ graph_trainer.py               # å›¾ç¥ç»ç½‘ç»œè®­ç»ƒå™¨
â”‚   â”œâ”€â”€ distributed_training.py        # åˆ†å¸ƒå¼è®­ç»ƒ(DDP/DeepSpeed)
â”‚   â”œâ”€â”€ fine_tuning_methods.py         # å¾®è°ƒæ–¹æ³•(LoRA/AdaLoRA/P-tuning/Prefix)
â”‚   â”œâ”€â”€ loss_functions.py              # æŸå¤±å‡½æ•°(CrossEntropy/Focal/Contrastive)
â”‚   â””â”€â”€ optimization.py                # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦
â”œâ”€â”€ evaluation/                        # ğŸ“Š è¯„ä¼°æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics.py                     # è¯„ä¼°æŒ‡æ ‡(Accuracy/F1/AUC/MAP)
â”‚   â”œâ”€â”€ rumor_metrics.py               # è°£è¨€æ£€æµ‹ä¸“ç”¨æŒ‡æ ‡
â”‚   â”œâ”€â”€ statistical_tests.py           # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
â”‚   â”œâ”€â”€ visualization.py               # ç»“æœå¯è§†åŒ–å’Œåˆ†æ
â”‚   â””â”€â”€ error_analysis.py              # é”™è¯¯åˆ†æå’Œæ¡ˆä¾‹ç ”ç©¶
â”œâ”€â”€ utils/                             # ğŸ› ï¸ å·¥å…·æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_utils.py                 # æ¨¡å‹ç›¸å…³å·¥å…·(ä¿å­˜/åŠ è½½/è½¬æ¢)
â”‚   â”œâ”€â”€ file_utils.py                  # æ–‡ä»¶æ“ä½œ(JSON/CSV/å›¾åƒè¯»å†™)
â”‚   â”œâ”€â”€ logging_utils.py               # æ—¥å¿—é…ç½®å’Œç®¡ç†
â”‚   â”œâ”€â”€ experiment_tracking.py         # å®éªŒè·Ÿè¸ª(WandB/TensorBoard)
â”‚   â”œâ”€â”€ reproducibility.py            # å®éªŒå¯å¤ç°æ€§å·¥å…·
â”‚   â””â”€â”€ visualization_utils.py         # é€šç”¨å¯è§†åŒ–å·¥å…·
â”œâ”€â”€ examples/                          # ğŸ“ ä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ quick_start.py                 # 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹
â”‚   â”œâ”€â”€ text_classification_demo.py    # æ–‡æœ¬åˆ†ç±»å®Œæ•´æµç¨‹
â”‚   â”œâ”€â”€ multimodal_analysis_demo.py    # å¤šæ¨¡æ€åˆ†æç¤ºä¾‹
â”‚   â”œâ”€â”€ graph_analysis_demo.py         # å›¾åˆ†æç¤ºä¾‹
â”‚   â”œâ”€â”€ llm_inference_demo.py          # å¤§æ¨¡å‹æ¨ç†ç¤ºä¾‹
â”‚   â”œâ”€â”€ rag_demo.py                    # RAGç³»ç»Ÿæ¼”ç¤º
â”‚   â””â”€â”€ tutorials/                     # Jupyteræ•™ç¨‹
â”‚       â”œâ”€â”€ 01_getting_started.ipynb
â”‚       â”œâ”€â”€ 02_text_models.ipynb
â”‚       â”œâ”€â”€ 03_multimodal_models.ipynb
â”‚       â”œâ”€â”€ 04_graph_models.ipynb
â”‚       â”œâ”€â”€ 05_advanced_techniques.ipynb
â”‚       â””â”€â”€ 06_llm_and_rag.ipynb
â”œâ”€â”€ tests/                             # ğŸ§ª æµ‹è¯•æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_preprocessing.py          # é¢„å¤„ç†æµ‹è¯•
â”‚   â”œâ”€â”€ test_models.py                 # æ¨¡å‹æµ‹è¯•
â”‚   â”œâ”€â”€ test_training.py               # è®­ç»ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_evaluation.py             # è¯„ä¼°æµ‹è¯•
â”‚   â””â”€â”€ test_utils.py                  # å·¥å…·æµ‹è¯•
â””â”€â”€ scripts/                           # ğŸ“œ æ‰§è¡Œè„šæœ¬
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ setup_environment.py           # ç¯å¢ƒè®¾ç½®è„šæœ¬
    â”œâ”€â”€ download_models.py             # é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½
    â”œâ”€â”€ prepare_dataset.py             # MR2æ•°æ®é›†é¢„å¤„ç†
    â”œâ”€â”€ run_experiments.py             # æ‰¹é‡å®éªŒæ‰§è¡Œ
    â”œâ”€â”€ hyperparameter_search.py       # è¶…å‚æ•°æœç´¢
    â””â”€â”€ model_comparison.py            # æ¨¡å‹æ€§èƒ½å¯¹æ¯”
```

### ğŸ“ æ•°æ®é¢„å¤„ç† (`preprocessing/`)
- `text_processing.py` - æ–‡æœ¬å¤„ç†ï¼ˆåˆ†è¯ã€æ¸…æ´—ã€å¢å¼ºï¼‰
- `image_processing.py` - å›¾åƒé¢„å¤„ç†å’Œç‰¹å¾æå–
- `graph_construction.py` - ç¤¾äº¤å›¾æ„å»ºå’Œå›¾ç‰¹å¾å·¥ç¨‹

### ğŸ¤– æ¨¡å‹å®ç° (`models/`)

#### ä¼ ç»Ÿæ–¹æ³• (`traditional/`)
- `ml_classifiers.py` - æœºå™¨å­¦ä¹ åˆ†ç±»å™¨ç»Ÿä¸€æ¥å£
  ```python
  # æ”¯æŒ: SVM, RandomForest, NaiveBayes, LogisticRegression
  classifier = MLClassifier(method='svm', **params)
  ```

#### é¢„è®­ç»ƒæ¨¡å‹ (`pretrained/`)
- `encoder_models.py` - ç¼–ç å™¨æ¨¡å‹ç»Ÿä¸€æ¥å£
  ```python
  # æ”¯æŒ: bert-base, roberta-base, albert-base, electra-base, deberta-base
  # ä¸­æ–‡: chinese-bert-wwm, chinese-roberta-wwm, macbert
  model = EncoderModel(model_name='bert-base-uncased', num_classes=3)
  ```

- `decoder_models.py` - è§£ç å™¨æ¨¡å‹
  ```python
  # æ”¯æŒ: gpt2, gpt-neo, gpt-j
  model = DecoderModel(model_name='gpt2', task='generation')
  ```

- `encoder_decoder_models.py` - ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹
  ```python
  # æ”¯æŒ: t5-base, mt5-base, umt5-base
  model = EncoderDecoderModel(model_name='t5-base')
  ```

#### å¤šæ¨¡æ€æ¨¡å‹ (`multimodal/`)
- `vision_language_models.py` - è§†è§‰-è¯­è¨€æ¨¡å‹
  ```python
  # æ”¯æŒ: clip, blip, albef, flava
  # ä¸­æ–‡: chinese-clip, wenlan
  model = VisionLanguageModel(model_name='clip', fusion_method='attention')
  ```

#### å›¾ç¥ç»ç½‘ç»œ (`graph_neural_networks/`)
- `basic_gnn_layers.py` - åŸºç¡€GNNå±‚
  ```python
  # æ”¯æŒ: GCN, GAT, GraphSAGE, GIN
  gnn = BasicGNN(layer_type='gcn', hidden_dim=128, num_layers=2)
  ```

- `multimodal_gnn.py` - å¤šæ¨¡æ€å›¾ç¥ç»ç½‘ç»œ
  ```python
  # ç»“åˆæ–‡æœ¬ã€å›¾åƒã€å›¾ç»“æ„
  model = MultimodalGNN(text_encoder='bert', image_encoder='resnet', gnn_type='gat')
  ```

#### å¤§è¯­è¨€æ¨¡å‹ (`llms/`)
- `open_source_llms.py` - å¼€æºå¤§è¯­è¨€æ¨¡å‹
  ```python
  # æ”¯æŒ: llama, llama2, chatglm, chatglm2, baichuan, qwen
  llm = OpenSourceLLM(model_name='chatglm2-6b', task='chat')
  ```

### ğŸ‹ï¸ è®­ç»ƒæ¡†æ¶ (`training/`)
- `base_trainer.py` - ç»Ÿä¸€è®­ç»ƒæ¥å£
- `fine_tuning_methods.py` - å‚æ•°é«˜æ•ˆå¾®è°ƒ
  ```python
  # æ”¯æŒ: LoRA, AdaLoRA, P-Tuning, Prefix-Tuning
  trainer = Trainer(model=model, fine_tuning_method='lora', lora_rank=16)
  ```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè®¾ç½®

```bash
# å…‹éš†ä»“åº“
git clone <repository-url>
cd code

# åˆ›å»ºç¯å¢ƒ
conda env create -f environment.yml
conda activate nlp-toolkit

# æˆ–ä½¿ç”¨pip
pip install -r requirements.txt
```

### åŸºç¡€ä½¿ç”¨

#### 1. æ–‡æœ¬åˆ†ç±»
```python
from models.pretrained.encoder_models import EncoderModel
from datasets.mr2_dataset import MR2Dataset
from training.base_trainer import Trainer

# åŠ è½½æ•°æ®
dataset = MR2Dataset(data_dir='../datasets/MR2')

# åˆ›å»ºæ¨¡å‹ (æ”¯æŒå¤šç§BERTå˜ä½“)
model = EncoderModel(
    model_name='bert-base-uncased',  # å¯é€‰: roberta, albert, electra, deberta
    num_classes=3
)

# è®­ç»ƒ
trainer = Trainer(model=model, dataset=dataset)
trainer.train()
```

#### 2. å¤šæ¨¡æ€åˆ†æ
```python
from models.multimodal.vision_language_models import VisionLanguageModel

# å¤šæ¨¡æ€æ¨¡å‹
model = VisionLanguageModel(
    model_name='clip',  # å¯é€‰: blip, albef, chinese-clip
    fusion_method='attention',
    num_classes=3
)

# å¤„ç†æ–‡æœ¬+å›¾åƒæ•°æ®
results = model(text_inputs, image_inputs)
```

#### 3. å›¾ç¥ç»ç½‘ç»œ
```python
from models.graph_neural_networks.basic_gnn_layers import BasicGNN
from preprocessing.graph_construction import SocialGraphBuilder

# æ„å»ºç¤¾äº¤å›¾
graph_builder = SocialGraphBuilder()
graph = graph_builder.build_user_post_graph(dataset)

# GNNæ¨¡å‹
gnn = BasicGNN(
    layer_type='gat',  # å¯é€‰: gcn, graphsage, gin
    input_dim=768,
    hidden_dim=128,
    num_classes=3
)
```

#### 4. å¤§è¯­è¨€æ¨¡å‹
```python
from models.llms.open_source_llms import OpenSourceLLM

# LLMæ¨ç†
llm = OpenSourceLLM(
    model_name='chatglm2-6b',  # å¯é€‰: llama2, baichuan, qwen
    task='classification'
)

results = llm.predict(texts, prompt_template="åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿ: {text}")
```

## ğŸ“Š æ”¯æŒçš„æ¨¡å‹

### æ–‡æœ¬ç¼–ç å™¨
| æ¨¡å‹ç³»åˆ— | å…·ä½“æ¨¡å‹ | é…ç½®åç§° |
|---------|---------|---------|
| **BERT** | BERT, RoBERTa, ALBERT, ELECTRA, DeBERTa | `bert-base-uncased`, `roberta-base`, `albert-base-v2`, `electra-base`, `deberta-base` |
| **ä¸­æ–‡BERT** | Chinese-BERT-wwm, MacBERT, ERNIE | `chinese-bert-wwm`, `hfl/chinese-macbert-base`, `ernie-1.0` |

### å¤šæ¨¡æ€æ¨¡å‹
| æ¨¡å‹ç³»åˆ— | å…·ä½“æ¨¡å‹ | é…ç½®åç§° |
|---------|---------|---------|
| **CLIP** | CLIP, Chinese-CLIP | `clip-vit-base-patch32`, `chinese-clip-vit-base-patch16` |
| **BLIP** | BLIP, BLIP-2 | `blip-base`, `blip2-opt-2.7b` |

### å›¾ç¥ç»ç½‘ç»œ
| GNNç±»å‹ | å®ç° | ç‰¹ç‚¹ |
|---------|------|------|
| **GCN** | å›¾å·ç§¯ç½‘ç»œ | åŸºç¡€å›¾å·ç§¯ |
| **GAT** | å›¾æ³¨æ„åŠ›ç½‘ç»œ | æ³¨æ„åŠ›æœºåˆ¶ |
| **GraphSAGE** | å›¾é‡‡æ ·èšåˆ | å¤§å›¾æ‰©å±•æ€§ |
| **GIN** | å›¾åŒæ„ç½‘ç»œ | ç†è®ºä¿è¯ |

### å¤§è¯­è¨€æ¨¡å‹
| æ¨¡å‹ç³»åˆ— | å…·ä½“æ¨¡å‹ | å‚æ•°é‡ |
|---------|---------|--------|
| **LLaMA** | LLaMA, LLaMA-2 | 7B-70B |
| **ChatGLM** | ChatGLM, ChatGLM2 | 6B-130B |
| **ç™¾å·** | Baichuan, Baichuan2 | 7B-13B |
| **é€šä¹‰åƒé—®** | Qwen, Qwen-Chat | 7B-72B |

## ğŸ”§ é…ç½®ç®¡ç†

æ‰€æœ‰æ¨¡å‹é…ç½®éƒ½åœ¨ `config/model_configs.yaml` ä¸­ç®¡ç†ï¼š

```yaml
# BERTç³»åˆ—é…ç½®
bert_models:
  bert-base-uncased:
    model_type: "encoder"
    hidden_size: 768
    num_attention_heads: 12
    num_hidden_layers: 12
  
  roberta-base:
    model_type: "encoder" 
    hidden_size: 768
    num_attention_heads: 12
    num_hidden_layers: 12

# å¤šæ¨¡æ€æ¨¡å‹é…ç½®
multimodal_models:
  clip:
    text_encoder: "clip-text"
    image_encoder: "clip-vision"
    projection_dim: 512
```

## ğŸ“š æ•°æ®é›†

å½“å‰æ”¯æŒçš„æ•°æ®é›†ï¼š
- **MR2æ•°æ®é›†**: å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ•°æ®é›†
  - è®­ç»ƒé›†: 500æ¡
  - éªŒè¯é›†: 300æ¡  
  - æµ‹è¯•é›†: 100æ¡
  - åŒ…å«: æ–‡æœ¬ + å›¾åƒ + ç”¨æˆ·ç¤¾äº¤å›¾

## ğŸ§ª å®éªŒç¤ºä¾‹

æŸ¥çœ‹ `examples/` ç›®å½•ä¸‹çš„å®Œæ•´ç¤ºä¾‹ï¼š

- `quick_start.py` - 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹
- `text_classification_demo.py` - æ–‡æœ¬åˆ†ç±»å®Œæ•´æµç¨‹
- `multimodal_analysis_demo.py` - å¤šæ¨¡æ€åˆ†æç¤ºä¾‹
- `graph_analysis_demo.py` - ç¤¾äº¤ç½‘ç»œå›¾åˆ†æ
- `tutorials/` - Jupyteræ•™ç¨‹åˆé›†

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯: `git checkout -b feature/new-model`
3. æäº¤æ›´æ”¹: `git commit -m 'Add new model support'`
4. æ¨é€åˆ†æ”¯: `git push origin feature/new-model`
5. åˆ›å»º Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ“§ è”ç³»æ–¹å¼

- é¡¹ç›®ç»´æŠ¤è€…: [ä½ çš„åå­—]
- é‚®ç®±: [ä½ çš„é‚®ç®±]
- é—®é¢˜åé¦ˆ: [GitHub Issues](issuesé“¾æ¥)

---

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªæ˜Ÿæ ‡ï¼**
```

è¿™æ ·çš„è®¾è®¡æ—¢å‡å°‘äº†ä»£ç å†—ä½™ï¼Œåˆä¿æŒäº†çµæ´»æ€§ã€‚é€šè¿‡ç»Ÿä¸€æ¥å£å’Œé…ç½®æ–‡ä»¶ï¼Œç”¨æˆ·å¯ä»¥è½»æ¾åˆ‡æ¢ä¸åŒçš„æ¨¡å‹å˜ä½“ï¼Œè€Œä¸éœ€è¦ä¿®æ”¹æ ¸å¿ƒä»£ç ã€‚