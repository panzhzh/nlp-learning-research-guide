#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# config/model_configs.yaml

# 模型配置文件 - 完整版本
# 包含所有支持的模型架构配置

# 传统机器学习模型配置
traditional_models:
  svm:
    model_type: "traditional"
    algorithm: "SVM"
    params:
      C: 1.0
      kernel: "rbf"
      gamma: "scale"
      probability: true
    
  random_forest:
    model_type: "traditional"
    algorithm: "RandomForest"
    params:
      n_estimators: 100
      max_depth: null
      min_samples_split: 2
      min_samples_leaf: 1
      random_state: 42
    
  naive_bayes:
    model_type: "traditional"
    algorithm: "MultinomialNB"
    params:
      alpha: 1.0
      fit_prior: true
    
  logistic_regression:
    model_type: "traditional"
    algorithm: "LogisticRegression"
    params:
      C: 1.0
      max_iter: 1000
      random_state: 42

# 神经网络基础模型配置
neural_networks:
  textcnn:
    model_type: "neural"
    architecture: "CNN"
    params:
      embedding_dim: 300
      filter_sizes: [3, 4, 5]
      num_filters: 100
      dropout_rate: 0.5
      num_classes: 3
    
  bilstm:
    model_type: "neural"
    architecture: "BiLSTM"
    params:
      embedding_dim: 300
      hidden_dim: 128
      num_layers: 2
      dropout_rate: 0.5
      num_classes: 3
    
  transformer_base:
    model_type: "neural"
    architecture: "Transformer"
    params:
      d_model: 512
      nhead: 8
      num_encoder_layers: 6
      dim_feedforward: 2048
      dropout_rate: 0.1
      num_classes: 3

# 预训练模型配置
pretrained_models:
  # BERT系列
  bert_base_uncased:
    model_type: "pretrained"
    family: "bert"
    model_name: "bert-base-uncased"
    tokenizer: "bert-base-uncased"
    params:
      hidden_size: 768
      num_hidden_layers: 12
      num_attention_heads: 12
      max_position_embeddings: 512
      vocab_size: 30522
      num_classes: 3
      dropout_rate: 0.1
    
  roberta_base:
    model_type: "pretrained"
    family: "roberta"
    model_name: "roberta-base"
    tokenizer: "roberta-base"
    params:
      hidden_size: 768
      num_hidden_layers: 12
      num_attention_heads: 12
      max_position_embeddings: 514
      vocab_size: 50265
      num_classes: 3
      dropout_rate: 0.1
    
  albert_base_v2:
    model_type: "pretrained"
    family: "albert"
    model_name: "albert-base-v2"
    tokenizer: "albert-base-v2"
    params:
      hidden_size: 768
      num_hidden_layers: 12
      num_attention_heads: 12
      max_position_embeddings: 512
      vocab_size: 30000
      num_classes: 3
      dropout_rate: 0.1

  deberta_base:
    model_type: "pretrained"
    family: "deberta"
    model_name: "microsoft/deberta-base"
    tokenizer: "microsoft/deberta-base"
    params:
      hidden_size: 768
      num_hidden_layers: 12
      num_attention_heads: 12
      max_position_embeddings: 512
      vocab_size: 50265
      num_classes: 3
      dropout_rate: 0.1

  electra_base:
    model_type: "pretrained"
    family: "electra"
    model_name: "google/electra-base-discriminator"
    tokenizer: "google/electra-base-discriminator"
    params:
      hidden_size: 768
      num_hidden_layers: 12
      num_attention_heads: 12
      max_position_embeddings: 512
      vocab_size: 30522
      num_classes: 3
      dropout_rate: 0.1

  # 中文预训练模型
  chinese_bert_wwm:
    model_type: "pretrained"
    family: "bert"
    model_name: "hfl/chinese-bert-wwm-ext"
    tokenizer: "hfl/chinese-bert-wwm-ext"
    params:
      hidden_size: 768
      num_hidden_layers: 12
      num_attention_heads: 12
      max_position_embeddings: 512
      vocab_size: 21128
      num_classes: 3
      dropout_rate: 0.1
    
  chinese_roberta_wwm:
    model_type: "pretrained"
    family: "roberta"
    model_name: "hfl/chinese-roberta-wwm-ext"
    tokenizer: "hfl/chinese-roberta-wwm-ext"
    params:
      hidden_size: 768
      num_hidden_layers: 12
      num_attention_heads: 12
      max_position_embeddings: 512
      vocab_size: 21128
      num_classes: 3
      dropout_rate: 0.1

  macbert_base:
    model_type: "pretrained"
    family: "bert"
    model_name: "hfl/chinese-macbert-base"
    tokenizer: "hfl/chinese-macbert-base"
    params:
      hidden_size: 768
      num_hidden_layers: 12
      num_attention_heads: 12
      max_position_embeddings: 512
      vocab_size: 21128
      num_classes: 3
      dropout_rate: 0.1

  # 多语言模型
  multilingual_bert:
    model_type: "pretrained"
    family: "bert"
    model_name: "bert-base-multilingual-cased"
    tokenizer: "bert-base-multilingual-cased"
    params:
      hidden_size: 768
      num_hidden_layers: 12
      num_attention_heads: 12
      max_position_embeddings: 512
      vocab_size: 119547
      num_classes: 3
      dropout_rate: 0.1

# 多模态模型配置
multimodal_models:
  clip_vit_b32:
    model_type: "multimodal"
    family: "clip"
    model_name: "ViT-B/32"
    params:
      text_embed_dim: 512
      image_embed_dim: 512
      projection_dim: 256
      temperature: 0.07
      num_classes: 3
      fusion_method: "concatenation"
    
  clip_vit_l14:
    model_type: "multimodal" 
    family: "clip"
    model_name: "ViT-L/14"
    params:
      text_embed_dim: 768
      image_embed_dim: 768
      projection_dim: 512
      temperature: 0.07
      num_classes: 3
      fusion_method: "concatenation"
      
  chinese_clip:
    model_type: "multimodal"
    family: "clip"
    model_name: "OFA-Sys/chinese-clip-vit-base-patch16"
    params:
      text_embed_dim: 512
      image_embed_dim: 512
      projection_dim: 512
      temperature: 0.07
      num_classes: 3
      fusion_method: "attention"
    
  simple_multimodal:
    model_type: "multimodal"
    family: "custom"
    model_name: "simple_clip"
    params:
      vocab_size: 30000
      text_embed_dim: 512
      image_embed_dim: 512
      projection_dim: 256
      num_classes: 3
      fusion_method: "concatenation"
    
  blip_base:
    model_type: "multimodal"
    family: "blip"
    model_name: "Salesforce/blip-image-captioning-base"
    params:
      vision_config:
        image_size: 384
        patch_size: 16
        num_channels: 3
      text_config:
        vocab_size: 30524
        hidden_size: 768
        num_hidden_layers: 12
        num_attention_heads: 12
      num_classes: 3
      fusion_method: "cross_attention"

# 图神经网络配置
graph_neural_networks:
  gcn:
    model_type: "graph"
    architecture: "GCN"
    params:
      input_dim: 768
      hidden_dims: [256, 128]
      output_dim: 3
      dropout_rate: 0.5
      activation: "relu"
    
  gat:
    model_type: "graph"
    architecture: "GAT"
    params:
      input_dim: 768
      hidden_dims: [256, 128]
      output_dim: 3
      num_heads: 8
      dropout_rate: 0.5
      activation: "elu"
    
  graphsage:
    model_type: "graph"
    architecture: "GraphSAGE"
    params:
      input_dim: 768
      hidden_dims: [256, 128]
      output_dim: 3
      aggregator: "mean"
      dropout_rate: 0.5

# 大语言模型配置
large_language_models:
  chatglm2_6b:
    model_type: "llm"
    family: "chatglm"
    model_name: "THUDM/chatglm2-6b"
    tokenizer: "THUDM/chatglm2-6b"
    params:
      hidden_size: 4096
      num_layers: 28
      num_attention_heads: 32
      vocab_size: 65024
      max_sequence_length: 32768
      use_cache: true
    
  qwen_7b_chat:
    model_type: "llm"
    family: "qwen"
    model_name: "Qwen/Qwen-7B-Chat"
    tokenizer: "Qwen/Qwen-7B-Chat"
    params:
      hidden_size: 4096
      num_layers: 32
      num_attention_heads: 32
      vocab_size: 151936
      max_sequence_length: 8192
      use_cache: true

# 模型组合配置
ensemble_models:
  voting_classifier:
    model_type: "ensemble"
    method: "voting"
    base_models: ["svm", "random_forest", "logistic_regression"]
    voting: "soft"
    
  stacking_classifier:
    model_type: "ensemble"
    method: "stacking"
    base_models: ["bert_base_uncased", "roberta_base", "chinese_bert_wwm"]
    meta_learner: "logistic_regression"

# 特征提取配置
feature_extraction:
  tfidf:
    vectorizer: "TfidfVectorizer"
    params:
      max_features: 10000
      ngram_range: [1, 3]
      min_df: 2
      max_df: 0.95
      stop_words: null
    
  count_vectorizer:
    vectorizer: "CountVectorizer"
    params:
      max_features: 10000
      ngram_range: [1, 2]
      min_df: 2
      max_df: 0.95
    
  word2vec:
    model: "Word2Vec"
    params:
      vector_size: 300
      window: 5
      min_count: 1
      workers: 4
      epochs: 10

# 模型微调配置
fine_tuning:
  lora:
    method: "LoRA"
    params:
      r: 16
      lora_alpha: 32
      lora_dropout: 0.1
      target_modules: ["query", "value"]
    
  adalora:
    method: "AdaLoRA"
    params:
      r: 16
      lora_alpha: 32
      lora_dropout: 0.1
      target_modules: ["query", "value"]
      beta1: 0.85
      beta2: 0.85
    
  p_tuning_v2:
    method: "P-Tuning-v2"
    params:
      pre_seq_len: 128
      prefix_projection: false
      prefix_hidden_size: 768

# 硬件和计算配置
compute_config:
  device: "auto"  # auto, cpu, cuda, mps
  mixed_precision: true
  gradient_checkpointing: false
  dataloader_num_workers: 4
  pin_memory: true
  
  # 分布式训练
  distributed:
    backend: "nccl"
    init_method: "env://"
    world_size: 1
    rank: 0

# 模型保存和加载配置
model_management:
  save_format: "pytorch"  # pytorch, onnx, torchscript
  checkpoint_dir: "outputs/models/checkpoints"
  best_model_dir: "outputs/models/best"
  model_hub: "huggingface"
  
  # 自动保存配置
  auto_save:
    save_steps: 500
    save_total_limit: 3
    save_best_only: true
    monitor: "val_f1"
    mode: "max"