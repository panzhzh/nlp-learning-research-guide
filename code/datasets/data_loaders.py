#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# datasets/data_loaders.py

"""
MR2æ•°æ®åŠ è½½å™¨é…ç½®æ¨¡å—
æä¾›çµæ´»çš„æ•°æ®åŠ è½½é…ç½®ï¼Œæ”¯æŒï¼š
- å¤šç§æ‰¹å¤„ç†ç­–ç•¥
- è‡ªé€‚åº”æ•°æ®å¢å¼º
- å¤šè¿›ç¨‹æ•°æ®åŠ è½½
- å†…å­˜ä¼˜åŒ–
- å®éªŒé‡ç°æ€§é…ç½®
"""

import torch
from torch.utils.data import DataLoader, WeightedRandomSampler, SubsetRandomSampler
from torch.utils.data.distributed import DistributedSampler
import numpy as np
from typing import Dict, List, Optional, Tuple, Union, Any, Callable
from pathlib import Path
import sys
import json
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
code_root = current_file.parent.parent
sys.path.append(str(code_root))

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
try:
    # å°è¯•å¤šç§å¯¼å…¥æ–¹å¼
    try:
        from datasets.mr2_dataset import MR2Dataset
    except ImportError:
        # å¦‚æœä¸Šé¢å¤±è´¥ï¼Œå°è¯•ç›´æ¥å¯¼å…¥
        import sys
        from pathlib import Path
        current_file = Path(__file__).resolve()
        datasets_dir = current_file.parent
        sys.path.insert(0, str(datasets_dir))
        from mr2_dataset import MR2Dataset
    
    from utils.config_manager import get_training_config, get_data_config
    USE_CUSTOM_MODULES = True
    print("âœ… æˆåŠŸå¯¼å…¥è‡ªå®šä¹‰æ¨¡å—")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—å¤±è´¥: {e}")
    USE_CUSTOM_MODULES = False
    # å®šä¹‰MR2Datasetå ä½ç¬¦
    class MR2Dataset:
        def __init__(self, *args, **kwargs):
            self.items = []
        
        def __len__(self):
            return len(self.items)
        
        def __getitem__(self, idx):
            return {'item_id': 'dummy', 'label': 0}

import logging
logger = logging.getLogger(__name__)


class MR2DataLoaderConfig:
    """
    MR2æ•°æ®åŠ è½½å™¨é…ç½®ç±»
    ç®¡ç†æ•°æ®åŠ è½½çš„å„ç§å‚æ•°å’Œç­–ç•¥
    """
    
    def __init__(self):
        """åˆå§‹åŒ–é…ç½®"""
        self.load_config()
        self.setup_default_config()
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if USE_CUSTOM_MODULES:
            try:
                self.training_config = get_training_config()
                self.data_config = get_data_config()
            except:
                self.training_config = {}
                self.data_config = {}
        else:
            self.training_config = {}
            self.data_config = {}
    
    def setup_default_config(self):
        """è®¾ç½®é»˜è®¤é…ç½®"""
        # é€šç”¨æ•°æ®åŠ è½½é…ç½®
        general_config = self.training_config.get('general', {}).get('data', {})
        
        self.default_config = {
            # æ‰¹æ¬¡é…ç½®
            'train_batch_size': general_config.get('train_batch_size', 32),
            'eval_batch_size': general_config.get('eval_batch_size', 64),
            'test_batch_size': general_config.get('eval_batch_size', 64),
            
            # å·¥ä½œè¿›ç¨‹é…ç½®
            'num_workers': general_config.get('data_workers', 4),
            'pin_memory': general_config.get('pin_memory', True),
            'persistent_workers': general_config.get('persistent_workers', True),
            
            # é‡‡æ ·é…ç½®
            'shuffle_train': True,
            'shuffle_val': False,
            'drop_last': False,
            
            # æ•°æ®å¢å¼ºé…ç½®
            'use_augmentation': True,
            'augmentation_prob': 0.5,
            
            # å†…å­˜ä¼˜åŒ–
            'prefetch_factor': 2,
            'timeout': 0,
            
            # åˆ†å¸ƒå¼è®­ç»ƒ
            'distributed': False,
            'world_size': 1,
            'rank': 0
        }


class AdvancedCollateFunction:
    """
    é«˜çº§æ‰¹å¤„ç†å‡½æ•°
    æ”¯æŒå¤šæ¨¡æ€æ•°æ®çš„æ™ºèƒ½æ‰¹å¤„ç†
    """
    
    def __init__(self, 
                 max_text_length: int = 512,
                 pad_token_id: int = 0,
                 return_attention_mask: bool = True):
        """
        åˆå§‹åŒ–æ‰¹å¤„ç†å‡½æ•°
        
        Args:
            max_text_length: æœ€å¤§æ–‡æœ¬é•¿åº¦
            pad_token_id: å¡«å……token ID
            return_attention_mask: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç 
        """
        self.max_text_length = max_text_length
        self.pad_token_id = pad_token_id
        self.return_attention_mask = return_attention_mask
    
    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ‰¹å¤„ç†å‡½æ•°
        
        Args:
            batch: æ‰¹æ¬¡æ•°æ®åˆ—è¡¨
            
        Returns:
            æ‰¹å¤„ç†åçš„æ•°æ®å­—å…¸
        """
        batch_size = len(batch)
        batch_data = {}
        
        # å¤„ç†åŸºæœ¬å­—æ®µ
        for key in ['item_id', 'label', 'language', 'source', 'has_image']:
            if key in batch[0]:
                batch_data[key] = [item[key] for item in batch]
        
        # å¤„ç†æ•°å€¼å‹æ ‡ç­¾
        if 'label' in batch_data:
            batch_data['labels'] = torch.tensor(batch_data['label'], dtype=torch.long)
        
        # å¤„ç†å›¾åƒæ•°æ®
        if 'image' in batch[0]:
            images = []
            for item in batch:
                if item.get('has_image', False):
                    images.append(item['image'])
                else:
                    # åˆ›å»ºç©ºå›¾åƒtensor
                    images.append(torch.zeros_like(batch[0]['image']))
            
            batch_data['images'] = torch.stack(images)
            batch_data['image_mask'] = torch.tensor([item.get('has_image', False) for item in batch])
        
        # å¤„ç†æ–‡æœ¬æ•°æ®
        if 'text' in batch[0]:
            texts = [item['text'] for item in batch]
            batch_data['texts'] = texts
            
            # å¤„ç†æ–‡æœ¬tokensï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'text_tokens' in batch[0]:
                batch_data['text_tokens'] = [item['text_tokens'] for item in batch]
        
        # å¤„ç†æ–‡æœ¬ç‰¹å¾
        if 'text_features' in batch[0]:
            # æ”¶é›†æ•°å€¼å‹ç‰¹å¾
            feature_keys = []
            if batch[0]['text_features']:
                feature_keys = [k for k, v in batch[0]['text_features'].items() 
                              if isinstance(v, (int, float))]
            
            if feature_keys:
                feature_matrix = []
                for item in batch:
                    features = item.get('text_features', {})
                    feature_vector = [features.get(k, 0) for k in feature_keys]
                    feature_matrix.append(feature_vector)
                
                batch_data['text_feature_matrix'] = torch.tensor(feature_matrix, dtype=torch.float)
                batch_data['text_feature_names'] = feature_keys
        
        # å¤„ç†æ£€ç´¢ä¿¡æ¯
        retrieval_fields = ['has_direct_retrieval', 'has_inverse_retrieval']
        for field in retrieval_fields:
            if field in batch[0]:
                batch_data[field] = torch.tensor([item.get(field, False) for item in batch])
        
        # å¤„ç†å›¾åƒä¿¡æ¯
        if 'image_info' in batch[0]:
            batch_data['image_info'] = [item.get('image_info', {}) for item in batch]
        
        return batch_data


class BalancedBatchSampler:
    """
    å¹³è¡¡æ‰¹æ¬¡é‡‡æ ·å™¨
    ç¡®ä¿æ¯ä¸ªæ‰¹æ¬¡ä¸­å„ç±»åˆ«æ ·æœ¬ç›¸å¯¹å¹³è¡¡
    """
    
    def __init__(self, 
                 dataset: MR2Dataset,
                 batch_size: int,
                 samples_per_class: Optional[int] = None):
        """
        åˆå§‹åŒ–å¹³è¡¡é‡‡æ ·å™¨
        
        Args:
            dataset: æ•°æ®é›†
            batch_size: æ‰¹æ¬¡å¤§å°
            samples_per_class: æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
        """
        self.dataset = dataset
        self.batch_size = batch_size
        self.samples_per_class = samples_per_class
        
        # æ„å»ºæ ‡ç­¾åˆ°ç´¢å¼•çš„æ˜ å°„
        self.label_to_indices = {}
        for idx, item in enumerate(dataset.items):
            label = item.get('label', -1)
            if label not in self.label_to_indices:
                self.label_to_indices[label] = []
            self.label_to_indices[label].append(idx)
        
        self.labels = list(self.label_to_indices.keys())
        self.num_classes = len(self.labels)
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«åœ¨æ‰¹æ¬¡ä¸­çš„æ ·æœ¬æ•°
        if self.samples_per_class is None:
            self.samples_per_class = max(1, batch_size // self.num_classes)
        
        print(f"ğŸ¯ å¹³è¡¡é‡‡æ ·å™¨åˆå§‹åŒ–:")
        print(f"   ç±»åˆ«æ•°: {self.num_classes}")
        print(f"   æ¯ç±»æ ·æœ¬æ•°: {self.samples_per_class}")
        for label in self.labels:
            print(f"   æ ‡ç­¾ {label}: {len(self.label_to_indices[label])} æ ·æœ¬")
    
    def __iter__(self):
        """è¿­ä»£å™¨"""
        while True:
            batch_indices = []
            
            for label in self.labels:
                # éšæœºé€‰æ‹©è¯¥ç±»åˆ«çš„æ ·æœ¬
                available_indices = self.label_to_indices[label]
                if len(available_indices) >= self.samples_per_class:
                    selected_indices = np.random.choice(
                        available_indices, 
                        size=self.samples_per_class, 
                        replace=False
                    )
                else:
                    # å¦‚æœæ ·æœ¬ä¸è¶³ï¼Œä½¿ç”¨é‡å¤é‡‡æ ·
                    selected_indices = np.random.choice(
                        available_indices, 
                        size=self.samples_per_class, 
                        replace=True
                    )
                
                batch_indices.extend(selected_indices)
            
            # éšæœºæ‰“ä¹±æ‰¹æ¬¡å†…çš„é¡ºåº
            np.random.shuffle(batch_indices)
            
            # ç¡®ä¿æ‰¹æ¬¡å¤§å°
            if len(batch_indices) > self.batch_size:
                batch_indices = batch_indices[:self.batch_size]
            elif len(batch_indices) < self.batch_size:
                # éšæœºè¡¥å……æ ·æœ¬
                all_indices = list(range(len(self.dataset)))
                additional_needed = self.batch_size - len(batch_indices)
                additional_indices = np.random.choice(
                    all_indices, 
                    size=additional_needed, 
                    replace=True
                )
                batch_indices.extend(additional_indices)
            
            yield batch_indices
    
    def __len__(self):
        """è¿”å›ä¸€ä¸ªepochçš„æ‰¹æ¬¡æ•°"""
        return len(self.dataset) // self.batch_size


class WeightedSamplerCreator:
    """
    åŠ æƒé‡‡æ ·å™¨åˆ›å»ºå™¨
    æ ¹æ®ç±»åˆ«åˆ†å¸ƒåˆ›å»ºåŠ æƒé‡‡æ ·å™¨
    """
    
    @staticmethod
    def create_weighted_sampler(dataset: MR2Dataset, 
                               sampling_strategy: str = 'inverse_freq') -> WeightedRandomSampler:
        """
        åˆ›å»ºåŠ æƒéšæœºé‡‡æ ·å™¨
        
        Args:
            dataset: æ•°æ®é›†
            sampling_strategy: é‡‡æ ·ç­–ç•¥ ('inverse_freq', 'sqrt_inv_freq', 'balanced')
            
        Returns:
            WeightedRandomSamplerå¯¹è±¡
        """
        # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
        labels = [item.get('label', -1) for item in dataset.items]
        label_counts = Counter(labels)
        
        # è®¡ç®—æƒé‡
        if sampling_strategy == 'inverse_freq':
            # é€†é¢‘ç‡æƒé‡
            total_samples = len(labels)
            weights = {label: total_samples / count for label, count in label_counts.items()}
        
        elif sampling_strategy == 'sqrt_inv_freq':
            # å¹³æ–¹æ ¹é€†é¢‘ç‡æƒé‡
            total_samples = len(labels)
            weights = {label: np.sqrt(total_samples / count) for label, count in label_counts.items()}
        
        elif sampling_strategy == 'balanced':
            # å¹³è¡¡æƒé‡
            max_count = max(label_counts.values())
            weights = {label: max_count / count for label, count in label_counts.items()}
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é‡‡æ ·ç­–ç•¥: {sampling_strategy}")
        
        # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…æƒé‡
        sample_weights = [weights[label] for label in labels]
        
        print(f"ğŸ“Š åŠ æƒé‡‡æ ·å™¨ç»Ÿè®¡:")
        print(f"   é‡‡æ ·ç­–ç•¥: {sampling_strategy}")
        for label, count in label_counts.items():
            weight = weights[label]
            print(f"   æ ‡ç­¾ {label}: {count} æ ·æœ¬, æƒé‡ {weight:.3f}")
        
        return WeightedRandomSampler(
            weights=sample_weights,
            num_samples=len(sample_weights),
            replacement=True
        )


class DataLoaderFactory:
    """
    æ•°æ®åŠ è½½å™¨å·¥å‚ç±»
    æ ¹æ®ä¸åŒéœ€æ±‚åˆ›å»ºåˆé€‚çš„æ•°æ®åŠ è½½å™¨
    """
    
    def __init__(self, config: Optional[MR2DataLoaderConfig] = None):
        """
        åˆå§‹åŒ–å·¥å‚
        
        Args:
            config: æ•°æ®åŠ è½½å™¨é…ç½®
        """
        self.config = config if config is not None else MR2DataLoaderConfig()
    
    def create_basic_dataloader(self,
                               dataset: MR2Dataset,
                               split: str = 'train',
                               batch_size: Optional[int] = None,
                               shuffle: Optional[bool] = None,
                               **kwargs) -> DataLoader:
        """
        åˆ›å»ºåŸºç¡€æ•°æ®åŠ è½½å™¨
        
        Args:
            dataset: æ•°æ®é›†
            split: æ•°æ®åˆ’åˆ†
            batch_size: æ‰¹æ¬¡å¤§å°
            shuffle: æ˜¯å¦æ‰“ä¹±
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            DataLoaderå¯¹è±¡
        """
        # ç¡®å®šæ‰¹æ¬¡å¤§å°
        if batch_size is None:
            if split == 'train':
                batch_size = self.config.default_config['train_batch_size']
            else:
                batch_size = self.config.default_config['eval_batch_size']
        
        # ç¡®å®šæ˜¯å¦æ‰“ä¹±
        if shuffle is None:
            shuffle = split == 'train' and self.config.default_config['shuffle_train']
        
        # åˆ›å»ºæ‰¹å¤„ç†å‡½æ•°
        collate_fn = AdvancedCollateFunction()
        
        # åˆå¹¶é…ç½®
        dataloader_config = {
            'batch_size': batch_size,
            'shuffle': shuffle,
            'num_workers': self.config.default_config['num_workers'],
            'pin_memory': self.config.default_config['pin_memory'],
            'drop_last': self.config.default_config['drop_last'],
            'collate_fn': collate_fn,
            'persistent_workers': self.config.default_config['persistent_workers'] and self.config.default_config['num_workers'] > 0
        }
        
        # æ›´æ–°ç”¨æˆ·æä¾›çš„å‚æ•°
        dataloader_config.update(kwargs)
        
        return DataLoader(dataset, **dataloader_config)
    
    def create_balanced_dataloader(self,
                                  dataset: MR2Dataset,
                                  batch_size: int,
                                  sampling_strategy: str = 'weighted',
                                  **kwargs) -> DataLoader:
        """
        åˆ›å»ºå¹³è¡¡æ•°æ®åŠ è½½å™¨
        
        Args:
            dataset: æ•°æ®é›†
            batch_size: æ‰¹æ¬¡å¤§å°
            sampling_strategy: å¹³è¡¡ç­–ç•¥ ('weighted', 'batch_balanced')
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            DataLoaderå¯¹è±¡
        """
        collate_fn = AdvancedCollateFunction()
        
        if sampling_strategy == 'weighted':
            # ä½¿ç”¨åŠ æƒé‡‡æ ·
            sampler = WeightedSamplerCreator.create_weighted_sampler(dataset)
            
            return DataLoader(
                dataset,
                batch_size=batch_size,
                sampler=sampler,
                num_workers=self.config.default_config['num_workers'],
                pin_memory=self.config.default_config['pin_memory'],
                collate_fn=collate_fn,
                **kwargs
            )
        
        elif sampling_strategy == 'batch_balanced':
            # ä½¿ç”¨æ‰¹æ¬¡å¹³è¡¡é‡‡æ ·
            batch_sampler = BalancedBatchSampler(dataset, batch_size)
            
            return DataLoader(
                dataset,
                batch_sampler=batch_sampler,
                num_workers=self.config.default_config['num_workers'],
                pin_memory=self.config.default_config['pin_memory'],
                collate_fn=collate_fn,
                **kwargs
            )
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¹³è¡¡ç­–ç•¥: {sampling_strategy}")
    
    def create_distributed_dataloader(self,
                                     dataset: MR2Dataset,
                                     batch_size: int,
                                     world_size: int,
                                     rank: int,
                                     **kwargs) -> DataLoader:
        """
        åˆ›å»ºåˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨
        
        Args:
            dataset: æ•°æ®é›†
            batch_size: æ‰¹æ¬¡å¤§å°
            world_size: æ€»è¿›ç¨‹æ•°
            rank: å½“å‰è¿›ç¨‹rank
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            DataLoaderå¯¹è±¡
        """
        # åˆ›å»ºåˆ†å¸ƒå¼é‡‡æ ·å™¨
        sampler = DistributedSampler(
            dataset,
            num_replicas=world_size,
            rank=rank,
            shuffle=True
        )
        
        collate_fn = AdvancedCollateFunction()
        
        return DataLoader(
            dataset,
            batch_size=batch_size,
            sampler=sampler,
            num_workers=self.config.default_config['num_workers'],
            pin_memory=self.config.default_config['pin_memory'],
            collate_fn=collate_fn,
            **kwargs
        )
    
    def create_evaluation_dataloader(self,
                                    dataset: MR2Dataset,
                                    batch_size: Optional[int] = None,
                                    **kwargs) -> DataLoader:
        """
        åˆ›å»ºè¯„ä¼°æ•°æ®åŠ è½½å™¨
        
        Args:
            dataset: æ•°æ®é›†
            batch_size: æ‰¹æ¬¡å¤§å°
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            DataLoaderå¯¹è±¡
        """
        if batch_size is None:
            batch_size = self.config.default_config['eval_batch_size']
        
        return self.create_basic_dataloader(
            dataset=dataset,
            split='eval',
            batch_size=batch_size,
            shuffle=False,
            **kwargs
        )


def create_mr2_dataloaders(data_dir: Union[str, Path],
                          batch_sizes: Optional[Dict[str, int]] = None,
                          sampling_strategy: str = 'basic',
                          num_workers: int = 4,
                          **dataset_kwargs) -> Dict[str, DataLoader]:
    """
    åˆ›å»ºå®Œæ•´çš„MR2æ•°æ®åŠ è½½å™¨é›†åˆ
    
    Args:
        data_dir: æ•°æ®ç›®å½•
        batch_sizes: å„æ•°æ®é›†çš„æ‰¹æ¬¡å¤§å°
        sampling_strategy: é‡‡æ ·ç­–ç•¥
        num_workers: å·¥ä½œè¿›ç¨‹æ•°
        **dataset_kwargs: æ•°æ®é›†å‚æ•°
        
    Returns:
        åŒ…å«train/val/testæ•°æ®åŠ è½½å™¨çš„å­—å…¸
    """
    if batch_sizes is None:
        batch_sizes = {'train': 32, 'val': 64, 'test': 64}
    
    # åˆ›å»ºå·¥å‚å’Œé…ç½®
    config = MR2DataLoaderConfig()
    config.default_config['num_workers'] = num_workers
    factory = DataLoaderFactory(config)
    
    dataloaders = {}
    
    for split in ['train', 'val', 'test']:
        try:
            print(f"\nğŸ“‚ åˆ›å»º {split} æ•°æ®åŠ è½½å™¨...")
            
            # åˆ›å»ºæ•°æ®é›†
            dataset = MR2Dataset(
                data_dir=data_dir,
                split=split,
                transform_type='train' if split == 'train' else 'val',
                **dataset_kwargs
            )
            
            # æ ¹æ®ç­–ç•¥åˆ›å»ºæ•°æ®åŠ è½½å™¨
            batch_size = batch_sizes.get(split, 32)
            
            if split == 'train' and sampling_strategy == 'balanced':
                dataloader = factory.create_balanced_dataloader(
                    dataset=dataset,
                    batch_size=batch_size,
                    sampling_strategy='weighted'
                )
            elif split == 'train' and sampling_strategy == 'batch_balanced':
                dataloader = factory.create_balanced_dataloader(
                    dataset=dataset,
                    batch_size=batch_size,
                    sampling_strategy='batch_balanced'
                )
            else:
                dataloader = factory.create_basic_dataloader(
                    dataset=dataset,
                    split=split,
                    batch_size=batch_size
                )
            
            dataloaders[split] = dataloader
            print(f"âœ… {split} æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ (æ‰¹æ¬¡å¤§å°: {batch_size})")
            
        except Exception as e:
            logger.warning(f"åˆ›å»º {split} æ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
            continue
    
    return dataloaders


def analyze_dataloader_performance(dataloader: DataLoader, 
                                  num_batches: int = 10) -> Dict[str, Any]:
    """
    åˆ†ææ•°æ®åŠ è½½å™¨æ€§èƒ½
    
    Args:
        dataloader: æ•°æ®åŠ è½½å™¨
        num_batches: åˆ†æçš„æ‰¹æ¬¡æ•°
        
    Returns:
        æ€§èƒ½åˆ†æç»“æœ
    """
    import time
    
    print(f"ğŸ” åˆ†ææ•°æ®åŠ è½½å™¨æ€§èƒ½...")
    
    start_time = time.time()
    batch_times = []
    batch_sizes = []
    
    for batch_idx, batch in enumerate(dataloader):
        batch_start = time.time()
        
        # è·å–æ‰¹æ¬¡ä¿¡æ¯
        if isinstance(batch, dict):
            batch_size = len(batch.get('item_id', []))
            has_images = batch.get('images') is not None
            has_texts = batch.get('texts') is not None
        else:
            batch_size = len(batch)
            has_images = False
            has_texts = False
        
        batch_end = time.time()
        batch_time = batch_end - batch_start
        
        batch_times.append(batch_time)
        batch_sizes.append(batch_size)
        
        if batch_idx >= num_batches - 1:
            break
    
    total_time = time.time() - start_time
    
    analysis = {
        'total_time': total_time,
        'avg_batch_time': np.mean(batch_times),
        'std_batch_time': np.std(batch_times),
        'avg_batch_size': np.mean(batch_sizes),
        'total_samples': sum(batch_sizes),
        'samples_per_second': sum(batch_sizes) / total_time,
        'batches_analyzed': len(batch_times)
    }
    
    print(f"ğŸ“Š æ€§èƒ½åˆ†æç»“æœ:")
    print(f"   æ€»æ—¶é—´: {analysis['total_time']:.2f}s")
    print(f"   å¹³å‡æ‰¹æ¬¡æ—¶é—´: {analysis['avg_batch_time']:.3f}s")
    print(f"   å¹³å‡æ‰¹æ¬¡å¤§å°: {analysis['avg_batch_size']:.1f}")
    print(f"   å¤„ç†é€Ÿåº¦: {analysis['samples_per_second']:.1f} æ ·æœ¬/ç§’")
    
    return analysis


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ”„ æµ‹è¯•æ•°æ®åŠ è½½å™¨é…ç½®")
    
    # è®¾ç½®æ•°æ®ç›®å½•
    data_dir = "data"
    
    if not USE_CUSTOM_MODULES:
        print("âš ï¸  è‡ªå®šä¹‰æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        print("âœ… æ•°æ®åŠ è½½å™¨æµ‹è¯•å®Œæˆ")
        exit()
    
    try:
        # æµ‹è¯•åŸºç¡€æ•°æ®åŠ è½½å™¨
        print(f"\nğŸ“¦ === æµ‹è¯•åŸºç¡€æ•°æ®åŠ è½½å™¨ ===")
        
        # åˆ›å»ºå•ä¸ªæ•°æ®åŠ è½½å™¨
        config = MR2DataLoaderConfig()
        factory = DataLoaderFactory(config)
        
        # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
        train_dataset = MR2Dataset(
            data_dir=data_dir,
            split='train',
            transform_type='train',
            use_cache=True
        )
        
        # åŸºç¡€æ•°æ®åŠ è½½å™¨
        basic_loader = factory.create_basic_dataloader(
            dataset=train_dataset,
            split='train',
            batch_size=8
        )
        
        print(f"âœ… åŸºç¡€æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æ‰¹æ¬¡
        print(f"\nğŸ§ª æµ‹è¯•æ‰¹æ¬¡æ•°æ®:")
        for batch_idx, batch in enumerate(basic_loader):
            print(f"   æ‰¹æ¬¡ {batch_idx}:")
            print(f"     æ‰¹æ¬¡å¤§å°: {len(batch['item_id'])}")
            print(f"     å›¾åƒå½¢çŠ¶: {batch['images'].shape}")
            print(f"     æ ‡ç­¾: {batch['labels']}")
            
            if batch_idx >= 2:
                break
        
        print(f"âœ… æ•°æ®åŠ è½½å™¨æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print(f"å»ºè®®ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬: python datasets/simple_mr2_dataset.py")