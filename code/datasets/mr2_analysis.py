#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# code/datasets/mr2_analysis.py

"""
MR2å¤šæ¨¡æ€è°£è¨€æ£€æµ‹æ•°æ®é›†æ·±åº¦åˆ†æ
å…¨é¢åˆ†ææ•°æ®é›†ç»“æ„ã€åˆ†å¸ƒç‰¹å¾ï¼Œç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
"""

import json
import os
import re
import sys
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Any
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®è‹±æ–‡å­—ä½“é¿å…å›¾è¡¨ä¹±ç ï¼Œå…¶ä»–è¾“å‡ºä¿æŒä¸­æ–‡
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

# æ·»åŠ é…ç½®ç®¡ç†å™¨è·¯å¾„
current_file = Path(__file__).resolve()
code_root = current_file.parent.parent
sys.path.append(str(code_root))

try:
    from utils.config_manager import get_config_manager, get_output_path, get_analysis_config, get_label_mapping, get_data_dir
    USE_CONFIG_MANAGER = True
except ImportError:
    print("âš ï¸  é…ç½®ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    USE_CONFIG_MANAGER = False


class MR2DatasetAnalyzer:
    """MR2æ•°æ®é›†åˆ†æå™¨"""
    
    def __init__(self, data_dir: str = 'data', output_dir: str = 'outputs'):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„ (ç›¸å¯¹äºcodeç›®å½•)
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ (ç›¸å¯¹äºcodeç›®å½•)
        """
        if USE_CONFIG_MANAGER:
            # ä½¿ç”¨é…ç½®ç®¡ç†å™¨
            self.config_manager = get_config_manager()
            self.config_manager.create_output_directories()
            
            self.data_dir = get_data_dir()
            self.charts_dir = get_output_path('datasets', 'charts')
            self.reports_dir = get_output_path('datasets', 'reports')
            self.analysis_dir = get_output_path('datasets', 'analysis')
            
            analysis_config = get_analysis_config()
            viz_config = analysis_config.get('visualization', {})
            self.colors = viz_config.get('colors', self._default_colors())
            self.label_mapping = get_label_mapping()
            
            print(f"ğŸ”§ ä½¿ç”¨é…ç½®ç®¡ç†å™¨")
            print(f"ğŸ”§ æ•°æ®ç›®å½•: {self.data_dir}")
            print(f"ğŸ”§ è¾“å‡ºç›®å½•: {self.charts_dir.parent}")
            
        else:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            self.data_dir = data_dir
            self.output_dir = output_dir
            self.charts_dir = Path(output_dir) / 'charts'
            self.reports_dir = Path(output_dir) / 'reports'
            self.analysis_dir = Path(output_dir) / 'analysis'
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            for dir_path in [self.charts_dir, self.reports_dir, self.analysis_dir]:
                dir_path.mkdir(parents=True, exist_ok=True)
            
            self.colors = self._default_colors()
            self.label_mapping = {0: 'Non-rumor', 1: 'Rumor', 2: 'Unverified'}
            
            print(f"ğŸ”§ ä½¿ç”¨é»˜è®¤é…ç½®")
            print(f"ğŸ”§ æ•°æ®ç›®å½•: {self.data_dir}")
            print(f"ğŸ”§ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # å­˜å‚¨åˆ†æç»“æœ
        self.analysis_results = {}
    
    def _default_colors(self):
        """é»˜è®¤é¢œè‰²é…ç½®"""
        return {
            'primary': '#FF6B6B',
            'secondary': '#4ECDC4', 
            'tertiary': '#45B7D1',
            'accent': '#96CEB4',
            'warning': '#FFEAA7',
            'info': '#DDA0DD',
            'success': '#98FB98'
        }
        
    def load_data(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®é›†"""
        print("ğŸ”„ å¼€å§‹åŠ è½½MR2æ•°æ®é›†...")
        
        self.datasets = {}
        splits = ['train', 'val', 'test']
        
        for split in splits:
            file_path = os.path.join(self.data_dir, f'dataset_items_{split}.json')
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    self.datasets[split] = json.load(f)
                print(f"âœ… åŠ è½½ {split} æ•°æ®: {len(self.datasets[split])} æ¡")
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ° {split} æ•°æ®æ–‡ä»¶: {file_path}")
                
        return self.datasets
    
    def basic_statistics(self):
        """åŸºç¡€ç»Ÿè®¡åˆ†æ"""
        print("\nğŸ“Š === åŸºç¡€ç»Ÿè®¡åˆ†æ ===")
        
        stats = {}
        total_samples = 0
        
        for split, data in self.datasets.items():
            split_stats = {
                'total_samples': len(data),
                'label_distribution': Counter(),
                'has_image': 0,
                'has_direct_annotation': 0,
                'has_inverse_annotation': 0
            }
            
            for item_id, item in data.items():
                # æ ‡ç­¾åˆ†å¸ƒ
                label = item.get('label', -1)
                split_stats['label_distribution'][label] += 1
                
                # å›¾åƒæ–‡ä»¶æ£€æŸ¥
                if 'image_path' in item:
                    image_path = os.path.join(self.data_dir, item['image_path'])
                    if os.path.exists(image_path):
                        split_stats['has_image'] += 1
                
                # æ£€ç´¢æ ‡æ³¨æ£€æŸ¥
                if 'direct_path' in item:
                    direct_path = os.path.join(self.data_dir, item['direct_path'], 'direct_annotation.json')
                    if os.path.exists(direct_path):
                        split_stats['has_direct_annotation'] += 1
                        
                if 'inv_path' in item:
                    inv_path = os.path.join(self.data_dir, item['inv_path'], 'inverse_annotation.json')
                    if os.path.exists(inv_path):
                        split_stats['has_inverse_annotation'] += 1
            
            stats[split] = split_stats
            total_samples += split_stats['total_samples']
            
            print(f"\n{split.upper()} æ•°æ®é›†:")
            print(f"  æ ·æœ¬æ€»æ•°: {split_stats['total_samples']}")
            print(f"  æ ‡ç­¾åˆ†å¸ƒ: {dict(split_stats['label_distribution'])}")
            print(f"  æœ‰å›¾åƒ: {split_stats['has_image']}")
            print(f"  æœ‰ç›´æ¥æ£€ç´¢: {split_stats['has_direct_annotation']}")
            print(f"  æœ‰åå‘æ£€ç´¢: {split_stats['has_inverse_annotation']}")
        
        print(f"\næ€»æ ·æœ¬æ•°: {total_samples}")
        self.analysis_results['basic_stats'] = stats
        return stats
    
    def text_analysis(self):
        """æ–‡æœ¬å†…å®¹åˆ†æ"""
        print("\nğŸ“ === æ–‡æœ¬å†…å®¹åˆ†æ ===")
        
        text_stats = {
            'total_texts': 0,
            'length_distribution': [],
            'language_distribution': Counter(),
            'word_count_distribution': [],
            'character_distribution': Counter(),
            'common_words': Counter(),
            'samples_by_length': {'short': [], 'medium': [], 'long': []}
        }
        
        all_texts = []
        
        for split, data in self.datasets.items():
            for item_id, item in data.items():
                caption = item.get('caption', '').strip()
                if caption:
                    all_texts.append(caption)
                    text_stats['total_texts'] += 1
                    
                    # æ–‡æœ¬é•¿åº¦
                    text_length = len(caption)
                    text_stats['length_distribution'].append(text_length)
                    
                    # è¯æ•°ç»Ÿè®¡
                    words = caption.split()
                    word_count = len(words)
                    text_stats['word_count_distribution'].append(word_count)
                    
                    # å¸¸ç”¨è¯ç»Ÿè®¡
                    for word in words:
                        cleaned_word = re.sub(r'[^\w]', '', word.lower())
                        if len(cleaned_word) > 2:  # è¿‡æ»¤çŸ­è¯
                            text_stats['common_words'][cleaned_word] += 1
                    
                    # å­—ç¬¦åˆ†å¸ƒ
                    for char in caption.lower():
                        if char.isalpha():
                            text_stats['character_distribution'][char] += 1
                    
                    # è¯­è¨€æ£€æµ‹ (ç®€å•è§„åˆ™)
                    if re.search(r'[\u4e00-\u9fff]', caption):
                        if re.search(r'[a-zA-Z]', caption):
                            text_stats['language_distribution']['mixed'] += 1
                        else:
                            text_stats['language_distribution']['chinese'] += 1
                    else:
                        text_stats['language_distribution']['english'] += 1
                    
                    # æŒ‰é•¿åº¦åˆ†ç±»æ ·æœ¬
                    if text_length < 30:
                        text_stats['samples_by_length']['short'].append(caption)
                    elif text_length < 100:
                        text_stats['samples_by_length']['medium'].append(caption)
                    else:
                        text_stats['samples_by_length']['long'].append(caption)
        
        # ç»Ÿè®¡æ‘˜è¦
        if text_stats['length_distribution']:
            print(f"æ–‡æœ¬æ€»æ•°: {text_stats['total_texts']}")
            print(f"å¹³å‡é•¿åº¦: {np.mean(text_stats['length_distribution']):.1f} å­—ç¬¦")
            print(f"å¹³å‡è¯æ•°: {np.mean(text_stats['word_count_distribution']):.1f} è¯")
            print(f"è¯­è¨€åˆ†å¸ƒ: {dict(text_stats['language_distribution'])}")
            print(f"æœ€å¸¸è§è¯æ±‡: {text_stats['common_words'].most_common(10)}")
            
            # å±•ç¤ºä¸åŒé•¿åº¦çš„æ ·æœ¬
            print(f"\nçŸ­æ–‡æœ¬æ ·ä¾‹ (<30å­—ç¬¦):")
            for i, text in enumerate(text_stats['samples_by_length']['short'][:5]):
                print(f"  {i+1}. {text}")
            
            print(f"\nä¸­ç­‰æ–‡æœ¬æ ·ä¾‹ (30-100å­—ç¬¦):")
            for i, text in enumerate(text_stats['samples_by_length']['medium'][:5]):
                print(f"  {i+1}. {text}")
                
            print(f"\né•¿æ–‡æœ¬æ ·ä¾‹ (>100å­—ç¬¦):")
            for i, text in enumerate(text_stats['samples_by_length']['long'][:3]):
                print(f"  {i+1}. {text}")
        
        self.analysis_results['text_stats'] = text_stats
        return text_stats
    
    def image_analysis(self):
        """å›¾åƒæ•°æ®åˆ†æ"""
        print("\nğŸ–¼ï¸  === å›¾åƒæ•°æ®åˆ†æ ===")
        
        image_stats = {
            'total_images': 0,
            'valid_images': 0,
            'image_sizes': [],
            'image_formats': Counter(),
            'size_distribution': {'width': [], 'height': []},
            'file_sizes': []
        }
        
        for split, data in self.datasets.items():
            for item_id, item in data.items():
                if 'image_path' in item:
                    image_stats['total_images'] += 1
                    image_path = os.path.join(self.data_dir, item['image_path'])
                    
                    if os.path.exists(image_path):
                        try:
                            # å°è¯•æ‰“å¼€å›¾åƒ
                            with Image.open(image_path) as img:
                                image_stats['valid_images'] += 1
                                
                                # å›¾åƒå°ºå¯¸
                                width, height = img.size
                                image_stats['size_distribution']['width'].append(width)
                                image_stats['size_distribution']['height'].append(height)
                                image_stats['image_sizes'].append((width, height))
                                
                                # å›¾åƒæ ¼å¼
                                image_stats['image_formats'][img.format] += 1
                                
                                # æ–‡ä»¶å¤§å°
                                file_size = os.path.getsize(image_path)
                                image_stats['file_sizes'].append(file_size)
                                
                        except Exception as e:
                            print(f"âš ï¸  æ— æ³•è¯»å–å›¾åƒ {image_path}: {e}")
        
        if image_stats['valid_images'] > 0:
            print(f"å›¾åƒæ€»æ•°: {image_stats['total_images']}")
            print(f"æœ‰æ•ˆå›¾åƒ: {image_stats['valid_images']}")
            print(f"å›¾åƒæ ¼å¼: {dict(image_stats['image_formats'])}")
            print(f"å¹³å‡å°ºå¯¸: {np.mean(image_stats['size_distribution']['width']):.0f} x {np.mean(image_stats['size_distribution']['height']):.0f}")
            print(f"å¹³å‡æ–‡ä»¶å¤§å°: {np.mean(image_stats['file_sizes'])/1024:.1f} KB")
        
        self.analysis_results['image_stats'] = image_stats
        return image_stats
    
    def annotation_analysis(self):
        """æ£€ç´¢æ ‡æ³¨æ•°æ®åˆ†æ"""
        print("\nğŸ” === æ£€ç´¢æ ‡æ³¨åˆ†æ ===")
        
        annotation_stats = {
            'direct_annotations': 0,
            'inverse_annotations': 0,
            'direct_stats': {
                'images_with_captions': [],
                'images_no_captions': [],
                'domains': Counter(),
                'total_retrieved_images': []
            },
            'inverse_stats': {
                'entities_count': [],
                'entity_scores': [],
                'best_guess_labels': [],
                'fully_matched': [],
                'partially_matched': [],
                'common_entities': Counter()
            }
        }
        
        for split, data in self.datasets.items():
            for item_id, item in data.items():
                # ç›´æ¥æ£€ç´¢åˆ†æ
                if 'direct_path' in item:
                    direct_file = os.path.join(self.data_dir, item['direct_path'], 'direct_annotation.json')
                    if os.path.exists(direct_file):
                        try:
                            with open(direct_file, 'r', encoding='utf-8') as f:
                                direct_data = json.load(f)
                                annotation_stats['direct_annotations'] += 1
                                
                                # ç»Ÿè®¡æ£€ç´¢ç»“æœ
                                if 'images_with_captions' in direct_data:
                                    count = len(direct_data['images_with_captions'])
                                    annotation_stats['direct_stats']['images_with_captions'].append(count)
                                    
                                    # åŸŸåç»Ÿè®¡
                                    for img_info in direct_data['images_with_captions']:
                                        domain = img_info.get('domain', 'unknown')
                                        annotation_stats['direct_stats']['domains'][domain] += 1
                                
                                if 'images_with_no_captions' in direct_data:
                                    count = len(direct_data['images_with_no_captions'])
                                    annotation_stats['direct_stats']['images_no_captions'].append(count)
                                
                                # æ€»æ£€ç´¢å›¾åƒæ•°
                                total = len(direct_data.get('images_with_captions', [])) + len(direct_data.get('images_with_no_captions', []))
                                annotation_stats['direct_stats']['total_retrieved_images'].append(total)
                                
                        except Exception as e:
                            print(f"âš ï¸  è¯»å–ç›´æ¥æ£€ç´¢æ–‡ä»¶å¤±è´¥ {direct_file}: {e}")
                
                # åå‘æ£€ç´¢åˆ†æ
                if 'inv_path' in item:
                    inverse_file = os.path.join(self.data_dir, item['inv_path'], 'inverse_annotation.json')
                    if os.path.exists(inverse_file):
                        try:
                            with open(inverse_file, 'r', encoding='utf-8') as f:
                                inverse_data = json.load(f)
                                annotation_stats['inverse_annotations'] += 1
                                
                                # å®ä½“åˆ†æ
                                entities = inverse_data.get('entities', [])
                                annotation_stats['inverse_stats']['entities_count'].append(len(entities))
                                
                                for entity in entities:
                                    annotation_stats['inverse_stats']['common_entities'][entity] += 1
                                
                                # å®ä½“åˆ†æ•°
                                scores = inverse_data.get('entities_scores', [])
                                annotation_stats['inverse_stats']['entity_scores'].extend(scores)
                                
                                # æœ€ä½³çŒœæµ‹æ ‡ç­¾
                                best_guess = inverse_data.get('best_guess_lbl', [])
                                annotation_stats['inverse_stats']['best_guess_labels'].extend(best_guess)
                                
                                # åŒ¹é…ç»“æœç»Ÿè®¡
                                fully_matched = len(inverse_data.get('all_fully_matched_captions', []))
                                partially_matched = len(inverse_data.get('all_partially_matched_captions', []))
                                annotation_stats['inverse_stats']['fully_matched'].append(fully_matched)
                                annotation_stats['inverse_stats']['partially_matched'].append(partially_matched)
                                
                        except Exception as e:
                            print(f"âš ï¸  è¯»å–åå‘æ£€ç´¢æ–‡ä»¶å¤±è´¥ {inverse_file}: {e}")
        
        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        print(f"ç›´æ¥æ£€ç´¢æ ‡æ³¨æ•°: {annotation_stats['direct_annotations']}")
        print(f"åå‘æ£€ç´¢æ ‡æ³¨æ•°: {annotation_stats['inverse_annotations']}")
        
        if annotation_stats['direct_stats']['total_retrieved_images']:
            print(f"å¹³å‡æ£€ç´¢å›¾åƒæ•°: {np.mean(annotation_stats['direct_stats']['total_retrieved_images']):.1f}")
            print(f"çƒ­é—¨åŸŸå: {annotation_stats['direct_stats']['domains'].most_common(5)}")
        
        if annotation_stats['inverse_stats']['entities_count']:
            print(f"å¹³å‡å®ä½“æ•°: {np.mean(annotation_stats['inverse_stats']['entities_count']):.1f}")
            print(f"å¸¸è§å®ä½“: {annotation_stats['inverse_stats']['common_entities'].most_common(10)}")
        
        self.analysis_results['annotation_stats'] = annotation_stats
        return annotation_stats
    
    def create_visualizations(self):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        print("\nğŸ“Š === ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ===")
        
        # è®¾ç½®å›¾è¡¨å‚æ•°
        plt.style.use('default')
        
        # 1. æ•°æ®é›†åŸºç¡€åˆ†å¸ƒå›¾
        self._plot_basic_distribution()
        
        # 2. æ–‡æœ¬é•¿åº¦åˆ†å¸ƒå›¾
        self._plot_text_distribution()
        
        # 3. å›¾åƒå°ºå¯¸åˆ†å¸ƒå›¾
        self._plot_image_distribution()
        
        # 4. æ£€ç´¢ç»“æœåˆ†æå›¾
        self._plot_annotation_analysis()
        
        # 5. ç»¼åˆåˆ†æä»ªè¡¨æ¿
        self._create_dashboard()
        
        print("âœ… æ‰€æœ‰å›¾è¡¨å·²ç”Ÿæˆå®Œæˆ")
    
    def _plot_basic_distribution(self):
        """ç»˜åˆ¶åŸºç¡€æ•°æ®åˆ†å¸ƒ"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('MR2 Dataset Basic Distribution Analysis', fontsize=16, fontweight='bold')
        
        # 1. æ•°æ®é›†å¤§å°åˆ†å¸ƒ
        splits = list(self.analysis_results['basic_stats'].keys())
        sizes = [self.analysis_results['basic_stats'][split]['total_samples'] for split in splits]
        
        colors = [self.colors['primary'], self.colors['secondary'], self.colors['tertiary']]
        axes[0, 0].bar(splits, sizes, color=colors)
        axes[0, 0].set_title('Dataset Size Distribution')
        axes[0, 0].set_ylabel('Number of Samples')
        for i, v in enumerate(sizes):
            axes[0, 0].text(i, v + max(sizes)*0.01, str(v), ha='center', va='bottom')
        
        # 2. æ ‡ç­¾åˆ†å¸ƒ (åˆå¹¶æ‰€æœ‰split)
        all_labels = Counter()
        for split_stats in self.analysis_results['basic_stats'].values():
            all_labels.update(split_stats['label_distribution'])
        
        labels = [self.label_mapping.get(k, f'Unknown({k})') for k in all_labels.keys()]
        counts = list(all_labels.values())
        
        axes[0, 1].pie(counts, labels=labels, autopct='%1.1f%%', colors=colors)
        axes[0, 1].set_title('Label Distribution')
        
        # 3. æ•°æ®å®Œæ•´æ€§åˆ†æ
        completeness_data = []
        for split, stats in self.analysis_results['basic_stats'].items():
            total = stats['total_samples']
            completeness_data.append({
                'Split': split,
                'Has Image': stats['has_image'] / total * 100,
                'Has Direct': stats['has_direct_annotation'] / total * 100,
                'Has Inverse': stats['has_inverse_annotation'] / total * 100
            })
        
        df_completeness = pd.DataFrame(completeness_data)
        x = np.arange(len(df_completeness))
        width = 0.25
        
        axes[1, 0].bar(x - width, df_completeness['Has Image'], width, label='Has Image', color=self.colors['primary'])
        axes[1, 0].bar(x, df_completeness['Has Direct'], width, label='Has Direct Search', color=self.colors['secondary'])
        axes[1, 0].bar(x + width, df_completeness['Has Inverse'], width, label='Has Inverse Search', color=self.colors['tertiary'])
        
        axes[1, 0].set_ylabel('Completeness (%)')
        axes[1, 0].set_title('Data Completeness Analysis')
        axes[1, 0].set_xticks(x)
        axes[1, 0].set_xticklabels(df_completeness['Split'])
        axes[1, 0].legend()
        
        # 4. æŒ‰splitçš„æ ‡ç­¾åˆ†å¸ƒ
        split_label_data = []
        for split, stats in self.analysis_results['basic_stats'].items():
            for label, count in stats['label_distribution'].items():
                split_label_data.append({
                    'Split': split,
                    'Label': self.label_mapping.get(label, f'Unknown({label})'),
                    'Count': count
                })
        
        df_split_labels = pd.DataFrame(split_label_data)
        pivot_df = df_split_labels.pivot(index='Split', columns='Label', values='Count').fillna(0)
        
        pivot_df.plot(kind='bar', ax=axes[1, 1], color=colors)
        axes[1, 1].set_title('Label Distribution by Split')
        axes[1, 1].set_ylabel('Number of Samples')
        axes[1, 1].legend(title='Labels')
        axes[1, 1].tick_params(axis='x', rotation=0)
        
        plt.tight_layout()
        plt.savefig(self.charts_dir / 'basic_distribution.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def _plot_text_distribution(self):
        """ç»˜åˆ¶æ–‡æœ¬åˆ†å¸ƒåˆ†æ"""
        if 'text_stats' not in self.analysis_results:
            return
            
        text_stats = self.analysis_results['text_stats']
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        fig.suptitle('Text Content Analysis', fontsize=16, fontweight='bold')
        
        # 1. æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
        axes[0, 0].hist(text_stats['length_distribution'], bins=30, color=self.colors['primary'], alpha=0.7)
        axes[0, 0].set_title('Text Length Distribution')
        axes[0, 0].set_xlabel('Number of Characters')
        axes[0, 0].set_ylabel('Frequency')
        
        # 2. è¯æ•°åˆ†å¸ƒ
        axes[0, 1].hist(text_stats['word_count_distribution'], bins=20, color=self.colors['secondary'], alpha=0.7)
        axes[0, 1].set_title('Word Count Distribution')
        axes[0, 1].set_xlabel('Number of Words')
        axes[0, 1].set_ylabel('Frequency')
        
        # 3. è¯­è¨€åˆ†å¸ƒ
        lang_data = text_stats['language_distribution']
        colors = [self.colors['primary'], self.colors['secondary'], self.colors['tertiary']]
        axes[0, 2].pie(lang_data.values(), labels=lang_data.keys(), autopct='%1.1f%%', colors=colors)
        axes[0, 2].set_title('Language Distribution')
        
        # 4. å¸¸ç”¨è¯äº‘å½¢å¼çš„æŸ±çŠ¶å›¾
        common_words = text_stats['common_words'].most_common(15)
        if common_words:
            words, counts = zip(*common_words)
            
            axes[1, 0].barh(range(len(words)), counts, color=self.colors['accent'])
            axes[1, 0].set_yticks(range(len(words)))
            axes[1, 0].set_yticklabels(words)
            axes[1, 0].set_title('Most Common Words (Top 15)')
            axes[1, 0].set_xlabel('Frequency')
        
        # 5. å­—ç¬¦é¢‘ç‡åˆ†å¸ƒ
        char_freq = text_stats['character_distribution'].most_common(20)
        if char_freq:
            chars, freqs = zip(*char_freq)
            
            axes[1, 1].bar(range(len(chars)), freqs, color=self.colors['warning'])
            axes[1, 1].set_xticks(range(len(chars)))
            axes[1, 1].set_xticklabels(chars)
            axes[1, 1].set_title('Character Frequency Distribution (Top 20)')
            axes[1, 1].set_ylabel('Frequency')
        
        # 6. æ–‡æœ¬é•¿åº¦ç»Ÿè®¡æ‘˜è¦
        if text_stats['length_distribution']:
            length_stats = {
                'Average Length': np.mean(text_stats['length_distribution']),
                'Median': np.median(text_stats['length_distribution']),
                'Max Length': np.max(text_stats['length_distribution']),
                'Min Length': np.min(text_stats['length_distribution']),
                'Std Dev': np.std(text_stats['length_distribution'])
            }
            
            axes[1, 2].axis('off')
            stats_text = '\n'.join([f'{k}: {v:.1f}' for k, v in length_stats.items()])
            axes[1, 2].text(0.1, 0.5, f'Text Length Statistics:\n\n{stats_text}', 
                           transform=axes[1, 2].transAxes, fontsize=12,
                           verticalalignment='center', bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
        
        plt.tight_layout()
        plt.savefig(self.charts_dir / 'text_distribution.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def _plot_image_distribution(self):
        """ç»˜åˆ¶å›¾åƒåˆ†å¸ƒåˆ†æ"""
        if 'image_stats' not in self.analysis_results:
            return
            
        image_stats = self.analysis_results['image_stats']
        
        if image_stats['valid_images'] == 0:
            print("âš ï¸  æ²¡æœ‰æœ‰æ•ˆå›¾åƒæ•°æ®ï¼Œè·³è¿‡å›¾åƒåˆ†æå›¾è¡¨")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Image Data Analysis', fontsize=16, fontweight='bold')
        
        # 1. å›¾åƒå°ºå¯¸æ•£ç‚¹å›¾
        widths = image_stats['size_distribution']['width']
        heights = image_stats['size_distribution']['height']
        
        axes[0, 0].scatter(widths, heights, alpha=0.6, color=self.colors['primary'])
        axes[0, 0].set_xlabel('Width (pixels)')
        axes[0, 0].set_ylabel('Height (pixels)')
        axes[0, 0].set_title('Image Size Distribution')
        
        # 2. å®½åº¦åˆ†å¸ƒç›´æ–¹å›¾
        axes[0, 1].hist(widths, bins=20, color=self.colors['secondary'], alpha=0.7)
        axes[0, 1].set_xlabel('Width (pixels)')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].set_title('Image Width Distribution')
        
        # 3. é«˜åº¦åˆ†å¸ƒç›´æ–¹å›¾
        axes[1, 0].hist(heights, bins=20, color=self.colors['tertiary'], alpha=0.7)
        axes[1, 0].set_xlabel('Height (pixels)')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].set_title('Image Height Distribution')
        
        # 4. å›¾åƒæ ¼å¼åˆ†å¸ƒ
        format_data = image_stats['image_formats']
        if format_data:
            colors = [self.colors['primary'], self.colors['secondary'], self.colors['tertiary'], self.colors['accent']]
            axes[1, 1].pie(format_data.values(), labels=format_data.keys(), autopct='%1.1f%%', colors=colors)
            axes[1, 1].set_title('Image Format Distribution')
        
        plt.tight_layout()
        plt.savefig(self.charts_dir / 'image_distribution.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def _plot_annotation_analysis(self):
        """ç»˜åˆ¶æ£€ç´¢æ ‡æ³¨åˆ†æ"""
        if 'annotation_stats' not in self.analysis_results:
            return
            
        annotation_stats = self.analysis_results['annotation_stats']
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        fig.suptitle('Retrieval Annotation Analysis', fontsize=16, fontweight='bold')
        
        # 1. æ£€ç´¢æ•°é‡å¯¹æ¯”
        annotation_counts = [
            annotation_stats['direct_annotations'],
            annotation_stats['inverse_annotations']
        ]
        
        if max(annotation_counts) > 0:
            axes[0, 0].bar(['Direct Search', 'Inverse Search'], annotation_counts, 
                          color=[self.colors['primary'], self.colors['secondary']])
            axes[0, 0].set_title('Annotation Count')
            axes[0, 0].set_ylabel('Number of Annotations')
            for i, v in enumerate(annotation_counts):
                axes[0, 0].text(i, v + max(annotation_counts)*0.01, str(v), ha='center', va='bottom')
        
        # 2. ç›´æ¥æ£€ç´¢å›¾åƒæ•°é‡åˆ†å¸ƒ
        if annotation_stats['direct_stats']['total_retrieved_images']:
            axes[0, 1].hist(annotation_stats['direct_stats']['total_retrieved_images'], 
                           bins=15, color=self.colors['tertiary'], alpha=0.7)
            axes[0, 1].set_title('Direct Search Image Count Distribution')
            axes[0, 1].set_xlabel('Number of Retrieved Images')
            axes[0, 1].set_ylabel('Frequency')
        
        # 3. çƒ­é—¨åŸŸååˆ†å¸ƒ
        if annotation_stats['direct_stats']['domains']:
            top_domains = annotation_stats['direct_stats']['domains'].most_common(10)
            domains, counts = zip(*top_domains)
            
            axes[0, 2].barh(range(len(domains)), counts, color=self.colors['accent'])
            axes[0, 2].set_yticks(range(len(domains)))
            axes[0, 2].set_yticklabels(domains)
            axes[0, 2].set_title('Top Retrieval Domains (Top 10)')
            axes[0, 2].set_xlabel('Frequency')
        
        # 4. åå‘æ£€ç´¢å®ä½“æ•°é‡åˆ†å¸ƒ
        if annotation_stats['inverse_stats']['entities_count']:
            axes[1, 0].hist(annotation_stats['inverse_stats']['entities_count'],
                           bins=15, color=self.colors['warning'], alpha=0.7)
            axes[1, 0].set_title('Inverse Search Entity Count Distribution')
            axes[1, 0].set_xlabel('Number of Entities')
            axes[1, 0].set_ylabel('Frequency')
        
        # 5. å®ä½“ç½®ä¿¡åº¦åˆ†å¸ƒ
        if annotation_stats['inverse_stats']['entity_scores']:
            axes[1, 1].hist(annotation_stats['inverse_stats']['entity_scores'],
                           bins=20, color=self.colors['info'], alpha=0.7)
            axes[1, 1].set_title('Entity Confidence Score Distribution')
            axes[1, 1].set_xlabel('Confidence Score')
            axes[1, 1].set_ylabel('Frequency')
        
        # 6. å¸¸è§å®ä½“è¯äº‘
        if annotation_stats['inverse_stats']['common_entities']:
            top_entities = annotation_stats['inverse_stats']['common_entities'].most_common(15)
            entities, counts = zip(*top_entities)
            
            axes[1, 2].barh(range(len(entities)), counts, color=self.colors['success'])
            axes[1, 2].set_yticks(range(len(entities)))
            axes[1, 2].set_yticklabels(entities)
            axes[1, 2].set_title('Common Detected Entities (Top 15)')
            axes[1, 2].set_xlabel('Frequency')
        
        plt.tight_layout()
        plt.savefig(self.charts_dir / 'annotation_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def _create_dashboard(self):
        """åˆ›å»ºç»¼åˆåˆ†æä»ªè¡¨æ¿"""
        fig = plt.figure(figsize=(20, 12))
        fig.suptitle('MR2 Dataset Comprehensive Analysis Dashboard', fontsize=20, fontweight='bold')
        
        # åˆ›å»ºç½‘æ ¼å¸ƒå±€
        gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)
        
        # 1. æ•°æ®é›†æ¦‚è§ˆ
        ax1 = fig.add_subplot(gs[0, :2])
        stats_summary = []
        for split, stats in self.analysis_results['basic_stats'].items():
            total = stats['total_samples']
            stats_summary.append({
                'Dataset': split.upper(),
                'Samples': total,
                'Rumor': stats['label_distribution'].get(1, 0),
                'Non-rumor': stats['label_distribution'].get(0, 0),
                'Unverified': stats['label_distribution'].get(2, 0),
                'Images': stats['has_image'],
                'Direct': stats['has_direct_annotation'],
                'Inverse': stats['has_inverse_annotation']
            })
        
        df_summary = pd.DataFrame(stats_summary)
        colors = list(self.colors.values())[:7]
        df_summary.set_index('Dataset').plot(kind='bar', ax=ax1, color=colors)
        ax1.set_title('Dataset Detailed Statistics', fontweight='bold')
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax1.tick_params(axis='x', rotation=0)
        
        # 2. æ–‡æœ¬é•¿åº¦vsæ ‡ç­¾å…³ç³»
        ax2 = fig.add_subplot(gs[0, 2:])
        if 'text_stats' in self.analysis_results:
            # æŒ‰æ ‡ç­¾åˆ†ç»„çš„æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
            text_by_label = {0: [], 1: [], 2: []}
            for split, data in self.datasets.items():
                for item_id, item in data.items():
                    caption = item.get('caption', '')
                    label = item.get('label', -1)
                    if caption and label in text_by_label:
                        text_by_label[label].append(len(caption))
            
            colors = [self.colors['primary'], self.colors['secondary'], self.colors['tertiary']]
            for label, lengths in text_by_label.items():
                if lengths:
                    ax2.hist(lengths, bins=20, alpha=0.6, 
                            label=self.label_mapping.get(label, f'Label {label}'),
                            color=colors[label % len(colors)])
            
            ax2.set_title('Text Length Distribution by Label', fontweight='bold')
            ax2.set_xlabel('Text Length (characters)')
            ax2.set_ylabel('Frequency')
            ax2.legend()
        
        # 3. å›¾åƒå°ºå¯¸èšç±»åˆ†æ
        ax3 = fig.add_subplot(gs[1, :2])
        if 'image_stats' in self.analysis_results and self.analysis_results['image_stats']['valid_images'] > 0:
            widths = self.analysis_results['image_stats']['size_distribution']['width']
            heights = self.analysis_results['image_stats']['size_distribution']['height']
            
            # è®¡ç®—å®½é«˜æ¯”
            aspect_ratios = [w/h for w, h in zip(widths, heights)]
            
            scatter = ax3.scatter(widths, heights, c=aspect_ratios, cmap='viridis', alpha=0.6)
            ax3.set_xlabel('Width (pixels)')
            ax3.set_ylabel('Height (pixels)')
            ax3.set_title('Image Size Clustering (color = aspect ratio)', fontweight='bold')
            
            # æ·»åŠ é¢œè‰²æ¡
            plt.colorbar(scatter, ax=ax3, label='Aspect Ratio')
        
        # 4. æ£€ç´¢æ•ˆæœåˆ†æ
        ax4 = fig.add_subplot(gs[1, 2:])
        if 'annotation_stats' in self.analysis_results:
            retrieval_data = {
                'Direct Search Coverage': self.analysis_results['annotation_stats']['direct_annotations'],
                'Inverse Search Coverage': self.analysis_results['annotation_stats']['inverse_annotations']
            }
            
            # è®¡ç®—æ€»æ ·æœ¬æ•°
            total_samples = sum(stats['total_samples'] for stats in self.analysis_results['basic_stats'].values())
            
            coverage_rates = [v/total_samples*100 for v in retrieval_data.values()]
            
            bars = ax4.bar(retrieval_data.keys(), coverage_rates, 
                          color=[self.colors['primary'], self.colors['secondary']])
            ax4.set_title('Retrieval Annotation Coverage Rate', fontweight='bold')
            ax4.set_ylabel('Coverage Rate (%)')
            
            for i, v in enumerate(coverage_rates):
                ax4.text(i, v + max(coverage_rates)*0.01, f'{v:.1f}%', ha='center', va='bottom')
        
        # 5. è¯­è¨€åˆ†å¸ƒé¥¼å›¾
        ax5 = fig.add_subplot(gs[2, 0])
        if 'text_stats' in self.analysis_results:
            lang_data = self.analysis_results['text_stats']['language_distribution']
            if lang_data:
                colors = [self.colors['primary'], self.colors['secondary'], self.colors['tertiary']]
                ax5.pie(lang_data.values(), labels=lang_data.keys(), autopct='%1.1f%%', colors=colors)
                ax5.set_title('Text Language Distribution', fontweight='bold')
        
        # 6. æ ‡ç­¾ä¸å¹³è¡¡åˆ†æ
        ax6 = fig.add_subplot(gs[2, 1])
        all_labels = Counter()
        for split_stats in self.analysis_results['basic_stats'].values():
            all_labels.update(split_stats['label_distribution'])
        
        if all_labels:
            labels = [self.label_mapping.get(k, f'Label {k}') for k in all_labels.keys()]
            counts = list(all_labels.values())
            
            colors = [self.colors['primary'], self.colors['secondary'], self.colors['tertiary']]
            bars = ax6.bar(labels, counts, color=colors)
            ax6.set_title('Label Imbalance Analysis', fontweight='bold')
            ax6.set_ylabel('Number of Samples')
            ax6.tick_params(axis='x', rotation=45)
            
            # æ·»åŠ ä¸å¹³è¡¡æ¯”ä¾‹
            max_count = max(counts)
            for i, (label, count) in enumerate(zip(labels, counts)):
                ratio = count / max_count
                ax6.text(i, count + max_count*0.01, f'{ratio:.2f}', ha='center', va='bottom')
        
        # 7. æ•°æ®è´¨é‡è¯„ä¼°
        ax7 = fig.add_subplot(gs[2, 2])
        quality_metrics = []
        for split, stats in self.analysis_results['basic_stats'].items():
            total = stats['total_samples']
            quality_score = (
                stats['has_image'] / total * 0.4 +
                stats['has_direct_annotation'] / total * 0.3 +
                stats['has_inverse_annotation'] / total * 0.3
            ) * 100
            quality_metrics.append(quality_score)
        
        splits = list(self.analysis_results['basic_stats'].keys())
        colors = [self.colors['accent'], self.colors['warning'], self.colors['info']]
        bars = ax7.bar(splits, quality_metrics, color=colors)
        ax7.set_title('Data Quality Score', fontweight='bold')
        ax7.set_ylabel('Quality Score (%)')
        
        for bar, score in zip(bars, quality_metrics):
            height = bar.get_height()
            ax7.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{score:.1f}%', ha='center', va='bottom')
        
        # 8. å…³é”®ç»Ÿè®¡æŒ‡æ ‡
        ax8 = fig.add_subplot(gs[2, 3])
        ax8.axis('off')
        
        # è®¡ç®—å…³é”®æŒ‡æ ‡
        total_samples = sum(stats['total_samples'] for stats in self.analysis_results['basic_stats'].values())
        total_images = sum(stats['has_image'] for stats in self.analysis_results['basic_stats'].values())
        total_annotations = self.analysis_results.get('annotation_stats', {}).get('direct_annotations', 0)
        
        avg_text_length = 0
        if 'text_stats' in self.analysis_results:
            avg_text_length = np.mean(self.analysis_results['text_stats']['length_distribution'])
        
        key_metrics = f"""
Key Metrics Summary:

ğŸ“Š Total Samples: {total_samples:,}
ğŸ–¼ï¸ Total Images: {total_images:,}
ğŸ” Total Annotations: {total_annotations:,}
ğŸ“ Avg Text Length: {avg_text_length:.1f} chars

ğŸ’¡ Dataset Features:
â€¢ Multi-modal: Text+Image+Retrieval
â€¢ Multi-lingual: Chinese+English mix
â€¢ Multi-source retrieval: Direct+Inverse
â€¢ Three-class: Rumor detection task
        """
        
        ax8.text(0.05, 0.95, key_metrics, transform=ax8.transAxes, fontsize=11,
                verticalalignment='top', bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.7))
        
        plt.savefig(self.charts_dir / 'comprehensive_dashboard.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_report(self):
        """ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š"""
        print("\nğŸ“„ === ç”Ÿæˆåˆ†ææŠ¥å‘Š ===")
        
        report_content = []
        report_content.append("# MR2å¤šæ¨¡æ€è°£è¨€æ£€æµ‹æ•°æ®é›†åˆ†ææŠ¥å‘Š\n")
        report_content.append(f"**ç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        report_content.append("---\n")
        
        # 1. æ‰§è¡Œæ‘˜è¦
        total_samples = sum(stats['total_samples'] for stats in self.analysis_results['basic_stats'].values())
        
        report_content.append("## æ‰§è¡Œæ‘˜è¦\n")
        report_content.append(f"MR2æ•°æ®é›†åŒ…å« **{total_samples:,}** ä¸ªå¤šæ¨¡æ€æ ·æœ¬ï¼Œä¸“é—¨ç”¨äºè°£è¨€æ£€æµ‹ä»»åŠ¡ã€‚")
        report_content.append("æ•°æ®é›†å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š\n")
        report_content.append("- ğŸ”„ **å¤šæ¨¡æ€æ•°æ®**: ç»“åˆæ–‡æœ¬ã€å›¾åƒå’Œæ£€ç´¢ä¿¡æ¯")
        report_content.append("- ğŸŒ **å¤šè¯­è¨€æ”¯æŒ**: ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬")
        report_content.append("- ğŸ” **ä¸°å¯Œæ ‡æ³¨**: ç›´æ¥æ£€ç´¢å’Œåå‘å›¾åƒæœç´¢")
        report_content.append("- ğŸ“Š **ä¸‰åˆ†ç±»ä»»åŠ¡**: è°£è¨€ã€éè°£è¨€ã€æœªéªŒè¯\n")
        
        # 2. æ•°æ®é›†ç»Ÿè®¡
        report_content.append("## æ•°æ®é›†ç»Ÿè®¡\n")
        report_content.append("### æ•°æ®é‡åˆ†å¸ƒ\n")
        for split, stats in self.analysis_results['basic_stats'].items():
            report_content.append(f"- **{split.upper()}**: {stats['total_samples']:,} æ ·æœ¬")
        
        report_content.append("\n### æ ‡ç­¾åˆ†å¸ƒ\n")
        all_labels = Counter()
        for split_stats in self.analysis_results['basic_stats'].values():
            all_labels.update(split_stats['label_distribution'])
        
        for label, count in all_labels.items():
            label_name = self.label_mapping.get(label, f'Unknown({label})')
            percentage = count / total_samples * 100
            report_content.append(f"- **{label_name}**: {count:,} ({percentage:.1f}%)")
        
        # 3. æ–‡æœ¬åˆ†æç»“æœ
        if 'text_stats' in self.analysis_results:
            text_stats = self.analysis_results['text_stats']
            report_content.append("\n## æ–‡æœ¬å†…å®¹åˆ†æ\n")
            report_content.append(f"- **æ–‡æœ¬æ€»æ•°**: {text_stats['total_texts']:,}")
            report_content.append(f"- **å¹³å‡é•¿åº¦**: {np.mean(text_stats['length_distribution']):.1f} å­—ç¬¦")
            report_content.append(f"- **å¹³å‡è¯æ•°**: {np.mean(text_stats['word_count_distribution']):.1f} è¯")
            
            report_content.append("\n### è¯­è¨€åˆ†å¸ƒ")
            for lang, count in text_stats['language_distribution'].items():
                percentage = count / text_stats['total_texts'] * 100
                report_content.append(f"- **{lang}**: {count:,} ({percentage:.1f}%)")
            
            report_content.append("\n### å¸¸è§è¯æ±‡ (Top 10)")
            for word, count in text_stats['common_words'].most_common(10):
                report_content.append(f"- **{word}**: {count} æ¬¡")
        
        # 4. å›¾åƒåˆ†æç»“æœ
        if 'image_stats' in self.analysis_results:
            image_stats = self.analysis_results['image_stats']
            report_content.append("\n## å›¾åƒæ•°æ®åˆ†æ\n")
            report_content.append(f"- **å›¾åƒæ€»æ•°**: {image_stats['total_images']:,}")
            report_content.append(f"- **æœ‰æ•ˆå›¾åƒ**: {image_stats['valid_images']:,}")
            
            if image_stats['valid_images'] > 0:
                report_content.append(f"- **å¹³å‡å°ºå¯¸**: {np.mean(image_stats['size_distribution']['width']):.0f} Ã— {np.mean(image_stats['size_distribution']['height']):.0f} åƒç´ ")
                report_content.append(f"- **å¹³å‡æ–‡ä»¶å¤§å°**: {np.mean(image_stats['file_sizes'])/1024:.1f} KB")
                
                report_content.append("\n### å›¾åƒæ ¼å¼åˆ†å¸ƒ")
                for format_type, count in image_stats['image_formats'].items():
                    percentage = count / image_stats['valid_images'] * 100
                    report_content.append(f"- **{format_type}**: {count:,} ({percentage:.1f}%)")
        
        # 5. æ£€ç´¢æ ‡æ³¨åˆ†æ
        if 'annotation_stats' in self.analysis_results:
            annotation_stats = self.analysis_results['annotation_stats']
            report_content.append("\n## æ£€ç´¢æ ‡æ³¨åˆ†æ\n")
            report_content.append(f"- **ç›´æ¥æ£€ç´¢æ ‡æ³¨**: {annotation_stats['direct_annotations']:,}")
            report_content.append(f"- **åå‘æ£€ç´¢æ ‡æ³¨**: {annotation_stats['inverse_annotations']:,}")
            
            if annotation_stats['direct_stats']['total_retrieved_images']:
                avg_images = np.mean(annotation_stats['direct_stats']['total_retrieved_images'])
                report_content.append(f"- **å¹³å‡æ£€ç´¢å›¾åƒæ•°**: {avg_images:.1f}")
            
            if annotation_stats['inverse_stats']['entities_count']:
                avg_entities = np.mean(annotation_stats['inverse_stats']['entities_count'])
                report_content.append(f"- **å¹³å‡è¯†åˆ«å®ä½“æ•°**: {avg_entities:.1f}")
        
        # 6. æ•°æ®è´¨é‡è¯„ä¼°
        report_content.append("\n## æ•°æ®è´¨é‡è¯„ä¼°\n")
        for split, stats in self.analysis_results['basic_stats'].items():
            total = stats['total_samples']
            image_rate = stats['has_image'] / total * 100
            direct_rate = stats['has_direct_annotation'] / total * 100
            inverse_rate = stats['has_inverse_annotation'] / total * 100
            
            report_content.append(f"### {split.upper()} æ•°æ®é›†")
            report_content.append(f"- **å›¾åƒå®Œæ•´æ€§**: {image_rate:.1f}%")
            report_content.append(f"- **ç›´æ¥æ£€ç´¢å®Œæ•´æ€§**: {direct_rate:.1f}%")
            report_content.append(f"- **åå‘æ£€ç´¢å®Œæ•´æ€§**: {inverse_rate:.1f}%")
        
        # 7. å»ºè®®å’Œç»“è®º
        report_content.append("\n## å»ºè®®å’Œç»“è®º\n")
        report_content.append("### æ•°æ®é›†ä¼˜åŠ¿")
        report_content.append("1. **å¤šæ¨¡æ€ç‰¹æ€§**: æä¾›äº†æ–‡æœ¬ã€å›¾åƒå’Œæ£€ç´¢ä¿¡æ¯çš„å…¨é¢ç»“åˆ")
        report_content.append("2. **çœŸå®åœºæ™¯**: æ¥æºäºçœŸå®çš„ç¤¾äº¤åª’ä½“è°£è¨€æ£€æµ‹åœºæ™¯")
        report_content.append("3. **ä¸°å¯Œæ ‡æ³¨**: åŒ…å«è¯¦ç»†çš„æ£€ç´¢éªŒè¯ä¿¡æ¯")
        
        report_content.append("\n### æ½œåœ¨æŒ‘æˆ˜")
        report_content.append("1. **æ ‡ç­¾ä¸å¹³è¡¡**: éœ€è¦è€ƒè™‘ç±»åˆ«ä¸å¹³è¡¡çš„å¤„ç†ç­–ç•¥")
        report_content.append("2. **å¤šè¯­è¨€å¤„ç†**: ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬éœ€è¦ç‰¹æ®Šçš„é¢„å¤„ç†æ–¹æ³•")
        report_content.append("3. **å›¾åƒè´¨é‡**: éƒ¨åˆ†å›¾åƒå¯èƒ½å­˜åœ¨è´¨é‡æˆ–æ ¼å¼é—®é¢˜")
        
        report_content.append("\n### å»ºè®®æ–¹æ³•")
        report_content.append("1. **å¤šæ¨¡æ€èåˆ**: è®¾è®¡æœ‰æ•ˆçš„æ–‡æœ¬-å›¾åƒèåˆç­–ç•¥")
        report_content.append("2. **æ£€ç´¢å¢å¼º**: åˆ©ç”¨æ£€ç´¢ä¿¡æ¯è¿›è¡Œæ¨¡å‹å¢å¼º")
        report_content.append("3. **é¢„è®­ç»ƒæ¨¡å‹**: ä½¿ç”¨ä¸­è‹±æ–‡å¤šè¯­è¨€é¢„è®­ç»ƒæ¨¡å‹")
        
        # ä¿å­˜æŠ¥å‘Š
        report_text = '\n'.join(report_content)
        report_file = self.reports_dir / 'mr2_dataset_analysis_report.md'
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        return report_text
    
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®é›†åˆ†ææµç¨‹"""
        print("ğŸš€ === å¼€å§‹MR2æ•°æ®é›†å®Œæ•´åˆ†æ ===")
        
        # 1. åŠ è½½æ•°æ®
        self.load_data()
        
        # 2. åŸºç¡€ç»Ÿè®¡åˆ†æ
        self.basic_statistics()
        
        # 3. æ–‡æœ¬åˆ†æ
        self.text_analysis()
        
        # 4. å›¾åƒåˆ†æ
        self.image_analysis()
        
        # 5. æ£€ç´¢æ ‡æ³¨åˆ†æ
        self.annotation_analysis()
        
        # 6. åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        self.create_visualizations()
        
        # 7. ç”Ÿæˆåˆ†ææŠ¥å‘Š
        self.generate_report()
        
        print(f"\nğŸ‰ === åˆ†æå®Œæˆ! ===")
        
        if USE_CONFIG_MANAGER:
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.charts_dir.parent}")
            print(f"ğŸ“Š å›¾è¡¨ç›®å½•: {self.charts_dir}")
            print(f"ğŸ“„ æŠ¥å‘Šç›®å½•: {self.reports_dir}")
        else:
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
            print(f"ğŸ“Š å›¾è¡¨ç›®å½•: {self.charts_dir}")
            print(f"ğŸ“„ æŠ¥å‘Šç›®å½•: {self.reports_dir}")
        
        return self.analysis_results


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºåˆ†æå™¨å®ä¾‹ (è‡ªåŠ¨æ£€æµ‹é…ç½®)
    analyzer = MR2DatasetAnalyzer()
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    results = analyzer.run_complete_analysis()
    
    print("\n" + "="*50)
    print("åˆ†æç»“æœé¢„è§ˆ:")
    print("="*50)
    
    # æ‰“å°å…³é”®ç»Ÿè®¡ä¿¡æ¯
    for split, stats in results['basic_stats'].items():
        print(f"\n{split.upper()} æ•°æ®é›†: {stats['total_samples']} æ ·æœ¬")
        print(f"  æ ‡ç­¾åˆ†å¸ƒ: {dict(stats['label_distribution'])}")
        print(f"  æ•°æ®å®Œæ•´æ€§: å›¾åƒ({stats['has_image']}) ç›´æ¥æ£€ç´¢({stats['has_direct_annotation']}) åå‘æ£€ç´¢({stats['has_inverse_annotation']})")