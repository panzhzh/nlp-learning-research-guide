#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# datasets/mr2_analysis.py

"""
MR2å¤šæ¨¡æ€è°£è¨€æ£€æµ‹æ•°æ®é›†æ·±åº¦åˆ†æ
å…¨é¢åˆ†ææ•°æ®é›†ç»“æ„ã€åˆ†å¸ƒç‰¹å¾ï¼Œç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
"""

import json
import os
import re
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Any
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾è¡¨é£æ ¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")


class MR2DatasetAnalyzer:
    """MR2æ•°æ®é›†åˆ†æå™¨"""
    
    def __init__(self, data_dir: str = '../data', output_dir: str = 'outputs'):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        """
        self.data_dir = data_dir
        self.output_dir = output_dir
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(os.path.join(output_dir, 'charts'), exist_ok=True)
        os.makedirs(os.path.join(output_dir, 'analysis'), exist_ok=True)
        
        # æ ‡ç­¾æ˜ å°„
        self.label_mapping = {
            0: 'Non-rumor',
            1: 'Rumor', 
            2: 'Unverified'
        }
        
        # å­˜å‚¨åˆ†æç»“æœ
        self.analysis_results = {}
        
    def load_data(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®é›†"""
        print("ğŸ”„ å¼€å§‹åŠ è½½MR2æ•°æ®é›†...")
        
        self.datasets = {}
        splits = ['train', 'val', 'test']
        
        for split in splits:
            file_path = os.path.join(self.data_dir, f'dataset_items_{split}.json')
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    self.datasets[split] = json.load(f)
                print(f"âœ… åŠ è½½ {split} æ•°æ®: {len(self.datasets[split])} æ¡")
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ° {split} æ•°æ®æ–‡ä»¶: {file_path}")
                
        return self.datasets
    
    def basic_statistics(self):
        """åŸºç¡€ç»Ÿè®¡åˆ†æ"""
        print("\nğŸ“Š === åŸºç¡€ç»Ÿè®¡åˆ†æ ===")
        
        stats = {}
        total_samples = 0
        
        for split, data in self.datasets.items():
            split_stats = {
                'total_samples': len(data),
                'label_distribution': Counter(),
                'has_image': 0,
                'has_direct_annotation': 0,
                'has_inverse_annotation': 0
            }
            
            for item_id, item in data.items():
                # æ ‡ç­¾åˆ†å¸ƒ
                label = item.get('label', -1)
                split_stats['label_distribution'][label] += 1
                
                # å›¾åƒæ–‡ä»¶æ£€æŸ¥
                if 'image_path' in item:
                    image_path = os.path.join(self.data_dir, item['image_path'])
                    if os.path.exists(image_path):
                        split_stats['has_image'] += 1
                
                # æ£€ç´¢æ ‡æ³¨æ£€æŸ¥
                if 'direct_path' in item:
                    direct_path = os.path.join(self.data_dir, item['direct_path'], 'direct_annotation.json')
                    if os.path.exists(direct_path):
                        split_stats['has_direct_annotation'] += 1
                        
                if 'inv_path' in item:
                    inv_path = os.path.join(self.data_dir, item['inv_path'], 'inverse_annotation.json')
                    if os.path.exists(inv_path):
                        split_stats['has_inverse_annotation'] += 1
            
            stats[split] = split_stats
            total_samples += split_stats['total_samples']
            
            print(f"\n{split.upper()} æ•°æ®é›†:")
            print(f"  æ ·æœ¬æ€»æ•°: {split_stats['total_samples']}")
            print(f"  æ ‡ç­¾åˆ†å¸ƒ: {dict(split_stats['label_distribution'])}")
            print(f"  æœ‰å›¾åƒ: {split_stats['has_image']}")
            print(f"  æœ‰ç›´æ¥æ£€ç´¢: {split_stats['has_direct_annotation']}")
            print(f"  æœ‰åå‘æ£€ç´¢: {split_stats['has_inverse_annotation']}")
        
        print(f"\næ€»æ ·æœ¬æ•°: {total_samples}")
        self.analysis_results['basic_stats'] = stats
        return stats
    
    def text_analysis(self):
        """æ–‡æœ¬å†…å®¹åˆ†æ"""
        print("\nğŸ“ === æ–‡æœ¬å†…å®¹åˆ†æ ===")
        
        text_stats = {
            'total_texts': 0,
            'length_distribution': [],
            'language_distribution': Counter(),
            'word_count_distribution': [],
            'character_distribution': Counter(),
            'common_words': Counter(),
            'samples_by_length': {'short': [], 'medium': [], 'long': []}
        }
        
        all_texts = []
        
        for split, data in self.datasets.items():
            for item_id, item in data.items():
                caption = item.get('caption', '').strip()
                if caption:
                    all_texts.append(caption)
                    text_stats['total_texts'] += 1
                    
                    # æ–‡æœ¬é•¿åº¦
                    text_length = len(caption)
                    text_stats['length_distribution'].append(text_length)
                    
                    # è¯æ•°ç»Ÿè®¡
                    words = caption.split()
                    word_count = len(words)
                    text_stats['word_count_distribution'].append(word_count)
                    
                    # å¸¸ç”¨è¯ç»Ÿè®¡
                    for word in words:
                        cleaned_word = re.sub(r'[^\w]', '', word.lower())
                        if len(cleaned_word) > 2:  # è¿‡æ»¤çŸ­è¯
                            text_stats['common_words'][cleaned_word] += 1
                    
                    # å­—ç¬¦åˆ†å¸ƒ
                    for char in caption.lower():
                        if char.isalpha():
                            text_stats['character_distribution'][char] += 1
                    
                    # è¯­è¨€æ£€æµ‹ (ç®€å•è§„åˆ™)
                    if re.search(r'[\u4e00-\u9fff]', caption):
                        if re.search(r'[a-zA-Z]', caption):
                            text_stats['language_distribution']['mixed'] += 1
                        else:
                            text_stats['language_distribution']['chinese'] += 1
                    else:
                        text_stats['language_distribution']['english'] += 1
                    
                    # æŒ‰é•¿åº¦åˆ†ç±»æ ·æœ¬
                    if text_length < 30:
                        text_stats['samples_by_length']['short'].append(caption)
                    elif text_length < 100:
                        text_stats['samples_by_length']['medium'].append(caption)
                    else:
                        text_stats['samples_by_length']['long'].append(caption)
        
        # ç»Ÿè®¡æ‘˜è¦
        if text_stats['length_distribution']:
            print(f"æ–‡æœ¬æ€»æ•°: {text_stats['total_texts']}")
            print(f"å¹³å‡é•¿åº¦: {np.mean(text_stats['length_distribution']):.1f} å­—ç¬¦")
            print(f"å¹³å‡è¯æ•°: {np.mean(text_stats['word_count_distribution']):.1f} è¯")
            print(f"è¯­è¨€åˆ†å¸ƒ: {dict(text_stats['language_distribution'])}")
            print(f"æœ€å¸¸è§è¯æ±‡: {text_stats['common_words'].most_common(10)}")
            
            # å±•ç¤ºä¸åŒé•¿åº¦çš„æ ·æœ¬
            print(f"\nçŸ­æ–‡æœ¬æ ·ä¾‹ (<30å­—ç¬¦): {text_stats['samples_by_length']['short'][:3]}")
            print(f"ä¸­ç­‰æ–‡æœ¬æ ·ä¾‹ (30-100å­—ç¬¦): {text_stats['samples_by_length']['medium'][:3]}")
            print(f"é•¿æ–‡æœ¬æ ·ä¾‹ (>100å­—ç¬¦): {text_stats['samples_by_length']['long'][:2]}")
        
        self.analysis_results['text_stats'] = text_stats
        return text_stats
    
    def image_analysis(self):
        """å›¾åƒæ•°æ®åˆ†æ"""
        print("\nğŸ–¼ï¸  === å›¾åƒæ•°æ®åˆ†æ ===")
        
        image_stats = {
            'total_images': 0,
            'valid_images': 0,
            'image_sizes': [],
            'image_formats': Counter(),
            'size_distribution': {'width': [], 'height': []},
            'file_sizes': []
        }
        
        for split, data in self.datasets.items():
            for item_id, item in data.items():
                if 'image_path' in item:
                    image_stats['total_images'] += 1
                    image_path = os.path.join(self.data_dir, item['image_path'])
                    
                    if os.path.exists(image_path):
                        try:
                            # å°è¯•æ‰“å¼€å›¾åƒ
                            with Image.open(image_path) as img:
                                image_stats['valid_images'] += 1
                                
                                # å›¾åƒå°ºå¯¸
                                width, height = img.size
                                image_stats['size_distribution']['width'].append(width)
                                image_stats['size_distribution']['height'].append(height)
                                image_stats['image_sizes'].append((width, height))
                                
                                # å›¾åƒæ ¼å¼
                                image_stats['image_formats'][img.format] += 1
                                
                                # æ–‡ä»¶å¤§å°
                                file_size = os.path.getsize(image_path)
                                image_stats['file_sizes'].append(file_size)
                                
                        except Exception as e:
                            print(f"âš ï¸  æ— æ³•è¯»å–å›¾åƒ {image_path}: {e}")
        
        if image_stats['valid_images'] > 0:
            print(f"å›¾åƒæ€»æ•°: {image_stats['total_images']}")
            print(f"æœ‰æ•ˆå›¾åƒ: {image_stats['valid_images']}")
            print(f"å›¾åƒæ ¼å¼: {dict(image_stats['image_formats'])}")
            print(f"å¹³å‡å°ºå¯¸: {np.mean(image_stats['size_distribution']['width']):.0f} x {np.mean(image_stats['size_distribution']['height']):.0f}")
            print(f"å¹³å‡æ–‡ä»¶å¤§å°: {np.mean(image_stats['file_sizes'])/1024:.1f} KB")
        
        self.analysis_results['image_stats'] = image_stats
        return image_stats
    
    def annotation_analysis(self):
        """æ£€ç´¢æ ‡æ³¨æ•°æ®åˆ†æ"""
        print("\nğŸ” === æ£€ç´¢æ ‡æ³¨åˆ†æ ===")
        
        annotation_stats = {
            'direct_annotations': 0,
            'inverse_annotations': 0,
            'direct_stats': {
                'images_with_captions': [],
                'images_no_captions': [],
                'domains': Counter(),
                'total_retrieved_images': []
            },
            'inverse_stats': {
                'entities_count': [],
                'entity_scores': [],
                'best_guess_labels': [],
                'fully_matched': [],
                'partially_matched': [],
                'common_entities': Counter()
            }
        }
        
        for split, data in self.datasets.items():
            for item_id, item in data.items():
                # ç›´æ¥æ£€ç´¢åˆ†æ
                if 'direct_path' in item:
                    direct_file = os.path.join(self.data_dir, item['direct_path'], 'direct_annotation.json')
                    if os.path.exists(direct_file):
                        try:
                            with open(direct_file, 'r', encoding='utf-8') as f:
                                direct_data = json.load(f)
                                annotation_stats['direct_annotations'] += 1
                                
                                # ç»Ÿè®¡æ£€ç´¢ç»“æœ
                                if 'images_with_captions' in direct_data:
                                    count = len(direct_data['images_with_captions'])
                                    annotation_stats['direct_stats']['images_with_captions'].append(count)
                                    
                                    # åŸŸåç»Ÿè®¡
                                    for img_info in direct_data['images_with_captions']:
                                        domain = img_info.get('domain', 'unknown')
                                        annotation_stats['direct_stats']['domains'][domain] += 1
                                
                                if 'images_with_no_captions' in direct_data:
                                    count = len(direct_data['images_with_no_captions'])
                                    annotation_stats['direct_stats']['images_no_captions'].append(count)
                                
                                # æ€»æ£€ç´¢å›¾åƒæ•°
                                total = len(direct_data.get('images_with_captions', [])) + len(direct_data.get('images_with_no_captions', []))
                                annotation_stats['direct_stats']['total_retrieved_images'].append(total)
                                
                        except Exception as e:
                            print(f"âš ï¸  è¯»å–ç›´æ¥æ£€ç´¢æ–‡ä»¶å¤±è´¥ {direct_file}: {e}")
                
                # åå‘æ£€ç´¢åˆ†æ
                if 'inv_path' in item:
                    inverse_file = os.path.join(self.data_dir, item['inv_path'], 'inverse_annotation.json')
                    if os.path.exists(inverse_file):
                        try:
                            with open(inverse_file, 'r', encoding='utf-8') as f:
                                inverse_data = json.load(f)
                                annotation_stats['inverse_annotations'] += 1
                                
                                # å®ä½“åˆ†æ
                                entities = inverse_data.get('entities', [])
                                annotation_stats['inverse_stats']['entities_count'].append(len(entities))
                                
                                for entity in entities:
                                    annotation_stats['inverse_stats']['common_entities'][entity] += 1
                                
                                # å®ä½“åˆ†æ•°
                                scores = inverse_data.get('entities_scores', [])
                                annotation_stats['inverse_stats']['entity_scores'].extend(scores)
                                
                                # æœ€ä½³çŒœæµ‹æ ‡ç­¾
                                best_guess = inverse_data.get('best_guess_lbl', [])
                                annotation_stats['inverse_stats']['best_guess_labels'].extend(best_guess)
                                
                                # åŒ¹é…ç»“æœç»Ÿè®¡
                                fully_matched = len(inverse_data.get('all_fully_matched_captions', []))
                                partially_matched = len(inverse_data.get('all_partially_matched_captions', []))
                                annotation_stats['inverse_stats']['fully_matched'].append(fully_matched)
                                annotation_stats['inverse_stats']['partially_matched'].append(partially_matched)
                                
                        except Exception as e:
                            print(f"âš ï¸  è¯»å–åå‘æ£€ç´¢æ–‡ä»¶å¤±è´¥ {inverse_file}: {e}")
        
        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        print(f"ç›´æ¥æ£€ç´¢æ ‡æ³¨æ•°: {annotation_stats['direct_annotations']}")
        print(f"åå‘æ£€ç´¢æ ‡æ³¨æ•°: {annotation_stats['inverse_annotations']}")
        
        if annotation_stats['direct_stats']['total_retrieved_images']:
            print(f"å¹³å‡æ£€ç´¢å›¾åƒæ•°: {np.mean(annotation_stats['direct_stats']['total_retrieved_images']):.1f}")
            print(f"çƒ­é—¨åŸŸå: {annotation_stats['direct_stats']['domains'].most_common(5)}")
        
        if annotation_stats['inverse_stats']['entities_count']:
            print(f"å¹³å‡å®ä½“æ•°: {np.mean(annotation_stats['inverse_stats']['entities_count']):.1f}")
            print(f"å¸¸è§å®ä½“: {annotation_stats['inverse_stats']['common_entities'].most_common(10)}")
        
        self.analysis_results['annotation_stats'] = annotation_stats
        return annotation_stats
    
    def create_visualizations(self):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        print("\nğŸ“Š === ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ===")
        
        # è®¾ç½®å›¾è¡¨å‚æ•°
        plt.style.use('default')
        fig_size = (15, 12)
        
        # 1. æ•°æ®é›†åŸºç¡€åˆ†å¸ƒå›¾
        self._plot_basic_distribution()
        
        # 2. æ–‡æœ¬é•¿åº¦åˆ†å¸ƒå›¾
        self._plot_text_distribution()
        
        # 3. å›¾åƒå°ºå¯¸åˆ†å¸ƒå›¾
        self._plot_image_distribution()
        
        # 4. æ£€ç´¢ç»“æœåˆ†æå›¾
        self._plot_annotation_analysis()
        
        # 5. ç»¼åˆåˆ†æä»ªè¡¨æ¿
        self._create_dashboard()
        
        print("âœ… æ‰€æœ‰å›¾è¡¨å·²ç”Ÿæˆå®Œæˆ")
    
    def _plot_basic_distribution(self):
        """ç»˜åˆ¶åŸºç¡€æ•°æ®åˆ†å¸ƒ"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('MR2æ•°æ®é›†åŸºç¡€åˆ†å¸ƒåˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. æ•°æ®é›†å¤§å°åˆ†å¸ƒ
        splits = list(self.analysis_results['basic_stats'].keys())
        sizes = [self.analysis_results['basic_stats'][split]['total_samples'] for split in splits]
        
        axes[0, 0].bar(splits, sizes, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        axes[0, 0].set_title('å„æ•°æ®é›†å¤§å°åˆ†å¸ƒ')
        axes[0, 0].set_ylabel('æ ·æœ¬æ•°é‡')
        for i, v in enumerate(sizes):
            axes[0, 0].text(i, v + max(sizes)*0.01, str(v), ha='center', va='bottom')
        
        # 2. æ ‡ç­¾åˆ†å¸ƒ (åˆå¹¶æ‰€æœ‰split)
        all_labels = Counter()
        for split_stats in self.analysis_results['basic_stats'].values():
            all_labels.update(split_stats['label_distribution'])
        
        labels = [self.label_mapping.get(k, f'Unknown({k})') for k in all_labels.keys()]
        counts = list(all_labels.values())
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
        
        axes[0, 1].pie(counts, labels=labels, autopct='%1.1f%%', colors=colors)
        axes[0, 1].set_title('æ ‡ç­¾åˆ†å¸ƒ')
        
        # 3. æ•°æ®å®Œæ•´æ€§åˆ†æ
        completeness_data = []
        for split, stats in self.analysis_results['basic_stats'].items():
            total = stats['total_samples']
            completeness_data.append({
                'Split': split,
                'Has Image': stats['has_image'] / total * 100,
                'Has Direct': stats['has_direct_annotation'] / total * 100,
                'Has Inverse': stats['has_inverse_annotation'] / total * 100
            })
        
        df_completeness = pd.DataFrame(completeness_data)
        x = np.arange(len(df_completeness))
        width = 0.25
        
        axes[1, 0].bar(x - width, df_completeness['Has Image'], width, label='æœ‰å›¾åƒ', color='#FF6B6B')
        axes[1, 0].bar(x, df_completeness['Has Direct'], width, label='æœ‰ç›´æ¥æ£€ç´¢', color='#4ECDC4')
        axes[1, 0].bar(x + width, df_completeness['Has Inverse'], width, label='æœ‰åå‘æ£€ç´¢', color='#45B7D1')
        
        axes[1, 0].set_ylabel('å®Œæ•´æ€§ (%)')
        axes[1, 0].set_title('æ•°æ®å®Œæ•´æ€§åˆ†æ')
        axes[1, 0].set_xticks(x)
        axes[1, 0].set_xticklabels(df_completeness['Split'])
        axes[1, 0].legend()
        
        # 4. æŒ‰splitçš„æ ‡ç­¾åˆ†å¸ƒ
        split_label_data = []
        for split, stats in self.analysis_results['basic_stats'].items():
            for label, count in stats['label_distribution'].items():
                split_label_data.append({
                    'Split': split,
                    'Label': self.label_mapping.get(label, f'Unknown({label})'),
                    'Count': count
                })
        
        df_split_labels = pd.DataFrame(split_label_data)
        pivot_df = df_split_labels.pivot(index='Split', columns='Label', values='Count').fillna(0)
        
        pivot_df.plot(kind='bar', ax=axes[1, 1], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        axes[1, 1].set_title('å„æ•°æ®é›†æ ‡ç­¾åˆ†å¸ƒ')
        axes[1, 1].set_ylabel('æ ·æœ¬æ•°é‡')
        axes[1, 1].legend(title='æ ‡ç­¾')
        axes[1, 1].tick_params(axis='x', rotation=0)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'charts', 'basic_distribution.png'), dpi=300, bbox_inches='tight')
        plt.show()
    
    def _plot_text_distribution(self):
        """ç»˜åˆ¶æ–‡æœ¬åˆ†å¸ƒåˆ†æ"""
        if 'text_stats' not in self.analysis_results:
            return
            
        text_stats = self.analysis_results['text_stats']
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        fig.suptitle('æ–‡æœ¬å†…å®¹åˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
        axes[0, 0].hist(text_stats['length_distribution'], bins=30, color='#FF6B6B', alpha=0.7)
        axes[0, 0].set_title('æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ')
        axes[0, 0].set_xlabel('å­—ç¬¦æ•°')
        axes[0, 0].set_ylabel('é¢‘æ¬¡')
        
        # 2. è¯æ•°åˆ†å¸ƒ
        axes[0, 1].hist(text_stats['word_count_distribution'], bins=20, color='#4ECDC4', alpha=0.7)
        axes[0, 1].set_title('è¯æ•°åˆ†å¸ƒ')
        axes[0, 1].set_xlabel('è¯æ•°')
        axes[0, 1].set_ylabel('é¢‘æ¬¡')
        
        # 3. è¯­è¨€åˆ†å¸ƒ
        lang_data = text_stats['language_distribution']
        axes[0, 2].pie(lang_data.values(), labels=lang_data.keys(), autopct='%1.1f%%',
                      colors=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        axes[0, 2].set_title('è¯­è¨€åˆ†å¸ƒ')
        
        # 4. å¸¸ç”¨è¯äº‘å½¢å¼çš„æŸ±çŠ¶å›¾
        common_words = text_stats['common_words'].most_common(15)
        words, counts = zip(*common_words)
        
        axes[1, 0].barh(range(len(words)), counts, color='#96CEB4')
        axes[1, 0].set_yticks(range(len(words)))
        axes[1, 0].set_yticklabels(words)
        axes[1, 0].set_title('æœ€å¸¸è§è¯æ±‡ (Top 15)')
        axes[1, 0].set_xlabel('å‡ºç°æ¬¡æ•°')
        
        # 5. å­—ç¬¦é¢‘ç‡åˆ†å¸ƒ
        char_freq = text_stats['character_distribution'].most_common(20)
        chars, freqs = zip(*char_freq)
        
        axes[1, 1].bar(range(len(chars)), freqs, color='#FFEAA7')
        axes[1, 1].set_xticks(range(len(chars)))
        axes[1, 1].set_xticklabels(chars)
        axes[1, 1].set_title('å­—ç¬¦é¢‘ç‡åˆ†å¸ƒ (Top 20)')
        axes[1, 1].set_ylabel('å‡ºç°æ¬¡æ•°')
        
        # 6. æ–‡æœ¬é•¿åº¦ç»Ÿè®¡æ‘˜è¦
        length_stats = {
            'å¹³å‡é•¿åº¦': np.mean(text_stats['length_distribution']),
            'ä¸­ä½æ•°': np.median(text_stats['length_distribution']),
            'æœ€å¤§é•¿åº¦': np.max(text_stats['length_distribution']),
            'æœ€å°é•¿åº¦': np.min(text_stats['length_distribution']),
            'æ ‡å‡†å·®': np.std(text_stats['length_distribution'])
        }
        
        axes[1, 2].axis('off')
        stats_text = '\n'.join([f'{k}: {v:.1f}' for k, v in length_stats.items()])
        axes[1, 2].text(0.1, 0.5, f'æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:\n\n{stats_text}', 
                       transform=axes[1, 2].transAxes, fontsize=12,
                       verticalalignment='center', bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'charts', 'text_distribution.png'), dpi=300, bbox_inches='tight')
        plt.show()
    
    def _plot_image_distribution(self):
        """ç»˜åˆ¶å›¾åƒåˆ†å¸ƒåˆ†æ"""
        if 'image_stats' not in self.analysis_results:
            return
            
        image_stats = self.analysis_results['image_stats']
        
        if image_stats['valid_images'] == 0:
            print("âš ï¸  æ²¡æœ‰æœ‰æ•ˆå›¾åƒæ•°æ®ï¼Œè·³è¿‡å›¾åƒåˆ†æå›¾è¡¨")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('å›¾åƒæ•°æ®åˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. å›¾åƒå°ºå¯¸æ•£ç‚¹å›¾
        widths = image_stats['size_distribution']['width']
        heights = image_stats['size_distribution']['height']
        
        axes[0, 0].scatter(widths, heights, alpha=0.6, color='#FF6B6B')
        axes[0, 0].set_xlabel('å®½åº¦ (åƒç´ )')
        axes[0, 0].set_ylabel('é«˜åº¦ (åƒç´ )')
        axes[0, 0].set_title('å›¾åƒå°ºå¯¸åˆ†å¸ƒ')
        
        # 2. å®½åº¦åˆ†å¸ƒç›´æ–¹å›¾
        axes[0, 1].hist(widths, bins=20, color='#4ECDC4', alpha=0.7)
        axes[0, 1].set_xlabel('å®½åº¦ (åƒç´ )')
        axes[0, 1].set_ylabel('é¢‘æ¬¡')
        axes[0, 1].set_title('å›¾åƒå®½åº¦åˆ†å¸ƒ')
        
        # 3. é«˜åº¦åˆ†å¸ƒç›´æ–¹å›¾
        axes[1, 0].hist(heights, bins=20, color='#45B7D1', alpha=0.7)
        axes[1, 0].set_xlabel('é«˜åº¦ (åƒç´ )')
        axes[1, 0].set_ylabel('é¢‘æ¬¡')
        axes[1, 0].set_title('å›¾åƒé«˜åº¦åˆ†å¸ƒ')
        
        # 4. å›¾åƒæ ¼å¼åˆ†å¸ƒ
        format_data = image_stats['image_formats']
        axes[1, 1].pie(format_data.values(), labels=format_data.keys(), autopct='%1.1f%%',
                      colors=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
        axes[1, 1].set_title('å›¾åƒæ ¼å¼åˆ†å¸ƒ')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'charts', 'image_distribution.png'), dpi=300, bbox_inches='tight')
        plt.show()
    
    def _plot_annotation_analysis(self):
        """ç»˜åˆ¶æ£€ç´¢æ ‡æ³¨åˆ†æ"""
        if 'annotation_stats' not in self.analysis_results:
            return
            
        annotation_stats = self.analysis_results['annotation_stats']
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        fig.suptitle('æ£€ç´¢æ ‡æ³¨æ•°æ®åˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. æ£€ç´¢æ•°é‡å¯¹æ¯”
        annotation_counts = [
            annotation_stats['direct_annotations'],
            annotation_stats['inverse_annotations']
        ]
        
        axes[0, 0].bar(['ç›´æ¥æ£€ç´¢', 'åå‘æ£€ç´¢'], annotation_counts, color=['#FF6B6B', '#4ECDC4'])
        axes[0, 0].set_title('æ£€ç´¢æ ‡æ³¨æ•°é‡')
        axes[0, 0].set_ylabel('æ ‡æ³¨æ•°é‡')
        for i, v in enumerate(annotation_counts):
            axes[0, 0].text(i, v + max(annotation_counts)*0.01, str(v), ha='center', va='bottom')
        
        # 2. ç›´æ¥æ£€ç´¢å›¾åƒæ•°é‡åˆ†å¸ƒ
        if annotation_stats['direct_stats']['total_retrieved_images']:
            axes[0, 1].hist(annotation_stats['direct_stats']['total_retrieved_images'], 
                           bins=15, color='#45B7D1', alpha=0.7)
            axes[0, 1].set_title('ç›´æ¥æ£€ç´¢å›¾åƒæ•°é‡åˆ†å¸ƒ')