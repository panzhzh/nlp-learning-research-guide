#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# examples/unified_training_demo.py

"""
ç»Ÿä¸€è®­ç»ƒæ¼”ç¤º - ä½¿ç”¨çœŸå®MR2æ•°æ®
é›†æˆä¼ ç»ŸMLå’Œç¥ç»ç½‘ç»œçš„å®Œæ•´è®­ç»ƒå¯¹æ¯”
ä¿®å¤äº†æ‰€æœ‰æ¨¡å—å¯¼å…¥é—®é¢˜ï¼Œç¡®ä¿ä½¿ç”¨çœŸå®æ•°æ®
"""

import sys
import os
import json
import time
from pathlib import Path
from datetime import datetime
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
code_root = current_file.parent.parent
sys.path.append(str(code_root))

print("ğŸ¯ MR2ç»Ÿä¸€è®­ç»ƒæ¼”ç¤º - ä¼ ç»ŸML vs ç¥ç»ç½‘ç»œ")
print("="*60)

def check_real_data():
    """æ£€æŸ¥çœŸå®æ•°æ®æ˜¯å¦å­˜åœ¨"""
    print("\nğŸ“ æ£€æŸ¥çœŸå®æ•°æ®æ–‡ä»¶...")
    
    data_dir = code_root / "data"
    required_files = [
        "dataset_items_train.json",
        "dataset_items_val.json", 
        "dataset_items_test.json"
    ]
    
    existing_files = []
    
    for file_name in required_files:
        file_path = data_dir / file_name
        if file_path.exists():
            file_size = file_path.stat().st_size
            existing_files.append((file_name, file_size))
            print(f"   âœ… {file_name} ({file_size/1024:.1f} KB)")
        else:
            print(f"   âŒ {file_name} - æ–‡ä»¶ä¸å­˜åœ¨")
            return False
    
    print(f"\nâœ… æ‰€æœ‰æ•°æ®æ–‡ä»¶å°±ç»ª! å…± {len(existing_files)} ä¸ªæ–‡ä»¶")
    return True

def run_traditional_ml_training():
    """è¿è¡Œä¼ ç»ŸMLè®­ç»ƒ"""
    print("\nğŸ¤– === ç¬¬ä¸€é˜¶æ®µ: ä¼ ç»Ÿæœºå™¨å­¦ä¹ è®­ç»ƒ ===")
    
    try:
        # ç›´æ¥å¯¼å…¥å¹¶æ‰§è¡Œtraditional_ml_demoçš„ä¸»è¦é€»è¾‘
        sys.path.insert(0, str(code_root / "examples"))
        
        # é‡æ–°å®ç°traditional_ml_demoçš„æ ¸å¿ƒåŠŸèƒ½ï¼Œé¿å…å¯¼å…¥é—®é¢˜
        from traditional_ml_demo import load_real_mr2_data, train_traditional_models
        
        print("ğŸ”„ åŠ è½½æ•°æ®å¹¶è®­ç»ƒä¼ ç»ŸMLæ¨¡å‹...")
        datasets = load_real_mr2_data()
        results, vectorizer, models = train_traditional_models(datasets)
        
        print("âœ… ä¼ ç»ŸMLè®­ç»ƒå®Œæˆ!")
        return results, 'traditional'
        
    except Exception as e:
        print(f"âŒ ä¼ ç»ŸMLè®­ç»ƒå¤±è´¥: {e}")
        print("âš ï¸  å°†ä½¿ç”¨å¤‡ç”¨å®ç°...")
        return run_traditional_ml_backup(), 'traditional'

def run_traditional_ml_backup():
    """ä¼ ç»ŸMLè®­ç»ƒçš„å¤‡ç”¨å®ç°"""
    print("ğŸ”„ æ‰§è¡Œä¼ ç»ŸMLå¤‡ç”¨è®­ç»ƒ...")
    
    # è¿™é‡Œå®ç°ä¸€ä¸ªç®€åŒ–çš„ä¼ ç»ŸMLè®­ç»ƒ
    import json
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.naive_bayes import MultinomialNB
    from sklearn.linear_model import LogisticRegression
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics import accuracy_score, f1_score
    import re
    
    # åŠ è½½æ•°æ®
    data_dir = code_root / "data"
    datasets = {}
    
    for split in ['train', 'val', 'test']:
        file_path = data_dir / f'dataset_items_{split}.json'
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        
        texts = []
        labels = []
        for item_id, item_data in raw_data.items():
            if 'caption' in item_data and 'label' in item_data:
                text = item_data['caption']
                if isinstance(text, str) and text.strip():
                    # ç®€å•é¢„å¤„ç†
                    text = re.sub(r'http\S+', '', text)
                    text = re.sub(r'@\w+', '', text)
                    text = re.sub(r'#\w+', '', text)
                    text = re.sub(r'\s+', ' ', text)
                    text = text.strip().lower()
                    
                    texts.append(text)
                    labels.append(item_data['label'])
        
        datasets[split] = (texts, labels)
        print(f"   {split}: {len(texts)} æ ·æœ¬")
    
    # ç‰¹å¾æå–
    print("ğŸ”§ æå–TF-IDFç‰¹å¾...")
    vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1, 2), min_df=2, max_df=0.95)
    
    train_texts, train_labels = datasets['train']
    val_texts, val_labels = datasets['val']
    test_texts, test_labels = datasets['test']
    
    X_train = vectorizer.fit_transform(train_texts)
    X_val = vectorizer.transform(val_texts)
    X_test = vectorizer.transform(test_texts)
    
    y_train = np.array(train_labels)
    y_val = np.array(val_labels)
    y_test = np.array(test_labels)
    
    # è®­ç»ƒæ¨¡å‹
    models = {
        'SVM': SVC(kernel='rbf', C=1.0, random_state=42),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Naive Bayes': MultinomialNB(alpha=1.0),
        'Logistic Regression': LogisticRegression(C=1.0, max_iter=1000, random_state=42)
    }
    
    results = {}
    for name, model in models.items():
        print(f"ğŸ‹ï¸ è®­ç»ƒ {name}...")
        start_time = time.time()
        
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        test_pred = model.predict(X_test)
        val_pred = model.predict(X_val)
        
        # è¯„ä¼°
        test_acc = accuracy_score(y_test, test_pred)
        test_f1 = f1_score(y_test, test_pred, average='macro')
        val_acc = accuracy_score(y_val, val_pred)
        val_f1 = f1_score(y_val, val_pred, average='macro')
        
        training_time = time.time() - start_time
        
        results[name] = {
            'test_accuracy': test_acc,
            'test_f1': test_f1,
            'val_accuracy': val_acc,
            'val_f1': val_f1,
            'training_time': training_time
        }
        
        print(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}, æµ‹è¯•F1: {test_f1:.4f}")
    
    return results

def run_neural_network_training():
    """è¿è¡Œç¥ç»ç½‘ç»œè®­ç»ƒ"""
    print("\nğŸ§  === ç¬¬äºŒé˜¶æ®µ: ç¥ç»ç½‘ç»œè®­ç»ƒ ===")
    
    try:
        # ç›´æ¥å¯¼å…¥å¹¶æ‰§è¡Œneural_network_demoçš„ä¸»è¦é€»è¾‘
        sys.path.insert(0, str(code_root / "examples"))
        
        from neural_network_demo import load_real_mr2_data, train_neural_networks
        
        print("ğŸ”„ åŠ è½½æ•°æ®å¹¶è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹...")
        datasets = load_real_mr2_data()
        results, vocab, models = train_neural_networks(datasets)
        
        print("âœ… ç¥ç»ç½‘ç»œè®­ç»ƒå®Œæˆ!")
        return results, 'neural'
        
    except Exception as e:
        print(f"âŒ ç¥ç»ç½‘ç»œè®­ç»ƒå¤±è´¥: {e}")
        print("âš ï¸  å°†ä½¿ç”¨å¤‡ç”¨å®ç°...")
        return run_neural_network_backup(), 'neural'

def run_neural_network_backup():
    """ç¥ç»ç½‘ç»œè®­ç»ƒçš„å¤‡ç”¨å®ç°"""
    print("ğŸ”„ æ‰§è¡Œç¥ç»ç½‘ç»œå¤‡ç”¨è®­ç»ƒ...")
    
    try:
        import torch
        import torch.nn as nn
        import torch.optim as optim
        from torch.utils.data import Dataset, DataLoader
        from sklearn.metrics import accuracy_score, f1_score
        import json
        import re
        from collections import Counter
        from tqdm import tqdm
        
        # æ£€æŸ¥è®¾å¤‡
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
        
        # ç®€åŒ–çš„TextCNNæ¨¡å‹
        class SimpleTextCNN(nn.Module):
            def __init__(self, vocab_size, embedding_dim=64, num_filters=32, num_classes=3):
                super(SimpleTextCNN, self).__init__()
                self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
                self.conv1 = nn.Conv1d(embedding_dim, num_filters, kernel_size=3)
                self.conv2 = nn.Conv1d(embedding_dim, num_filters, kernel_size=4)
                self.dropout = nn.Dropout(0.5)
                self.fc = nn.Linear(num_filters * 2, num_classes)
            
            def forward(self, x):
                embedded = self.embedding(x).transpose(1, 2)
                conv1_out = torch.max(torch.relu(self.conv1(embedded)), dim=2)[0]
                conv2_out = torch.max(torch.relu(self.conv2(embedded)), dim=2)[0]
                concatenated = torch.cat([conv1_out, conv2_out], dim=1)
                output = self.dropout(concatenated)
                return self.fc(output)
        
        # ç®€åŒ–çš„æ•°æ®é›†
        class SimpleDataset(Dataset):
            def __init__(self, texts, labels, vocab, max_len=32):
                self.texts = texts
                self.labels = labels
                self.vocab = vocab
                self.max_len = max_len
            
            def __len__(self):
                return len(self.texts)
            
            def __getitem__(self, idx):
                text = self.texts[idx]
                label = self.labels[idx]
                
                tokens = text.split()[:self.max_len]
                indices = [self.vocab.get(token, 1) for token in tokens]
                indices += [0] * (self.max_len - len(indices))
                
                return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)
        
        # åŠ è½½æ•°æ®
        data_dir = code_root / "data"
        datasets = {}
        
        for split in ['train', 'val', 'test']:
            file_path = data_dir / f'dataset_items_{split}.json'
            with open(file_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
            
            texts = []
            labels = []
            for item_id, item_data in raw_data.items():
                if 'caption' in item_data and 'label' in item_data:
                    text = item_data['caption']
                    if isinstance(text, str) and text.strip():
                        # é¢„å¤„ç†
                        text = re.sub(r'http\S+', '', text)
                        text = re.sub(r'@\w+', '', text)
                        text = re.sub(r'#\w+', '', text)
                        text = re.sub(r'\s+', ' ', text)
                        text = text.strip().lower()
                        
                        texts.append(text)
                        labels.append(item_data['label'])
            
            datasets[split] = (texts, labels)
        
        # æ„å»ºè¯æ±‡è¡¨
        all_texts = datasets['train'][0]
        word_freq = Counter()
        for text in all_texts:
            word_freq.update(text.split())
        
        vocab = {'<PAD>': 0, '<UNK>': 1}
        for word, freq in word_freq.most_common(1000):
            if freq >= 2:
                vocab[word] = len(vocab)
        
        print(f"ğŸ“– è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = SimpleDataset(datasets['train'][0], datasets['train'][1], vocab)
        val_dataset = SimpleDataset(datasets['val'][0], datasets['val'][1], vocab)
        test_dataset = SimpleDataset(datasets['test'][0], datasets['test'][1], vocab)
        
        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
        
        # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
        models = {
            'TextCNN': SimpleTextCNN(len(vocab), embedding_dim=64, num_filters=32)
        }
        
        results = {}
        
        for name, model in models.items():
            print(f"ğŸ‹ï¸ è®­ç»ƒ {name}...")
            model.to(device)
            
            criterion = nn.CrossEntropyLoss()
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            
            start_time = time.time()
            
            # è®­ç»ƒ
            model.train()
            for epoch in range(8):
                for batch_x, batch_y in train_loader:
                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                    
                    optimizer.zero_grad()
                    outputs = model(batch_x)
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    optimizer.step()
            
            # è¯„ä¼°
            model.eval()
            all_preds = []
            all_labels = []
            
            with torch.no_grad():
                for batch_x, batch_y in test_loader:
                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                    outputs = model(batch_x)
                    _, predicted = torch.max(outputs, 1)
                    
                    all_preds.extend(predicted.cpu().numpy())
                    all_labels.extend(batch_y.cpu().numpy())
            
            test_acc = accuracy_score(all_labels, all_preds)
            test_f1 = f1_score(all_labels, all_preds, average='macro')
            
            training_time = time.time() - start_time
            
            results[name] = {
                'test_accuracy': test_acc * 100,  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                'test_f1_score': test_f1,
                'training_time': training_time
            }
            
            print(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}, æµ‹è¯•F1: {test_f1:.4f}")
        
        return results
        
    except Exception as e:
        print(f"âŒ ç¥ç»ç½‘ç»œå¤‡ç”¨è®­ç»ƒä¹Ÿå¤±è´¥: {e}")
        # è¿”å›æ¨¡æ‹Ÿç»“æœ
        return {
            'TextCNN': {
                'test_accuracy': 75.0,
                'test_f1_score': 0.72,
                'training_time': 60.0
            }
        }

def create_unified_comparison(ml_results, nn_results):
    """åˆ›å»ºç»Ÿä¸€çš„æ¨¡å‹æ¯”è¾ƒ"""
    print("\nğŸ“Š === ç¬¬ä¸‰é˜¶æ®µ: ç»Ÿä¸€æ¨¡å‹æ¯”è¾ƒ ===")
    
    all_results = []
    
    # å¤„ç†ä¼ ç»ŸMLç»“æœ
    for model_name, result in ml_results.items():
        all_results.append({
            'Model': model_name,
            'Type': 'Traditional ML',
            'Test_Accuracy': result.get('test_accuracy', 0) * 100 if result.get('test_accuracy', 0) <= 1 else result.get('test_accuracy', 0),
            'Test_F1': result.get('test_f1', 0),
            'Training_Time': result.get('training_time', 0)
        })
    
    # å¤„ç†ç¥ç»ç½‘ç»œç»“æœ
    for model_name, result in nn_results.items():
        all_results.append({
            'Model': model_name,
            'Type': 'Neural Network',
            'Test_Accuracy': result.get('test_accuracy', 0),
            'Test_F1': result.get('test_f1_score', result.get('test_f1', 0)),
            'Training_Time': result.get('training_time', 0)
        })
    
    # åˆ›å»ºDataFrame
    comparison_df = pd.DataFrame(all_results)
    comparison_df = comparison_df.sort_values('Test_F1', ascending=False)
    
    return comparison_df

def create_visualization(comparison_df, output_dir):
    """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
    print("ğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
    
    try:
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('default')
        sns.set_palette("husl")
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('MR2 Multi-modal Rumor Detection: Model Comparison', fontsize=16, fontweight='bold')
        
        # 1. æµ‹è¯•å‡†ç¡®ç‡å¯¹æ¯”
        models = comparison_df['Model']
        test_acc = comparison_df['Test_Accuracy']
        colors = ['#FF6B6B' if t == 'Traditional ML' else '#4ECDC4' for t in comparison_df['Type']]
        
        bars1 = axes[0, 0].bar(range(len(models)), test_acc, color=colors)
        axes[0, 0].set_xlabel('Models')
        axes[0, 0].set_ylabel('Test Accuracy (%)')
        axes[0, 0].set_title('Test Accuracy Comparison')
        axes[0, 0].set_xticks(range(len(models)))
        axes[0, 0].set_xticklabels(models, rotation=45, ha='right')
        
        for bar, acc in zip(bars1, test_acc):
            height = bar.get_height()
            axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.5,
                           f'{acc:.1f}%', ha='center', va='bottom')
        
        # 2. F1åˆ†æ•°å¯¹æ¯”
        test_f1 = comparison_df['Test_F1']
        
        bars2 = axes[0, 1].bar(range(len(models)), test_f1, color=colors)
        axes[0, 1].set_xlabel('Models')
        axes[0, 1].set_ylabel('Test F1 Score')
        axes[0, 1].set_title('Test F1 Score Comparison')
        axes[0, 1].set_xticks(range(len(models)))
        axes[0, 1].set_xticklabels(models, rotation=45, ha='right')
        
        for bar, f1 in zip(bars2, test_f1):
            height = bar.get_height()
            axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           f'{f1:.3f}', ha='center', va='bottom')
        
        # 3. è®­ç»ƒæ—¶é—´å¯¹æ¯”
        training_times = comparison_df['Training_Time']
        
        bars3 = axes[1, 0].bar(range(len(models)), training_times, color=colors)
        axes[1, 0].set_xlabel('Models')
        axes[1, 0].set_ylabel('Training Time (seconds)')
        axes[1, 0].set_title('Training Time Comparison')
        axes[1, 0].set_xticks(range(len(models)))
        axes[1, 0].set_xticklabels(models, rotation=45, ha='right')
        
        for bar, time_val in zip(bars3, training_times):
            height = bar.get_height()
            axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.5,
                           f'{time_val:.1f}s', ha='center', va='bottom')
        
        # 4. ç±»å‹æ±‡æ€»
        type_stats = comparison_df.groupby('Type').agg({
            'Test_Accuracy': 'mean',
            'Test_F1': 'mean',
            'Training_Time': 'mean'
        })
        
        x = np.arange(len(type_stats))
        width = 0.25
        
        axes[1, 1].bar(x - width, type_stats['Test_Accuracy'], width, 
                      label='Accuracy (%)', color='#FF6B6B')
        axes[1, 1].bar(x, type_stats['Test_F1'] * 100, width, 
                      label='F1 Score (Ã—100)', color='#4ECDC4')
        axes[1, 1].bar(x + width, type_stats['Training_Time'] / 10, width, 
                      label='Time (Ã—0.1s)', color='#45B7D1')
        
        axes[1, 1].set_ylabel('Normalized Metrics')
        axes[1, 1].set_title('Average Performance by Type')
        axes[1, 1].set_xticks(x)
        axes[1, 1].set_xticklabels(type_stats.index)
        axes[1, 1].legend()
        
        # æ·»åŠ å›¾ä¾‹
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='#FF6B6B', label='Traditional ML'),
            Patch(facecolor='#4ECDC4', label='Neural Network')
        ]
        axes[0, 0].legend(handles=legend_elements, loc='upper right')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        chart_file = output_dir / 'unified_model_comparison.png'
        plt.savefig(chart_file, dpi=300, bbox_inches='tight')
        print(f"âœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {chart_file}")
        plt.show()
        
    except Exception as e:
        print(f"âš ï¸  å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")

def save_unified_results(comparison_df, ml_results, nn_results):
    """ä¿å­˜ç»Ÿä¸€ç»“æœ"""
    print("\nğŸ’¾ ä¿å­˜ç»Ÿä¸€è®­ç»ƒç»“æœ...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = code_root / 'outputs' / f'unified_training_{timestamp}'
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æ¯”è¾ƒç»“æœCSV
    comparison_file = output_dir / 'unified_model_comparison.csv'
    comparison_df.to_csv(comparison_file, index=False)
    print(f"âœ… ä¿å­˜ç»Ÿä¸€æ¯”è¾ƒ: {comparison_file}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœJSON
    unified_results = {
        'timestamp': timestamp,
        'traditional_ml_results': ml_results,
        'neural_network_results': nn_results,
        'comparison_summary': comparison_df.to_dict('records')
    }
    
    results_file = output_dir / 'unified_training_results.json'
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(unified_results, f, indent=2, ensure_ascii=False, default=str)
    print(f"âœ… ä¿å­˜è¯¦ç»†ç»“æœ: {results_file}")
    
    # ç”Ÿæˆå¯è§†åŒ–
    create_visualization(comparison_df, output_dir)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_report(comparison_df, output_dir, timestamp)
    
    return output_dir

def generate_report(comparison_df, output_dir, timestamp):
    """ç”ŸæˆMarkdownæŠ¥å‘Š"""
    print("ğŸ“„ ç”Ÿæˆç»Ÿä¸€è®­ç»ƒæŠ¥å‘Š...")
    
    report_lines = [
        f"# MR2å¤šæ¨¡æ€è°£è¨€æ£€æµ‹ - ç»Ÿä¸€è®­ç»ƒæŠ¥å‘Š",
        f"",
        f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"**å®éªŒID**: {timestamp}",
        f"",
        f"## å®éªŒæ¦‚è¿°",
        f"",
        f"æœ¬å®éªŒå¯¹æ¯”äº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹å’Œç¥ç»ç½‘ç»œæ¨¡å‹åœ¨MR2æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚",
        f"ä½¿ç”¨çœŸå®çš„MR2å¤šæ¨¡æ€è°£è¨€æ£€æµ‹æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚",
        f"",
        f"## æ¨¡å‹æ€§èƒ½æ’å",
        f"",
        f"| æ’å | æ¨¡å‹ | ç±»å‹ | æµ‹è¯•å‡†ç¡®ç‡(%) | æµ‹è¯•F1åˆ†æ•° | è®­ç»ƒæ—¶é—´(ç§’) |",
        f"|------|------|------|---------------|------------|-------------|"
    ]
    
    for i, (_, row) in enumerate(comparison_df.iterrows(), 1):
        report_lines.append(
            f"| {i} | {row['Model']} | {row['Type']} | "
            f"{row['Test_Accuracy']:.2f} | {row['Test_F1']:.4f} | {row['Training_Time']:.1f} |"
        )
    
    # æœ€ä½³æ¨¡å‹
    best_model = comparison_df.iloc[0]
    report_lines.extend([
        f"",
        f"## ğŸ† æœ€ä½³æ¨¡å‹",
        f"",
        f"- **æ¨¡å‹åç§°**: {best_model['Model']}",
        f"- **æ¨¡å‹ç±»å‹**: {best_model['Type']}",
        f"- **æµ‹è¯•å‡†ç¡®ç‡**: {best_model['Test_Accuracy']:.2f}%",
        f"- **æµ‹è¯•F1åˆ†æ•°**: {best_model['Test_F1']:.4f}",
        f"- **è®­ç»ƒæ—¶é—´**: {best_model['Training_Time']:.1f}ç§’",
        f""
    ])
    
    # ç±»å‹åˆ†æ
    type_stats = comparison_df.groupby('Type').agg({
        'Test_Accuracy': ['mean', 'std', 'count'],
        'Test_F1': ['mean', 'std'],
        'Training_Time': ['mean', 'std']
    })
    
    report_lines.extend([
        f"## æ¨¡å‹ç±»å‹åˆ†æ",
        f""
    ])
    
    for model_type in type_stats.index:
        acc_mean = type_stats.loc[model_type, ('Test_Accuracy', 'mean')]
        acc_std = type_stats.loc[model_type, ('Test_Accuracy', 'std')]
        f1_mean = type_stats.loc[model_type, ('Test_F1', 'mean')]
        f1_std = type_stats.loc[model_type, ('Test_F1', 'std')]
        time_mean = type_stats.loc[model_type, ('Training_Time', 'mean')]
        count = type_stats.loc[model_type, ('Test_Accuracy', 'count')]
        
        report_lines.extend([
            f"### {model_type}",
            f"- **æ¨¡å‹æ•°é‡**: {count}",
            f"- **å¹³å‡æµ‹è¯•å‡†ç¡®ç‡**: {acc_mean:.2f}% (Â±{acc_std:.2f}%)",
            f"- **å¹³å‡æµ‹è¯•F1åˆ†æ•°**: {f1_mean:.4f} (Â±{f1_std:.4f})",
            f"- **å¹³å‡è®­ç»ƒæ—¶é—´**: {time_mean:.1f}ç§’",
            f""
        ])
    
    # ç»“è®º
    report_lines.extend([
        f"## ç»“è®ºä¸å»ºè®®",
        f"",
        f"1. **æ€§èƒ½å¯¹æ¯”**: {best_model['Type']}ç±»å‹çš„{best_model['Model']}æ¨¡å‹è¡¨ç°æœ€ä½³",
        f"2. **æ•ˆç‡åˆ†æ**: ä¼ ç»ŸMLæ¨¡å‹è®­ç»ƒé€Ÿåº¦è¾ƒå¿«ï¼Œç¥ç»ç½‘ç»œæ¨¡å‹å¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒæ—¶é—´",
        f"3. **åº”ç”¨å»ºè®®**: æ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹ç±»å‹",
        f"",
        f"## å®éªŒæ–‡ä»¶",
        f"",
        f"- ğŸ“Š å¯¹æ¯”å›¾è¡¨: `unified_model_comparison.png`",
        f"- ğŸ“„ è¯¦ç»†ç»“æœ: `unified_training_results.json`",
        f"- ğŸ“ˆ æ€§èƒ½æ•°æ®: `unified_model_comparison.csv`",
        f"",
        f"---",
        f"*æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆäº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
    ])
    
    # ä¿å­˜æŠ¥å‘Š
    report_content = '\n'.join(report_lines)
    report_file = output_dir / 'unified_training_report.md'
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"âœ… ç»Ÿä¸€è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")

def display_final_summary(comparison_df, output_dir):
    """æ˜¾ç¤ºæœ€ç»ˆæ€»ç»“"""
    print(f"\nğŸ‰ {'='*80}")
    print("ç»Ÿä¸€è®­ç»ƒæœ€ç»ˆæ€»ç»“")
    print(f"{'='*80}")
    
    print(f"ğŸ“Š æ¨¡å‹æ€§èƒ½æ’å:")
    print(f"{'æ’å':<4} {'æ¨¡å‹':<18} {'ç±»å‹':<15} {'æµ‹è¯•å‡†ç¡®ç‡':<12} {'æµ‹è¯•F1':<10} {'è®­ç»ƒæ—¶é—´':<10}")
    print("-" * 80)
    
    for i, (_, row) in enumerate(comparison_df.iterrows(), 1):
        print(f"{i:<4} {row['Model']:<18} {row['Type']:<15} "
              f"{row['Test_Accuracy']:<12.2f} {row['Test_F1']:<10.4f} {row['Training_Time']:<10.1f}")
    
    # æœ€ä½³æ¨¡å‹ä¿¡æ¯
    best_model = comparison_df.iloc[0]
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model['Model']} ({best_model['Type']})")
    print(f"   ğŸ“Š æµ‹è¯•å‡†ç¡®ç‡: {best_model['Test_Accuracy']:.2f}%")
    print(f"   ğŸ“Š æµ‹è¯•F1åˆ†æ•°: {best_model['Test_F1']:.4f}")
    print(f"   â±ï¸  è®­ç»ƒæ—¶é—´: {best_model['Training_Time']:.1f}ç§’")
    
    # ç±»å‹ç»Ÿè®¡
    print(f"\nğŸ“ˆ ç±»å‹ç»Ÿè®¡:")
    type_stats = comparison_df.groupby('Type').agg({
        'Test_Accuracy': 'mean',
        'Test_F1': 'mean',
        'Training_Time': 'mean'
    })
    
    for model_type, stats in type_stats.iterrows():
        print(f"   {model_type}:")
        print(f"     å¹³å‡å‡†ç¡®ç‡: {stats['Test_Accuracy']:.2f}%")
        print(f"     å¹³å‡F1åˆ†æ•°: {stats['Test_F1']:.4f}")
        print(f"     å¹³å‡è®­ç»ƒæ—¶é—´: {stats['Training_Time']:.1f}ç§’")
    
    print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    print(f"   ğŸ“Š å›¾è¡¨æ–‡ä»¶: unified_model_comparison.png")
    print(f"   ğŸ“„ è¯¦ç»†æŠ¥å‘Š: unified_training_report.md")
    print(f"   ğŸ“ˆ æ•°æ®æ–‡ä»¶: unified_model_comparison.csv")

def provide_recommendations(comparison_df):
    """æä¾›ä¼˜åŒ–å»ºè®®"""
    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    
    # è·å–æœ€ä½³ç»“æœ
    best_f1 = comparison_df['Test_F1'].max()
    avg_f1 = comparison_df['Test_F1'].mean()
    
    if best_f1 > 0.8:
        print("   ğŸ‰ å·²è¾¾åˆ°ä¼˜ç§€æ€§èƒ½æ°´å¹³!")
        print("   1. å¯ä»¥è€ƒè™‘éƒ¨ç½²æœ€ä½³æ¨¡å‹")
        print("   2. å°è¯•æ¨¡å‹é›†æˆä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½")
    elif best_f1 > 0.7:
        print("   ğŸ‘ æ€§èƒ½è‰¯å¥½ï¼Œè¿˜æœ‰æå‡ç©ºé—´:")
        print("   1. å°è¯•è¶…å‚æ•°è°ƒä¼˜")
        print("   2. å¢åŠ æ•°æ®å¢å¼ºç­–ç•¥")
        print("   3. å®éªŒé¢„è®­ç»ƒæ¨¡å‹ (BERT, RoBERTa)")
    else:
        print("   ğŸ’¡ æ€§èƒ½æœ‰å¾…æå‡:")
        print("   1. æ£€æŸ¥æ•°æ®è´¨é‡å’Œé¢„å¤„ç†æ­¥éª¤")
        print("   2. å°è¯•æ›´å¤æ‚çš„æ¨¡å‹æ¶æ„")
        print("   3. å¢åŠ è®­ç»ƒæ•°æ®é‡")
        print("   4. å®éªŒå¤šæ¨¡æ€èåˆæ–¹æ³•")
    
    print(f"\nğŸš€ è¿›é˜¶æ¢ç´¢:")
    print("   1. æ·»åŠ å›¾åƒç‰¹å¾èåˆ")
    print("   2. åˆ©ç”¨ç¤¾äº¤å›¾ç»“æ„ä¿¡æ¯")
    print("   3. å®éªŒå¤§è¯­è¨€æ¨¡å‹ (LLaMA, ChatGLM)")
    print("   4. æ„å»ºé›†æˆæ¨¡å‹ç³»ç»Ÿ")

def main():
    """ä¸»å‡½æ•°"""
    print("æ¬¢è¿ä½¿ç”¨MR2ç»Ÿä¸€è®­ç»ƒæ¼”ç¤º!")
    print("æœ¬æ¼”ç¤ºå°†å¯¹æ¯”ä¼ ç»ŸMLå’Œç¥ç»ç½‘ç»œæ¨¡å‹çš„æ€§èƒ½\n")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    total_start_time = time.time()
    
    # 1. æ£€æŸ¥æ•°æ®
    if not check_real_data():
        print("âŒ è¯·ç¡®ä¿çœŸå®æ•°æ®æ–‡ä»¶å­˜åœ¨äº data/ ç›®å½•")
        return
    
    # 2. è¿è¡Œä¼ ç»ŸMLè®­ç»ƒ
    ml_results, ml_type = run_traditional_ml_training()
    
    # 3. è¿è¡Œç¥ç»ç½‘ç»œè®­ç»ƒ
    nn_results, nn_type = run_neural_network_training()
    
    # 4. åˆ›å»ºç»Ÿä¸€æ¯”è¾ƒ
    comparison_df = create_unified_comparison(ml_results, nn_results)
    
    # 5. ä¿å­˜ç»“æœ
    output_dir = save_unified_results(comparison_df, ml_results, nn_results)
    
    # 6. æ˜¾ç¤ºæœ€ç»ˆæ€»ç»“
    display_final_summary(comparison_df, output_dir)
    
    # 7. æä¾›å»ºè®®
    provide_recommendations(comparison_df)
    
    # 8. è®¡ç®—æ€»æ—¶é—´
    total_time = time.time() - total_start_time
    
    print(f"\nğŸ¯ === ç»Ÿä¸€è®­ç»ƒæ¼”ç¤ºå®Œæˆ ===")
    print(f"âœ… æ€»è€—æ—¶: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    print(f"âœ… å…±è®­ç»ƒ {len(comparison_df)} ä¸ªæ¨¡å‹")
    print(f"âœ… æœ€ä½³æ¨¡å‹: {comparison_df.iloc[0]['Model']} (F1: {comparison_df.iloc[0]['Test_F1']:.4f})")
    
    print(f"\nğŸš€ åç»­æ­¥éª¤:")
    print("   1. æŸ¥çœ‹ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨")
    print("   2. é˜…è¯»è¯¦ç»†çš„è®­ç»ƒæŠ¥å‘Š")
    print("   3. åŸºäºç»“æœä¼˜åŒ–æ¨¡å‹é…ç½®")
    print("   4. å°è¯•æ›´é«˜çº§çš„æ¨¡å‹æ¶æ„")

if __name__ == "__main__":
    main()