#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# models/multimodal/vision_language_models.py

"""
å¤šæ¨¡æ€è§†è§‰-è¯­è¨€æ¨¡å‹å®ç° - ä¿®å¤ç‰ˆæœ¬
ä¿®å¤æ•°æ®åŠ è½½å™¨è°ƒç”¨é—®é¢˜
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from torchvision import transforms
from PIL import Image
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, classification_report
from tqdm import tqdm
import json
from typing import Dict, List, Tuple, Any, Optional, Union
from pathlib import Path
import sys
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
code_root = current_file.parent.parent.parent
sys.path.append(str(code_root))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from datasets.data_loaders import create_all_dataloaders
    from utils.config_manager import get_config_manager, get_output_path
    from preprocessing.text_processing import TextProcessor
    from preprocessing.image_processing import ImageProcessor
    USE_PROJECT_MODULES = True
    print("âœ… æˆåŠŸå¯¼å…¥é¡¹ç›®æ¨¡å—")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥é¡¹ç›®æ¨¡å—å¤±è´¥: {e}")
    USE_PROJECT_MODULES = False

# å°è¯•å¯¼å…¥CLIP
try:
    import clip
    HAS_CLIP = True
    print("âœ… CLIPå¯ç”¨")
except ImportError:
    print("âš ï¸  CLIPæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–å®ç°")
    HAS_CLIP = False

# å°è¯•å¯¼å…¥transformersç”¨äºBLIP
try:
    from transformers import BlipProcessor, BlipForImageTextRetrieval
    HAS_BLIP = True
    print("âœ… BLIPå¯ç”¨")
except ImportError:
    print("âš ï¸  BLIPä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CLIPæˆ–ç®€åŒ–å®ç°")
    HAS_BLIP = False

import logging
logger = logging.getLogger(__name__)


class MultiModalDataset(Dataset):
    """å¤šæ¨¡æ€æ•°æ®é›†ç±»"""
    
    def __init__(self, texts: List[str], image_paths: List[str], labels: List[int], 
                 text_processor, image_processor, max_text_length: int = 77):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€æ•°æ®é›†
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨
            text_processor: æ–‡æœ¬å¤„ç†å™¨
            image_processor: å›¾åƒå¤„ç†å™¨
            max_text_length: æœ€å¤§æ–‡æœ¬é•¿åº¦
        """
        self.texts = texts
        self.image_paths = image_paths
        self.labels = labels
        self.text_processor = text_processor
        self.image_processor = image_processor
        self.max_text_length = max_text_length
        
        # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´
        min_length = min(len(texts), len(image_paths), len(labels))
        self.texts = texts[:min_length]
        self.image_paths = image_paths[:min_length]
        self.labels = labels[:min_length]
        
        print(f"ğŸ“Š å¤šæ¨¡æ€æ•°æ®é›†åˆå§‹åŒ–: {len(self.texts)} æ ·æœ¬")
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        image_path = self.image_paths[idx]
        label = self.labels[idx]
        
        # å¤„ç†æ–‡æœ¬
        if hasattr(self.text_processor, 'tokenize') and callable(self.text_processor.tokenize):
            # ä½¿ç”¨CLIP tokenizer
            text_tokens = self.text_processor.tokenize(text)
        else:
            # ç®€å•æ–‡æœ¬å¤„ç†
            words = text.split()[:self.max_text_length]
            text_tokens = torch.zeros(self.max_text_length, dtype=torch.long)
            for i, word in enumerate(words):
                text_tokens[i] = hash(word) % 30000  # ç®€å•hashç¼–ç 
        
        # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†çœŸå®å›¾åƒè·¯å¾„
        try:
            if image_path and Path(image_path).exists():
                # æœ‰çœŸå®å›¾åƒæ–‡ä»¶
                if hasattr(self.image_processor, 'process_single_image'):
                    # ä½¿ç”¨é¡¹ç›®çš„å›¾åƒå¤„ç†å™¨
                    image_tensor = self.image_processor.process_single_image(
                        image_path, transform_type='val'
                    )
                    if image_tensor is None:
                        # å¤„ç†å¤±è´¥ï¼Œåˆ›å»ºç©ºå›¾åƒtensor
                        image_tensor = torch.zeros(3, 224, 224)
                else:
                    # ä½¿ç”¨ç®€å•å›¾åƒå¤„ç†
                    image = Image.open(image_path).convert('RGB')
                    transform = transforms.Compose([
                        transforms.Resize((224, 224)),
                        transforms.ToTensor(),
                        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                           std=[0.229, 0.224, 0.225])
                    ])
                    image_tensor = transform(image)
            else:
                # æ²¡æœ‰å›¾åƒæ–‡ä»¶æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºtensor
                image_tensor = torch.zeros(3, 224, 224)
                
        except Exception as e:
            # å¤„ç†ä»»ä½•å›¾åƒåŠ è½½é”™è¯¯
            print(f"âš ï¸  åŠ è½½å›¾åƒå¤±è´¥ {image_path}: {e}")
            image_tensor = torch.zeros(3, 224, 224)
        
        return {
            'text': text_tokens,
            'image': image_tensor,
            'labels': torch.tensor(label, dtype=torch.long),
            'text_raw': text,
            'image_path': str(image_path) if image_path else ""
        }

class SimpleCLIPModel(nn.Module):
    """ç®€åŒ–çš„CLIPé£æ ¼æ¨¡å‹"""
    
    def __init__(self, vocab_size: int = 30000, text_embed_dim: int = 512, 
                 image_embed_dim: int = 512, projection_dim: int = 256, 
                 num_classes: int = 3):
        """
        åˆå§‹åŒ–ç®€åŒ–CLIPæ¨¡å‹
        
        Args:
            vocab_size: è¯æ±‡è¡¨å¤§å°
            text_embed_dim: æ–‡æœ¬åµŒå…¥ç»´åº¦
            image_embed_dim: å›¾åƒåµŒå…¥ç»´åº¦
            projection_dim: æŠ•å½±ç»´åº¦
            num_classes: åˆ†ç±»æ•°é‡
        """
        super(SimpleCLIPModel, self).__init__()
        
        # æ–‡æœ¬ç¼–ç å™¨
        self.text_embedding = nn.Embedding(vocab_size, text_embed_dim)
        self.text_transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(text_embed_dim, 8, dim_feedforward=2048),
            num_layers=6
        )
        self.text_projection = nn.Linear(text_embed_dim, projection_dim)
        
        # å›¾åƒç¼–ç å™¨ï¼ˆç®€åŒ–çš„CNNï¼‰
        self.image_encoder = nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((7, 7)),
            nn.Flatten(),
            nn.Linear(256 * 7 * 7, image_embed_dim),
            nn.ReLU()
        )
        self.image_projection = nn.Linear(image_embed_dim, projection_dim)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(projection_dim * 2, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, num_classes)
        )
        
        # æ¸©åº¦å‚æ•°ï¼ˆç”¨äºå¯¹æ¯”å­¦ä¹ ï¼‰
        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
    
    def encode_text(self, text):
        """ç¼–ç æ–‡æœ¬"""
        # æ–‡æœ¬åµŒå…¥
        x = self.text_embedding(text)  # [batch, seq_len, embed_dim]
        x = x.transpose(0, 1)  # [seq_len, batch, embed_dim]
        
        # Transformerç¼–ç 
        x = self.text_transformer(x)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = x.mean(dim=0)  # [batch, embed_dim]
        
        # æŠ•å½±
        x = self.text_projection(x)
        
        return F.normalize(x, dim=-1)
    
    def encode_image(self, image):
        """ç¼–ç å›¾åƒ"""
        x = self.image_encoder(image)
        x = self.image_projection(x)
        return F.normalize(x, dim=-1)
    
    def forward(self, text, image):
        """å‰å‘ä¼ æ’­"""
        text_features = self.encode_text(text)
        image_features = self.encode_image(image)
        
        # èåˆç‰¹å¾
        fused_features = torch.cat([text_features, image_features], dim=1)
        
        # åˆ†ç±»
        logits = self.classifier(fused_features)
        
        return logits, text_features, image_features


class CLIPBasedClassifier(nn.Module):
    """åŸºäºå®˜æ–¹CLIPçš„åˆ†ç±»å™¨"""
    
    def __init__(self, clip_model_name: str = "ViT-B/32", num_classes: int = 3):
        """
        åˆå§‹åŒ–CLIPåˆ†ç±»å™¨
        
        Args:
            clip_model_name: CLIPæ¨¡å‹åç§°
            num_classes: åˆ†ç±»æ•°é‡
        """
        super(CLIPBasedClassifier, self).__init__()
        
        if HAS_CLIP:
            # åŠ è½½é¢„è®­ç»ƒCLIPæ¨¡å‹
            self.clip_model, self.clip_preprocess = clip.load(clip_model_name, device="cpu")
            
            # å†»ç»“CLIPå‚æ•°
            for param in self.clip_model.parameters():
                param.requires_grad = False
            
            # è·å–ç‰¹å¾ç»´åº¦
            with torch.no_grad():
                dummy_text = clip.tokenize(["hello world"])
                dummy_image = torch.randn(1, 3, 224, 224)
                text_features = self.clip_model.encode_text(dummy_text).float()
                image_features = self.clip_model.encode_image(dummy_image).float()
                feature_dim = text_features.shape[-1] + image_features.shape[-1]
            
            # åˆ†ç±»å¤´
            self.classifier = nn.Sequential(
                nn.Linear(feature_dim, 512),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(512, num_classes)
            )
            
            self.use_clip = True
            print("âœ… ä½¿ç”¨å®˜æ–¹CLIPæ¨¡å‹")
        else:
            # ä½¿ç”¨ç®€åŒ–å®ç°
            self.clip_model = SimpleCLIPModel(num_classes=num_classes)
            self.use_clip = False
            print("âš ï¸  ä½¿ç”¨ç®€åŒ–CLIPå®ç°")
    
    def forward(self, text, image):
        """å‰å‘ä¼ æ’­"""
        if self.use_clip and HAS_CLIP:
            # ä½¿ç”¨å®˜æ–¹CLIP
            with torch.no_grad():
                text_features = self.clip_model.encode_text(text).float()
                image_features = self.clip_model.encode_image(image).float()
            
            # èåˆç‰¹å¾
            fused_features = torch.cat([text_features, image_features], dim=1)
            logits = self.classifier(fused_features)
            
            return logits
        else:
            # ä½¿ç”¨ç®€åŒ–å®ç°
            return self.clip_model(text, image)[0]


class MultiModalTrainer:
    """å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, data_dir: str = "data", device: str = "auto"):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
        """
        self.data_dir = data_dir
        
        # è®¾ç½®è®¾å¤‡
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.models = {}
        self.results = {}
        
        # åˆå§‹åŒ–å¤„ç†å™¨
        if USE_PROJECT_MODULES:
            self.text_processor = TextProcessor(language='mixed')
            self.image_processor = ImageProcessor(target_size=(224, 224))
        else:
            self.text_processor = None
            self.image_processor = None
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if USE_PROJECT_MODULES:
            config_manager = get_config_manager()
            self.output_dir = get_output_path('models', 'multimodal')
        else:
            self.output_dir = Path('outputs/models/multimodal')
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ ‡ç­¾æ˜ å°„
        self.label_mapping = {0: 'Non-rumor', 1: 'Rumor', 2: 'Unverified'}
        
        print(f"ğŸ­ å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ•°æ®ç›®å½•: {self.data_dir}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_data(self) -> Dict[str, Tuple[List[str], List[str], List[int]]]:
        """åŠ è½½MR2å¤šæ¨¡æ€æ•°æ®é›† - ä¿®å¤ç‰ˆæœ¬ï¼Œä½¿ç”¨çœŸå®å›¾åƒè·¯å¾„"""
        print("ğŸ“š åŠ è½½MR2å¤šæ¨¡æ€æ•°æ®é›†...")
        
        if USE_PROJECT_MODULES:
            try:
                # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å‡½æ•°è°ƒç”¨æ–¹å¼
                dataloaders = create_all_dataloaders(
                    batch_sizes={'train': 16, 'val': 16, 'test': 16}
                )
                
                data = {}
                for split, dataloader in dataloaders.items():
                    texts = []
                    image_paths = []
                    labels = []
                    
                    for batch in dataloader:
                        # æå–æ–‡æœ¬
                        if 'text' in batch:
                            texts.extend(batch['text'])
                        elif 'caption' in batch:
                            texts.extend(batch['caption'])
                        
                        # ä¿®å¤ï¼šæå–çœŸå®çš„å›¾åƒè·¯å¾„
                        if 'image_path' in batch:
                            # æ„å»ºå®Œæ•´çš„å›¾åƒè·¯å¾„
                            batch_image_paths = []
                            for img_path in batch['image_path']:
                                if img_path:  # ç¡®ä¿è·¯å¾„ä¸ä¸ºç©º
                                    # æ„å»ºç›¸å¯¹äºæ•°æ®ç›®å½•çš„å®Œæ•´è·¯å¾„
                                    full_path = Path(self.data_dir) / img_path
                                    batch_image_paths.append(str(full_path))
                                else:
                                    # å¦‚æœæ²¡æœ‰å›¾åƒè·¯å¾„ï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²ï¼Œåç»­å¤„ç†æ—¶ä¼šåˆ›å»ºç©ºtensor
                                    batch_image_paths.append("")
                            image_paths.extend(batch_image_paths)
                        else:
                            # å¦‚æœbatchä¸­æ²¡æœ‰image_pathï¼Œä¸ºæ¯ä¸ªæ–‡æœ¬åˆ›å»ºç©ºè·¯å¾„
                            text_count = len(batch.get('text', batch.get('caption', [])))
                            image_paths.extend([""] * text_count)
                        
                        # æå–æ ‡ç­¾
                        if 'labels' in batch:
                            labels.extend(batch['labels'].tolist())
                        elif 'label' in batch:
                            labels.extend(batch['label'])
                    
                    data[split] = (texts, image_paths, labels)
                    print(f"âœ… åŠ è½½ {split}: {len(texts)} æ ·æœ¬")
                    
                    # ç»Ÿè®¡æœ‰æ•ˆå›¾åƒæ•°é‡
                    valid_images = sum(1 for path in image_paths if path and Path(path).exists())
                    print(f"   æœ‰æ•ˆå›¾åƒ: {valid_images}/{len(image_paths)}")
                
                return data
                
            except Exception as e:
                print(f"âŒ ä½¿ç”¨é¡¹ç›®æ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
                return self._create_demo_data()
        else:
            return self._create_demo_data()

    def _create_demo_data(self) -> Dict[str, Tuple[List[str], List[str], List[int]]]:
        """åˆ›å»ºæ¼”ç¤ºæ•°æ® - ä¿®å¤ç‰ˆæœ¬ï¼Œä¸åˆ›å»ºè™šæ‹Ÿå›¾åƒè·¯å¾„"""
        print("ğŸ”§ åˆ›å»ºå¤šæ¨¡æ€æ¼”ç¤ºæ•°æ®...")
        print("âš ï¸  æ³¨æ„ï¼šç”±äºæ²¡æœ‰çœŸå®æ•°æ®ï¼Œå°†ä½¿ç”¨ç©ºå›¾åƒè·¯å¾„")
        
        demo_texts = [
            "è¿™æ˜¯ä¸€ä¸ªå…³äºç§‘æŠ€è¿›æ­¥çš„çœŸå®æ–°é—»æŠ¥é“",
            "This is fake news about celebrity scandal",
            "æœªç»è¯å®çš„ä¼ è¨€éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥éªŒè¯",
            "Breaking: Major breakthrough in AI technology",
            "ç½‘ä¼ æŸåœ°å‘ç”Ÿé‡å¤§äº‹æ•…ï¼Œå®˜æ–¹å°šæœªç¡®è®¤",
            "Scientists discover new species in deep ocean",
            "è°£ä¼ æŸå…¬å¸å€’é—­ï¼Œå®é™…æƒ…å†µæœ‰å¾…æ ¸å®",
            "Weather alert: Severe storm approaching",
            "ç¤¾äº¤åª’ä½“æµä¼ çš„æœªè¯å®æ¶ˆæ¯å¼•å‘å…³æ³¨",
            "Economic indicators show positive trends"
        ]
        
        demo_labels = [0, 1, 2, 0, 2, 0, 1, 0, 2, 0]
        
        # ä¿®å¤ï¼šä¸åˆ›å»ºè™šæ‹Ÿå›¾åƒè·¯å¾„ï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
        # è¿™æ ·åœ¨å®é™…å¤„ç†æ—¶ä¼šåˆ›å»ºç©ºçš„tensorï¼Œè€Œä¸æ˜¯å°è¯•åŠ è½½ä¸å­˜åœ¨çš„æ–‡ä»¶
        demo_image_paths = [""] * len(demo_texts)
        
        # æ‰©å±•æ•°æ®
        extended_texts = demo_texts * 8
        extended_image_paths = demo_image_paths * 8
        extended_labels = demo_labels * 8
        
        # æŒ‰æ¯”ä¾‹åˆ†å‰²æ•°æ®
        total_size = len(extended_texts)
        train_size = int(0.7 * total_size)
        val_size = int(0.15 * total_size)
        
        print(f"ğŸ“ åˆ›å»ºæ¼”ç¤ºæ•°æ®: {total_size} ä¸ªæ ·æœ¬ï¼ˆæ— å›¾åƒï¼‰")
        
        return {
            'train': (extended_texts[:train_size], 
                     extended_image_paths[:train_size],
                     extended_labels[:train_size]),
            'val': (extended_texts[train_size:train_size+val_size], 
                   extended_image_paths[train_size:train_size+val_size],
                   extended_labels[train_size:train_size+val_size]),
            'test': (extended_texts[train_size+val_size:], 
                    extended_image_paths[train_size+val_size:],
                    extended_labels[train_size+val_size:])
        }

    def create_models(self):
        """åˆ›å»ºå¤šæ¨¡æ€æ¨¡å‹"""
        print("ğŸ­ åˆ›å»ºå¤šæ¨¡æ€æ¨¡å‹...")
        
        # CLIPæ¨¡å‹
        self.models['clip'] = CLIPBasedClassifier(
            clip_model_name="ViT-B/32",
            num_classes=3
        ).to(self.device)
        
        # ç®€åŒ–å¤šæ¨¡æ€æ¨¡å‹
        self.models['simple_multimodal'] = SimpleCLIPModel(
            vocab_size=30000,
            text_embed_dim=256,
            image_embed_dim=256,
            projection_dim=128,
            num_classes=3
        ).to(self.device)
        
        print(f"âœ… åˆ›å»ºäº† {len(self.models)} ä¸ªå¤šæ¨¡æ€æ¨¡å‹")
        
        # æ‰“å°æ¨¡å‹å‚æ•°é‡
        for name, model in self.models.items():
            param_count = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"   {name}: {param_count:,} å‚æ•° ({trainable_params:,} å¯è®­ç»ƒ)")
    
    def train_single_model(self, model_name: str, train_loader: DataLoader, 
                          val_loader: DataLoader, epochs: int = 5, 
                          learning_rate: float = 1e-4) -> Dict[str, Any]:
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        print(f"ğŸ‹ï¸ è®­ç»ƒ {model_name} æ¨¡å‹...")
        
        model = self.models[model_name]
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        
        train_losses = []
        val_accuracies = []
        best_val_acc = 0
        best_model_state = None
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0
            
            train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]", leave=False)
            for batch in train_pbar:
                text = batch['text'].to(self.device)
                image = batch['image'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                optimizer.zero_grad()
                
                # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒç”¨ä¸åŒçš„å‰å‘ä¼ æ’­
                if isinstance(model, CLIPBasedClassifier):
                    logits = model(text, image)
                else:
                    logits = model(text, image)[0]  # SimpleCLIPModelè¿”å›å¤šä¸ªå€¼
                
                loss = criterion(logits, labels)
                
                loss.backward()
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = torch.max(logits.data, 1)
                train_total += labels.size(0)
                train_correct += (predicted == labels).sum().item()
                
                train_pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{100*train_correct/train_total:.2f}%'
                })
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_correct = 0
            val_total = 0
            all_preds = []
            all_labels = []
            
            with torch.no_grad():
                for batch in val_loader:
                    text = batch['text'].to(self.device)
                    image = batch['image'].to(self.device)
                    labels = batch['labels'].to(self.device)
                    
                    if isinstance(model, CLIPBasedClassifier):
                        logits = model(text, image)
                    else:
                        logits = model(text, image)[0]
                    
                    _, predicted = torch.max(logits.data, 1)
                    
                    val_total += labels.size(0)
                    val_correct += (predicted == labels).sum().item()
                    
                    all_preds.extend(predicted.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())
            
            train_acc = 100 * train_correct / train_total
            val_acc = 100 * val_correct / val_total
            avg_train_loss = train_loss / len(train_loader)
            
            train_losses.append(avg_train_loss)
            val_accuracies.append(val_acc)
            
            print(f"Epoch {epoch+1}/{epochs}: "
                  f"Train Loss: {avg_train_loss:.4f}, "
                  f"Train Acc: {train_acc:.2f}%, "
                  f"Val Acc: {val_acc:.2f}%")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_model_state = model.state_dict().copy()
        
        # æ¢å¤æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
        
        # è®¡ç®—æœ€ç»ˆF1åˆ†æ•°
        val_f1 = f1_score(all_labels, all_preds, average='macro')
        
        result = {
            'model_name': model_name,
            'best_val_accuracy': best_val_acc,
            'val_f1_score': val_f1,
            'train_losses': train_losses,
            'val_accuracies': val_accuracies,
            'epochs_trained': epochs
        }
        
        print(f"âœ… {model_name} è®­ç»ƒå®Œæˆ:")
        print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
        print(f"   éªŒè¯F1åˆ†æ•°: {val_f1:.4f}")
        
        return result
    
    def evaluate_model(self, model_name: str, test_loader: DataLoader) -> Dict[str, Any]:
        """è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½"""
        print(f"ğŸ“Š è¯„ä¼° {model_name} æ¨¡å‹...")
        
        model = self.models[model_name]
        model.eval()
        
        test_correct = 0
        test_total = 0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="Testing"):
                text = batch['text'].to(self.device)
                image = batch['image'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                if isinstance(model, CLIPBasedClassifier):
                    logits = model(text, image)
                else:
                    logits = model(text, image)[0]
                
                _, predicted = torch.max(logits.data, 1)
                
                test_total += labels.size(0)
                test_correct += (predicted == labels).sum().item()
                
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        test_acc = 100 * test_correct / test_total
        test_f1 = f1_score(all_labels, all_preds, average='macro')
        
        # åˆ†ç±»æŠ¥å‘Š
        report = classification_report(
            all_labels, all_preds,
            target_names=list(self.label_mapping.values()),
            output_dict=True
        )
        
        result = {
            'test_accuracy': test_acc,
            'test_f1_score': test_f1,
            'classification_report': report
        }
        
        print(f"âœ… {model_name} æµ‹è¯•ç»“æœ:")
        print(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
        print(f"   æµ‹è¯•F1åˆ†æ•°: {test_f1:.4f}")
        
        return result
    
    def train_all_models(self, epochs: int = 5, batch_size: int = 8, learning_rate: float = 1e-4):
        """è®­ç»ƒæ‰€æœ‰å¤šæ¨¡æ€æ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ‰€æœ‰å¤šæ¨¡æ€æ¨¡å‹...")
        
        # åŠ è½½æ•°æ®
        data = self.load_data()
        
        # åˆ›å»ºæ¨¡å‹
        self.create_models()
        
        # å¤„ç†æ–‡æœ¬çš„tokenizer
        if HAS_CLIP:
            text_processor = clip.tokenize
        else:
            text_processor = self.text_processor
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = MultiModalDataset(
            data['train'][0], data['train'][1], data['train'][2],
            text_processor, self.image_processor
        )
        val_dataset = MultiModalDataset(
            data['val'][0], data['val'][1], data['val'][2],
            text_processor, self.image_processor
        )
        test_dataset = MultiModalDataset(
            data['test'][0], data['test'][1], data['test'][2],
            text_processor, self.image_processor
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
        for model_name in self.models.keys():
            print(f"\n{'='*60}")
            print(f"è®­ç»ƒæ¨¡å‹: {model_name.upper()}")
            print(f"{'='*60}")
            
            try:
                # è®­ç»ƒæ¨¡å‹
                train_result = self.train_single_model(
                    model_name, train_loader, val_loader, epochs, learning_rate
                )
                
                # æµ‹è¯•è¯„ä¼°
                test_result = self.evaluate_model(model_name, test_loader)
                
                # åˆå¹¶ç»“æœ
                self.results[model_name] = {**train_result, **test_result}
                
            except Exception as e:
                print(f"âŒ è®­ç»ƒæ¨¡å‹å¤±è´¥: {model_name}, é”™è¯¯: {e}")
                continue
        
        # ä¿å­˜æ¨¡å‹å’Œç»“æœ
        self.save_models_and_results()
        
        # æ˜¾ç¤ºæœ€ç»ˆå¯¹æ¯”
        self.print_model_comparison()
    
    def save_models_and_results(self):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œç»“æœ"""
        print("\nğŸ’¾ ä¿å­˜æ¨¡å‹å’Œç»“æœ...")
        
        # ä¿å­˜æ¯ä¸ªæ¨¡å‹
        for model_name, model in self.models.items():
            model_file = self.output_dir / f'{model_name}_model.pth'
            torch.save(model.state_dict(), model_file)
            print(f"âœ… ä¿å­˜æ¨¡å‹: {model_file}")
        
        # ä¿å­˜ç»“æœ
        results_file = self.output_dir / 'training_results.json'
        # è½¬æ¢numpyç±»å‹ä¸ºå¯åºåˆ—åŒ–çš„ç±»å‹
        serializable_results = {}
        for model_name, result in self.results.items():
            serializable_result = {}
            for key, value in result.items():
                if isinstance(value, (list, np.ndarray)):
                    serializable_result[key] = list(value) if hasattr(value, 'tolist') else value
                else:
                    serializable_result[key] = value
            serializable_results[model_name] = serializable_result
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        print(f"âœ… ä¿å­˜è®­ç»ƒç»“æœ: {results_file}")
    
    def print_model_comparison(self):
        """æ‰“å°æ¨¡å‹æ¯”è¾ƒç»“æœ"""
        print(f"\nğŸ“Š {'='*70}")
        print("å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
        print(f"{'='*70}")
        
        if not self.results:
            print("âš ï¸  æ²¡æœ‰è®­ç»ƒç»“æœå¯æ˜¾ç¤º")
            return
        
        print(f"{'æ¨¡å‹':<20} {'éªŒè¯å‡†ç¡®ç‡':<10} {'æµ‹è¯•å‡†ç¡®ç‡':<10} {'éªŒè¯F1':<8} {'æµ‹è¯•F1':<8}")
        print("-" * 70)
        
        # æŒ‰æµ‹è¯•F1åˆ†æ•°æ’åº
        sorted_results = sorted(self.results.items(), 
                              key=lambda x: x[1].get('test_f1_score', 0), 
                              reverse=True)
        
        for model_name, result in sorted_results:
            val_acc = result.get('best_val_accuracy', 0)
            test_acc = result.get('test_accuracy', 0)
            val_f1 = result.get('val_f1_score', 0)
            test_f1 = result.get('test_f1_score', 0)
            
            print(f"{model_name:<20} "
                  f"{val_acc:<10.2f} "
                  f"{test_acc:<10.2f} "
                  f"{val_f1:<8.4f} "
                  f"{test_f1:<8.4f}")
        
        if sorted_results:
            best_model = sorted_results[0]
            print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model[0]}")
            print(f"   æµ‹è¯•F1åˆ†æ•°: {best_model[1].get('test_f1_score', 0):.4f}")
            print(f"   æµ‹è¯•å‡†ç¡®ç‡: {best_model[1].get('test_accuracy', 0):.2f}%")


def main():
    """ä¸»å‡½æ•°ï¼Œæ¼”ç¤ºå¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒæµç¨‹"""
    print("ğŸš€ å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒæ¼”ç¤º")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MultiModalTrainer(data_dir="data")
    
    # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    trainer.train_all_models(
        epochs=5,           # è®­ç»ƒè½®æ•°
        batch_size=8,       # å°æ‰¹æ¬¡ï¼Œé€‚åº”æ˜¾å­˜é™åˆ¶
        learning_rate=1e-4  # å¤šæ¨¡æ€æ¨¡å‹å­¦ä¹ ç‡
    )
    
    print("\nâœ… å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒæ¼”ç¤ºå®Œæˆ!")
    print(f"ğŸ“ æ¨¡å‹å’Œç»“æœå·²ä¿å­˜åˆ°: {trainer.output_dir}")


if __name__ == "__main__":
    main()