#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# models/neural_networks/text_models.py

"""
æ–‡æœ¬ç¥ç»ç½‘ç»œæ¨¡å‹å®ç°
åŒ…å«TextCNNã€BiLSTMã€TextRCNNç­‰ç»å…¸æ–‡æœ¬åˆ†ç±»æ¨¡å‹
æ”¯æŒå¤šè¯­è¨€æ–‡æœ¬å’ŒMR2æ•°æ®é›†
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, classification_report
from tqdm import tqdm
import json
import pickle
from typing import Dict, List, Tuple, Any, Optional, Union
from pathlib import Path
import sys
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
code_root = current_file.parent.parent.parent
sys.path.append(str(code_root))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from datasets.data_loaders import create_all_dataloaders
    from utils.config_manager import get_config_manager, get_output_path
    from preprocessing.text_processing import TextProcessor
    USE_PROJECT_MODULES = True
    print("âœ… æˆåŠŸå¯¼å…¥é¡¹ç›®æ¨¡å—")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥é¡¹ç›®æ¨¡å—å¤±è´¥: {e}")
    USE_PROJECT_MODULES = False

import logging
logger = logging.getLogger(__name__)


class TextDataset(Dataset):
    """æ–‡æœ¬æ•°æ®é›†ç±»"""
    
    def __init__(self, texts: List[str], labels: List[int], vocab: Dict[str, int], 
                 max_length: int = 256):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨
            vocab: è¯æ±‡è¡¨
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.texts = texts
        self.labels = labels
        self.vocab = vocab
        self.max_length = max_length
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºç´¢å¼•åºåˆ—
        tokens = text.split()
        indices = [self.vocab.get(token, self.vocab.get('<UNK>', 0)) for token in tokens]
        
        # æˆªæ–­æˆ–å¡«å……
        if len(indices) > self.max_length:
            indices = indices[:self.max_length]
        else:
            indices.extend([self.vocab.get('<PAD>', 0)] * (self.max_length - len(indices)))
        
        return {
            'input_ids': torch.tensor(indices, dtype=torch.long),
            'labels': torch.tensor(label, dtype=torch.long),
            'attention_mask': torch.tensor([1 if i != self.vocab.get('<PAD>', 0) else 0 for i in indices], dtype=torch.long)
        }


class TextCNN(nn.Module):
    """TextCNNæ¨¡å‹"""
    
    def __init__(self, vocab_size: int, embedding_dim: int = 300, 
                 filter_sizes: List[int] = [3, 4, 5], num_filters: int = 100,
                 num_classes: int = 3, dropout: float = 0.5):
        """
        åˆå§‹åŒ–TextCNN
        
        Args:
            vocab_size: è¯æ±‡è¡¨å¤§å°
            embedding_dim: è¯åµŒå…¥ç»´åº¦
            filter_sizes: å·ç§¯æ ¸å°ºå¯¸åˆ—è¡¨
            num_filters: æ¯ç§å°ºå¯¸çš„å·ç§¯æ ¸æ•°é‡
            num_classes: åˆ†ç±»æ•°é‡
            dropout: dropoutæ¦‚ç‡
        """
        super(TextCNN, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        
        # å¤šä¸ªå·ç§¯å±‚
        self.convs = nn.ModuleList([
            nn.Conv1d(embedding_dim, num_filters, kernel_size=k)
            for k in filter_sizes
        ])
        
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)
        
    def forward(self, input_ids, attention_mask=None):
        # è¯åµŒå…¥: (batch_size, seq_len, embedding_dim)
        embedded = self.embedding(input_ids)
        
        # è½¬ç½®ä¸ºå·ç§¯è¾“å…¥æ ¼å¼: (batch_size, embedding_dim, seq_len)
        embedded = embedded.transpose(1, 2)
        
        # å¤šå°ºåº¦å·ç§¯å’Œæ± åŒ–
        conv_outputs = []
        for conv in self.convs:
            # å·ç§¯: (batch_size, num_filters, new_seq_len)
            conv_out = F.relu(conv(embedded))
            # æœ€å¤§æ± åŒ–: (batch_size, num_filters)
            pooled = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)
            conv_outputs.append(pooled)
        
        # æ‹¼æ¥æ‰€æœ‰å·ç§¯ç»“æœ: (batch_size, len(filter_sizes) * num_filters)
        concatenated = torch.cat(conv_outputs, dim=1)
        
        # Dropoutå’Œå…¨è¿æ¥
        output = self.dropout(concatenated)
        logits = self.fc(output)
        
        return logits


class BiLSTM(nn.Module):
    """åŒå‘LSTMæ¨¡å‹"""
    
    def __init__(self, vocab_size: int, embedding_dim: int = 300,
                 hidden_dim: int = 128, num_layers: int = 2,
                 num_classes: int = 3, dropout: float = 0.5):
        """
        åˆå§‹åŒ–BiLSTM
        
        Args:
            vocab_size: è¯æ±‡è¡¨å¤§å°
            embedding_dim: è¯åµŒå…¥ç»´åº¦
            hidden_dim: LSTMéšè—å±‚ç»´åº¦
            num_layers: LSTMå±‚æ•°
            num_classes: åˆ†ç±»æ•°é‡
            dropout: dropoutæ¦‚ç‡
        """
        super(BiLSTM, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(
            embedding_dim, hidden_dim, num_layers,
            bidirectional=True, dropout=dropout if num_layers > 1 else 0,
            batch_first=True
        )
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 å› ä¸ºæ˜¯åŒå‘
        
    def forward(self, input_ids, attention_mask=None):
        # è¯åµŒå…¥: (batch_size, seq_len, embedding_dim)
        embedded = self.embedding(input_ids)
        
        # LSTM: output (batch_size, seq_len, hidden_dim * 2)
        lstm_out, (hidden, cell) = self.lstm(embedded)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºï¼ˆè€ƒè™‘paddingï¼‰
        if attention_mask is not None:
            # è·å–æ¯ä¸ªåºåˆ—çš„å®é™…é•¿åº¦
            lengths = attention_mask.sum(dim=1) - 1  # -1å› ä¸ºç´¢å¼•ä»0å¼€å§‹
            lengths = lengths.clamp(min=0)
            
            # æå–æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªæœ‰æ•ˆè¾“å‡º
            batch_size = lstm_out.size(0)
            last_outputs = lstm_out[range(batch_size), lengths]
        else:
            # å¦‚æœæ²¡æœ‰attention_maskï¼Œä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥
            last_outputs = lstm_out[:, -1, :]
        
        # Dropoutå’Œå…¨è¿æ¥
        output = self.dropout(last_outputs)
        logits = self.fc(output)
        
        return logits


class TextRCNN(nn.Module):
    """Text-RCNNæ¨¡å‹ï¼ˆç»“åˆRNNå’ŒCNNï¼‰"""
    
    def __init__(self, vocab_size: int, embedding_dim: int = 300,
                 hidden_dim: int = 128, num_classes: int = 3, dropout: float = 0.5):
        """
        åˆå§‹åŒ–TextRCNN
        
        Args:
            vocab_size: è¯æ±‡è¡¨å¤§å°
            embedding_dim: è¯åµŒå…¥ç»´åº¦
            hidden_dim: RNNéšè—å±‚ç»´åº¦
            num_classes: åˆ†ç±»æ•°é‡
            dropout: dropoutæ¦‚ç‡
        """
        super(TextRCNN, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)
        
        # ä¸Šä¸‹æ–‡è¡¨ç¤ºçš„çº¿æ€§å˜æ¢
        self.context_weight = nn.Linear(hidden_dim * 2 + embedding_dim, hidden_dim)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim, num_classes)
        
    def forward(self, input_ids, attention_mask=None):
        # è¯åµŒå…¥: (batch_size, seq_len, embedding_dim)
        embedded = self.embedding(input_ids)
        
        # LSTM: (batch_size, seq_len, hidden_dim * 2)
        lstm_out, _ = self.lstm(embedded)
        
        # æ‹¼æ¥è¯åµŒå…¥å’ŒLSTMè¾“å‡º: (batch_size, seq_len, hidden_dim * 2 + embedding_dim)
        combined = torch.cat([embedded, lstm_out], dim=2)
        
        # ä¸Šä¸‹æ–‡è¡¨ç¤º: (batch_size, seq_len, hidden_dim)
        context = torch.tanh(self.context_weight(combined))
        
        # æœ€å¤§æ± åŒ–: (batch_size, hidden_dim)
        pooled = F.max_pool1d(context.transpose(1, 2), context.size(1)).squeeze(2)
        
        # Dropoutå’Œå…¨è¿æ¥
        output = self.dropout(pooled)
        logits = self.fc(output)
        
        return logits


class NeuralTextClassifier:
    """ç¥ç»ç½‘ç»œæ–‡æœ¬åˆ†ç±»å™¨è®­ç»ƒå™¨"""
    
    def __init__(self, data_dir: str = "data", device: str = "auto"):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
        """
        self.data_dir = data_dir
        
        # è®¾ç½®è®¾å¤‡
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.vocab = {}
        self.models = {}
        self.results = {}
        
        # åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨
        if USE_PROJECT_MODULES:
            self.text_processor = TextProcessor(language='mixed')
        else:
            self.text_processor = None
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if USE_PROJECT_MODULES:
            config_manager = get_config_manager()
            self.output_dir = get_output_path('models', 'neural_networks')
        else:
            self.output_dir = Path('outputs/models/neural_networks')
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ ‡ç­¾æ˜ å°„
        self.label_mapping = {0: 'Non-rumor', 1: 'Rumor', 2: 'Unverified'}
        
        print(f"ğŸ§  ç¥ç»ç½‘ç»œåˆ†ç±»å™¨è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ•°æ®ç›®å½•: {self.data_dir}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_data(self) -> Dict[str, Tuple[List[str], List[int]]]:
        """åŠ è½½MR2æ•°æ®é›†"""
        print("ğŸ“š åŠ è½½MR2æ•°æ®é›†...")
        
        if USE_PROJECT_MODULES:
            try:
                # ä½¿ç”¨é¡¹ç›®çš„æ•°æ®åŠ è½½å™¨
                dataloaders = create_all_dataloaders(
                    data_dir=self.data_dir,
                    batch_sizes={'train': 32, 'val': 32, 'test': 32}
                )
                
                data = {}
                for split, dataloader in dataloaders.items():
                    texts = []
                    labels = []
                    
                    for batch in dataloader:
                        # æå–æ–‡æœ¬å’Œæ ‡ç­¾
                        if 'text' in batch:
                            texts.extend(batch['text'])
                        elif 'caption' in batch:
                            texts.extend(batch['caption'])
                        
                        if 'labels' in batch:
                            labels.extend(batch['labels'].tolist())
                        elif 'label' in batch:
                            labels.extend(batch['label'])
                    
                    data[split] = (texts, labels)
                    print(f"âœ… åŠ è½½ {split}: {len(texts)} æ ·æœ¬")
                
                return data
                
            except Exception as e:
                print(f"âŒ ä½¿ç”¨é¡¹ç›®æ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
                return self._create_demo_data()
        else:
            return self._create_demo_data()
    
    def _create_demo_data(self) -> Dict[str, Tuple[List[str], List[int]]]:
        """åˆ›å»ºæ¼”ç¤ºæ•°æ®"""
        print("ğŸ”§ åˆ›å»ºæ¼”ç¤ºæ•°æ®...")
        
        demo_texts = [
            "è¿™æ˜¯ä¸€ä¸ªå…³äºç§‘æŠ€è¿›æ­¥çš„çœŸå®æ–°é—»æŠ¥é“ï¼ŒåŒ…å«äº†è¯¦ç»†çš„æŠ€æœ¯ç»†èŠ‚",
            "This is fake news about celebrity scandal without any credible sources",
            "æœªç»è¯å®çš„ä¼ è¨€éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥éªŒè¯ï¼Œç›®å‰æ— æ³•ç¡®å®šçœŸä¼ª",
            "Breaking news: Major breakthrough in artificial intelligence technology announced by researchers",
            "ç½‘ä¼ æŸåœ°å‘ç”Ÿé‡å¤§äº‹æ•…ï¼Œå®˜æ–¹å°šæœªç¡®è®¤æ¶ˆæ¯çœŸå®æ€§",
            "Scientists discover new species in deep ocean with advanced submarine technology",
            "è°£ä¼ æŸçŸ¥åå…¬å¸å³å°†å€’é—­ï¼Œä½†å…¬å¸å®˜æ–¹å·²è¾Ÿè°£æ­¤æ¶ˆæ¯",
            "Weather alert: Severe storm approaching coastal areas according to meteorological department",
            "ç¤¾äº¤åª’ä½“å¹¿æ³›æµä¼ çš„æœªè¯å®æ¶ˆæ¯å¼•å‘å…¬ä¼—å…³æ³¨å’Œè®¨è®º",
            "Economic indicators show positive growth trends in multiple sectors this quarter",
            "æ–°ç ”ç©¶è¡¨æ˜æ°”å€™å˜åŒ–å¯¹ç”Ÿæ€ç³»ç»Ÿäº§ç”Ÿæ·±è¿œå½±å“",
            "Unverified claims about health benefits of new supplement spread online",
            "æ”¿åºœå‘å¸ƒå®˜æ–¹å£°æ˜æ¾„æ¸…ç½‘ç»œä¼ è¨€å¹¶æä¾›å‡†ç¡®ä¿¡æ¯",
            "False information about vaccine side effects causes public concern",
            "ä¸“å®¶å‘¼åå…¬ä¼—ç†æ€§å¯¹å¾…ç½‘ç»œä¿¡æ¯ï¼Œé¿å…ä¼ æ’­è°£è¨€"
        ]
        
        demo_labels = [0, 1, 2, 0, 2, 0, 1, 0, 2, 0, 0, 2, 0, 1, 0]
        
        # æ‰©å±•æ•°æ®ä»¥ä¾¿è®­ç»ƒ
        extended_texts = demo_texts * 8
        extended_labels = demo_labels * 8
        
        # æŒ‰æ¯”ä¾‹åˆ†å‰²æ•°æ®
        total_size = len(extended_texts)
        train_size = int(0.7 * total_size)
        val_size = int(0.15 * total_size)
        
        return {
            'train': (extended_texts[:train_size], extended_labels[:train_size]),
            'val': (extended_texts[train_size:train_size+val_size], 
                   extended_labels[train_size:train_size+val_size]),
            'test': (extended_texts[train_size+val_size:], 
                    extended_labels[train_size+val_size:])
        }
    
    def preprocess_texts(self, texts: List[str]) -> List[str]:
        """é¢„å¤„ç†æ–‡æœ¬"""
        if self.text_processor:
            processed_texts = []
            for text in texts:
                cleaned_text = self.text_processor.clean_text(text)
                tokens = self.text_processor.tokenize(cleaned_text)
                processed_text = ' '.join(tokens) if tokens else cleaned_text
                processed_texts.append(processed_text)
            return processed_texts
        else:
            # ç®€å•çš„æ–‡æœ¬æ¸…ç†
            import re
            processed_texts = []
            for text in texts:
                text = re.sub(r'http\S+', '', text)
                text = re.sub(r'@\w+', '', text)
                text = re.sub(r'#\w+', '', text)
                text = re.sub(r'\s+', ' ', text)
                text = text.strip().lower()
                processed_texts.append(text)
            return processed_texts
    
    def build_vocabulary(self, texts: List[str], min_freq: int = 2, max_vocab_size: int = 10000) -> Dict[str, int]:
        """æ„å»ºè¯æ±‡è¡¨"""
        print("ğŸ“– æ„å»ºè¯æ±‡è¡¨...")
        
        # ç»Ÿè®¡è¯é¢‘
        word_freq = {}
        for text in texts:
            for word in text.split():
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # æŒ‰é¢‘ç‡æ’åºå¹¶æ„å»ºè¯æ±‡è¡¨
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        
        vocab = {'<PAD>': 0, '<UNK>': 1}
        for word, freq in sorted_words:
            if freq >= min_freq and len(vocab) < max_vocab_size:
                vocab[word] = len(vocab)
        
        print(f"âœ… è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼Œå¤§å°: {len(vocab)}")
        return vocab
    
    def create_models(self):
        """åˆ›å»ºç¥ç»ç½‘ç»œæ¨¡å‹"""
        print("ğŸ§  åˆ›å»ºç¥ç»ç½‘ç»œæ¨¡å‹...")
        
        vocab_size = len(self.vocab)
        
        # TextCNN
        self.models['textcnn'] = TextCNN(
            vocab_size=vocab_size,
            embedding_dim=128,  # å‡å°ç»´åº¦ä»¥é€‚åº”å°æ•°æ®é›†
            filter_sizes=[3, 4, 5],
            num_filters=64,  # å‡å°‘æ»¤æ³¢å™¨æ•°é‡
            num_classes=3,
            dropout=0.5
        ).to(self.device)
        
        # BiLSTM
        self.models['bilstm'] = BiLSTM(
            vocab_size=vocab_size,
            embedding_dim=128,
            hidden_dim=64,  # å‡å°éšè—å±‚ç»´åº¦
            num_layers=2,
            num_classes=3,
            dropout=0.5
        ).to(self.device)
        
        # TextRCNN
        self.models['textrcnn'] = TextRCNN(
            vocab_size=vocab_size,
            embedding_dim=128,
            hidden_dim=64,
            num_classes=3,
            dropout=0.5
        ).to(self.device)
        
        print(f"âœ… åˆ›å»ºäº† {len(self.models)} ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹")
        
        # æ‰“å°æ¨¡å‹å‚æ•°é‡
        for name, model in self.models.items():
            param_count = sum(p.numel() for p in model.parameters())
            print(f"   {name}: {param_count:,} å‚æ•°")
    
    def train_single_model(self, model_name: str, train_loader: DataLoader, 
                          val_loader: DataLoader, epochs: int = 10, 
                          learning_rate: float = 0.001) -> Dict[str, Any]:
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        print(f"ğŸ‹ï¸ è®­ç»ƒ {model_name} æ¨¡å‹...")
        
        model = self.models[model_name]
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        
        train_losses = []
        val_accuracies = []
        best_val_acc = 0
        best_model_state = None
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0
            
            train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]", leave=False)
            for batch in train_pbar:
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['labels'].to(self.device)
                attention_mask = batch.get('attention_mask', None)
                if attention_mask is not None:
                    attention_mask = attention_mask.to(self.device)
                
                optimizer.zero_grad()
                
                logits = model(input_ids, attention_mask)
                loss = criterion(logits, labels)
                
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = torch.max(logits.data, 1)
                train_total += labels.size(0)
                train_correct += (predicted == labels).sum().item()
                
                train_pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{100*train_correct/train_total:.2f}%'
                })
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_correct = 0
            val_total = 0
            all_preds = []
            all_labels = []
            
            with torch.no_grad():
                for batch in val_loader:
                    input_ids = batch['input_ids'].to(self.device)
                    labels = batch['labels'].to(self.device)
                    attention_mask = batch.get('attention_mask', None)
                    if attention_mask is not None:
                        attention_mask = attention_mask.to(self.device)
                    
                    logits = model(input_ids, attention_mask)
                    _, predicted = torch.max(logits.data, 1)
                    
                    val_total += labels.size(0)
                    val_correct += (predicted == labels).sum().item()
                    
                    all_preds.extend(predicted.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())
            
            train_acc = 100 * train_correct / train_total
            val_acc = 100 * val_correct / val_total
            avg_train_loss = train_loss / len(train_loader)
            
            train_losses.append(avg_train_loss)
            val_accuracies.append(val_acc)
            
            print(f"Epoch {epoch+1}/{epochs}: "
                  f"Train Loss: {avg_train_loss:.4f}, "
                  f"Train Acc: {train_acc:.2f}%, "
                  f"Val Acc: {val_acc:.2f}%")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_model_state = model.state_dict().copy()
        
        # æ¢å¤æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
        
        # è®¡ç®—æœ€ç»ˆéªŒè¯é›†F1åˆ†æ•°
        val_f1 = f1_score(all_labels, all_preds, average='macro')
        
        result = {
            'model_name': model_name,
            'best_val_accuracy': best_val_acc,
            'val_f1_score': val_f1,
            'train_losses': train_losses,
            'val_accuracies': val_accuracies,
            'epochs_trained': epochs
        }
        
        print(f"âœ… {model_name} è®­ç»ƒå®Œæˆ:")
        print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
        print(f"   éªŒè¯F1åˆ†æ•°: {val_f1:.4f}")
        
        return result
    
    def evaluate_model(self, model_name: str, test_loader: DataLoader) -> Dict[str, Any]:
        """è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½"""
        print(f"ğŸ“Š è¯„ä¼° {model_name} æ¨¡å‹...")
        
        model = self.models[model_name]
        model.eval()
        
        test_correct = 0
        test_total = 0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="Testing"):
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['labels'].to(self.device)
                attention_mask = batch.get('attention_mask', None)
                if attention_mask is not None:
                    attention_mask = attention_mask.to(self.device)
                
                logits = model(input_ids, attention_mask)
                _, predicted = torch.max(logits.data, 1)
                
                test_total += labels.size(0)
                test_correct += (predicted == labels).sum().item()
                
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        test_acc = 100 * test_correct / test_total
        test_f1 = f1_score(all_labels, all_preds, average='macro')
        
        # åˆ†ç±»æŠ¥å‘Š
        report = classification_report(
            all_labels, all_preds,
            target_names=list(self.label_mapping.values()),
            output_dict=True
        )
        
        result = {
            'test_accuracy': test_acc,
            'test_f1_score': test_f1,
            'classification_report': report
        }
        
        print(f"âœ… {model_name} æµ‹è¯•ç»“æœ:")
        print(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
        print(f"   æµ‹è¯•F1åˆ†æ•°: {test_f1:.4f}")
        
        return result
    
    def train_all_models(self, epochs: int = 10, batch_size: int = 32, learning_rate: float = 0.001):
        """è®­ç»ƒæ‰€æœ‰ç¥ç»ç½‘ç»œæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ‰€æœ‰ç¥ç»ç½‘ç»œæ¨¡å‹...")
        
        # åŠ è½½æ•°æ®
        data = self.load_data()
        
        # é¢„å¤„ç†æ–‡æœ¬
        all_texts = data['train'][0] + data['val'][0] + data['test'][0]
        all_preprocessed = self.preprocess_texts(all_texts)
        
        # æ„å»ºè¯æ±‡è¡¨
        train_preprocessed = self.preprocess_texts(data['train'][0])
        self.vocab = self.build_vocabulary(train_preprocessed)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = TextDataset(train_preprocessed, data['train'][1], self.vocab)
        val_dataset = TextDataset(self.preprocess_texts(data['val'][0]), data['val'][1], self.vocab)
        test_dataset = TextDataset(self.preprocess_texts(data['test'][0]), data['test'][1], self.vocab)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        # åˆ›å»ºæ¨¡å‹
        self.create_models()
        
        # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
        for model_name in self.models.keys():
            print(f"\n{'='*60}")
            print(f"è®­ç»ƒæ¨¡å‹: {model_name.upper()}")
            print(f"{'='*60}")
            
            # è®­ç»ƒæ¨¡å‹
            train_result = self.train_single_model(
                model_name, train_loader, val_loader, epochs, learning_rate
            )
            
            # æµ‹è¯•è¯„ä¼°
            test_result = self.evaluate_model(model_name, test_loader)
            
            # åˆå¹¶ç»“æœ
            self.results[model_name] = {**train_result, **test_result}
        
        # ä¿å­˜æ¨¡å‹å’Œç»“æœ
        self.save_models_and_results()
        
        # æ˜¾ç¤ºæœ€ç»ˆå¯¹æ¯”
        self.print_model_comparison()
    
    def save_models_and_results(self):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œç»“æœ"""
        print("\nğŸ’¾ ä¿å­˜æ¨¡å‹å’Œç»“æœ...")
        
        # ä¿å­˜è¯æ±‡è¡¨
        vocab_file = self.output_dir / 'vocabulary.pkl'
        with open(vocab_file, 'wb') as f:
            pickle.dump(self.vocab, f)
        print(f"âœ… ä¿å­˜è¯æ±‡è¡¨: {vocab_file}")
        
        # ä¿å­˜æ¯ä¸ªæ¨¡å‹
        for model_name, model in self.models.items():
            model_file = self.output_dir / f'{model_name}_model.pth'
            torch.save(model.state_dict(), model_file)
            print(f"âœ… ä¿å­˜æ¨¡å‹: {model_file}")
        
        # ä¿å­˜ç»“æœ
        results_file = self.output_dir / 'training_results.json'
        # è½¬æ¢numpyç±»å‹ä¸ºå¯åºåˆ—åŒ–çš„ç±»å‹
        serializable_results = {}
        for model_name, result in self.results.items():
            serializable_result = {}
            for key, value in result.items():
                if isinstance(value, (list, np.ndarray)):
                    serializable_result[key] = list(value) if hasattr(value, 'tolist') else value
                else:
                    serializable_result[key] = value
            serializable_results[model_name] = serializable_result
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        print(f"âœ… ä¿å­˜è®­ç»ƒç»“æœ: {results_file}")
        
        # ä¿å­˜æ¨¡å‹æ¯”è¾ƒ
        comparison_data = []
        for model_name, result in self.results.items():
            comparison_data.append({
                'Model': model_name,
                'Best_Val_Accuracy': result['best_val_accuracy'],
                'Test_Accuracy': result['test_accuracy'],
                'Val_F1': result['val_f1_score'],
                'Test_F1': result['test_f1_score'],
                'Parameters': sum(p.numel() for p in self.models[model_name].parameters())
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        comparison_file = self.output_dir / 'model_comparison.csv'
        comparison_df.to_csv(comparison_file, index=False)
        print(f"âœ… ä¿å­˜æ¨¡å‹æ¯”è¾ƒ: {comparison_file}")
    
    def print_model_comparison(self):
        """æ‰“å°æ¨¡å‹æ¯”è¾ƒç»“æœ"""
        print(f"\nğŸ“Š {'='*70}")
        print("ç¥ç»ç½‘ç»œæ¨¡å‹æ€§èƒ½å¯¹æ¯”")
        print(f"{'='*70}")
        
        print(f"{'æ¨¡å‹':<12} {'éªŒè¯å‡†ç¡®ç‡':<10} {'æµ‹è¯•å‡†ç¡®ç‡':<10} {'éªŒè¯F1':<8} {'æµ‹è¯•F1':<8} {'å‚æ•°é‡':<10}")
        print("-" * 70)
        
        # æŒ‰æµ‹è¯•F1åˆ†æ•°æ’åº
        sorted_results = sorted(self.results.items(), 
                              key=lambda x: x[1]['test_f1_score'], 
                              reverse=True)
        
        for model_name, result in sorted_results:
            param_count = sum(p.numel() for p in self.models[model_name].parameters())
            print(f"{model_name:<12} "
                  f"{result['best_val_accuracy']:<10.2f} "
                  f"{result['test_accuracy']:<10.2f} "
                  f"{result['val_f1_score']:<8.4f} "
                  f"{result['test_f1_score']:<8.4f} "
                  f"{param_count:<10,}")
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = sorted_results[0]
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model[0]}")
        print(f"   æµ‹è¯•F1åˆ†æ•°: {best_model[1]['test_f1_score']:.4f}")
        print(f"   æµ‹è¯•å‡†ç¡®ç‡: {best_model[1]['test_accuracy']:.2f}%")


def main():
    """ä¸»å‡½æ•°ï¼Œæ¼”ç¤ºè®­ç»ƒæµç¨‹"""
    print("ğŸš€ ç¥ç»ç½‘ç»œæ–‡æœ¬åˆ†ç±»å™¨è®­ç»ƒæ¼”ç¤º")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = NeuralTextClassifier(data_dir="data")
    
    # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    trainer.train_all_models(
        epochs=15,          # è®­ç»ƒè½®æ•°
        batch_size=16,      # æ‰¹æ¬¡å¤§å°
        learning_rate=0.001 # å­¦ä¹ ç‡
    )
    
    print("\nâœ… ç¥ç»ç½‘ç»œæ¨¡å‹è®­ç»ƒæ¼”ç¤ºå®Œæˆ!")
    print(f"ğŸ“ æ¨¡å‹å’Œç»“æœå·²ä¿å­˜åˆ°: {trainer.output_dir}")


if __name__ == "__main__":
    main()