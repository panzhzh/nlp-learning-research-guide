#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# models/pretrained/encoder_models.py

"""
é¢„è®­ç»ƒç¼–ç å™¨æ¨¡å‹å®ç°
æ”¯æŒBERTã€RoBERTaã€ALBERTã€DeBERTaç­‰ä¸»æµé¢„è®­ç»ƒæ¨¡å‹
å®Œå…¨å¤ç”¨ç°æœ‰çš„æ•°æ®åŠ è½½å’Œè®­ç»ƒæ¡†æ¶
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from transformers import (
    AutoTokenizer, AutoModel, AutoConfig,
    BertTokenizer, BertModel,
    RobertaTokenizer, RobertaModel,
    AlbertTokenizer, AlbertModel,
    DebertaTokenizer, DebertaModel
)
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, classification_report
from tqdm import tqdm
import json
from typing import Dict, List, Tuple, Any, Optional, Union
from pathlib import Path
import sys
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
code_root = current_file.parent.parent.parent
sys.path.append(str(code_root))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from datasets.data_loaders import create_all_dataloaders
    from utils.config_manager import get_config_manager, get_output_path
    from preprocessing.text_processing import TextProcessor
    USE_PROJECT_MODULES = True
    print("âœ… æˆåŠŸå¯¼å…¥é¡¹ç›®æ¨¡å—")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥é¡¹ç›®æ¨¡å—å¤±è´¥: {e}")
    USE_PROJECT_MODULES = False

import logging
logger = logging.getLogger(__name__)


class PretrainedTextDataset:
    """é¢„è®­ç»ƒæ¨¡å‹ä¸“ç”¨çš„æ–‡æœ¬æ•°æ®é›†"""
    
    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 512):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨
            tokenizer: é¢„è®­ç»ƒæ¨¡å‹çš„tokenizer
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        # ä½¿ç”¨tokenizerç¼–ç 
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }


class PretrainedClassifier(nn.Module):
    """é€šç”¨çš„é¢„è®­ç»ƒæ¨¡å‹åˆ†ç±»å™¨"""
    
    def __init__(self, model_name: str, num_classes: int = 3, dropout: float = 0.1):
        """
        åˆå§‹åŒ–é¢„è®­ç»ƒåˆ†ç±»å™¨
        
        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
            num_classes: åˆ†ç±»æ•°é‡
            dropout: dropoutæ¦‚ç‡
        """
        super(PretrainedClassifier, self).__init__()
        
        self.model_name = model_name
        self.num_classes = num_classes
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œé…ç½®
        try:
            self.config = AutoConfig.from_pretrained(model_name)
            self.bert = AutoModel.from_pretrained(model_name)
        except Exception as e:
            print(f"âš ï¸  åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ¨¡å‹: {e}")
            # å¤‡ç”¨æ¨¡å‹é…ç½®
            self.config = AutoConfig.from_pretrained('bert-base-uncased')
            self.bert = AutoModel.from_pretrained('bert-base-uncased')
        
        # åˆ†ç±»å¤´
        hidden_size = self.config.hidden_size
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(hidden_size, num_classes)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–åˆ†ç±»å¤´æƒé‡"""
        nn.init.normal_(self.classifier.weight, mean=0.0, std=0.02)
        nn.init.zeros_(self.classifier.bias)
    
    def forward(self, input_ids, attention_mask):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_ids: è¾“å…¥token ids
            attention_mask: æ³¨æ„åŠ›æ©ç 
            
        Returns:
            åˆ†ç±»logits
        """
        # è·å–BERTè¾“å‡º
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # ä½¿ç”¨[CLS]tokençš„è¡¨ç¤º
        pooled_output = outputs.pooler_output
        
        # åº”ç”¨dropoutå’Œåˆ†ç±»
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        
        return logits


class PretrainedModelTrainer:
    """é¢„è®­ç»ƒæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, data_dir: str = "data", device: str = "auto"):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
        """
        self.data_dir = data_dir
        
        # è®¾ç½®è®¾å¤‡
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.models = {}
        self.tokenizers = {}
        self.results = {}
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if USE_PROJECT_MODULES:
            config_manager = get_config_manager()
            self.output_dir = get_output_path('models', 'pretrained')
        else:
            self.output_dir = Path('outputs/models/pretrained')
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ ‡ç­¾æ˜ å°„
        self.label_mapping = {0: 'Non-rumor', 1: 'Rumor', 2: 'Unverified'}
        
        # æ”¯æŒçš„æ¨¡å‹é…ç½®
        self.model_configs = {
            'bert-base-uncased': {
                'name': 'bert-base-uncased',
                'tokenizer_class': BertTokenizer,
                'model_class': BertModel,
                'description': 'BERT Base (Uncased)'
            },
            'roberta-base': {
                'name': 'roberta-base',
                'tokenizer_class': RobertaTokenizer,
                'model_class': RobertaModel,
                'description': 'RoBERTa Base'
            },
            'albert-base-v2': {
                'name': 'albert-base-v2',
                'tokenizer_class': AlbertTokenizer,
                'model_class': AlbertModel,
                'description': 'ALBERT Base v2'
            },
            'chinese-bert-wwm': {
                'name': 'hfl/chinese-bert-wwm-ext',
                'tokenizer_class': BertTokenizer,
                'model_class': BertModel,
                'description': 'Chinese BERT (Whole Word Masking)'
            }
        }
        
        print(f"ğŸ¤– é¢„è®­ç»ƒæ¨¡å‹è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ•°æ®ç›®å½•: {self.data_dir}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   æ”¯æŒæ¨¡å‹: {list(self.model_configs.keys())}")
    
    def load_data(self) -> Dict[str, Tuple[List[str], List[int]]]:
        """åŠ è½½MR2æ•°æ®é›†"""
        print("ğŸ“š åŠ è½½MR2æ•°æ®é›†...")
        
        if USE_PROJECT_MODULES:
            try:
                # ä½¿ç”¨é¡¹ç›®çš„æ•°æ®åŠ è½½å™¨
                dataloaders = create_all_dataloaders(
                    data_dir=self.data_dir,
                    batch_sizes={'train': 32, 'val': 32, 'test': 32}
                )
                
                data = {}
                for split, dataloader in dataloaders.items():
                    texts = []
                    labels = []
                    
                    for batch in dataloader:
                        # æå–æ–‡æœ¬å’Œæ ‡ç­¾
                        if 'text' in batch:
                            texts.extend(batch['text'])
                        elif 'caption' in batch:
                            texts.extend(batch['caption'])
                        
                        if 'labels' in batch:
                            labels.extend(batch['labels'].tolist())
                        elif 'label' in batch:
                            labels.extend(batch['label'])
                    
                    data[split] = (texts, labels)
                    print(f"âœ… åŠ è½½ {split}: {len(texts)} æ ·æœ¬")
                
                return data
                
            except Exception as e:
                print(f"âŒ ä½¿ç”¨é¡¹ç›®æ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
                return self._create_demo_data()
        else:
            return self._create_demo_data()
    
    def _create_demo_data(self) -> Dict[str, Tuple[List[str], List[int]]]:
        """åˆ›å»ºæ¼”ç¤ºæ•°æ®"""
        print("ğŸ”§ åˆ›å»ºæ¼”ç¤ºæ•°æ®...")
        
        demo_texts = [
            "è¿™æ˜¯ä¸€ä¸ªå…³äºç§‘æŠ€è¿›æ­¥çš„çœŸå®æ–°é—»æŠ¥é“ï¼ŒåŒ…å«äº†è¯¦ç»†çš„æŠ€æœ¯ç»†èŠ‚å’Œæƒå¨æ¥æº",
            "This is a fake news about celebrity scandal without any credible sources or verification",
            "æœªç»è¯å®çš„ä¼ è¨€éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥éªŒè¯ï¼Œç›®å‰æ— æ³•ç¡®å®šçœŸä¼ªï¼Œè¯·ç­‰å¾…å®˜æ–¹æ¶ˆæ¯",
            "Breaking news: Major breakthrough in artificial intelligence technology announced by leading researchers",
            "ç½‘ä¼ æŸåœ°å‘ç”Ÿé‡å¤§äº‹æ•…ï¼Œä½†å®˜æ–¹å°šæœªç¡®è®¤ï¼Œè¯·ä»¥æƒå¨åª’ä½“æŠ¥é“ä¸ºå‡†",
            "Scientists discover new species in deep ocean using advanced submarine technology and equipment",
            "è°£ä¼ æŸçŸ¥åå…¬å¸å³å°†å€’é—­ï¼Œä½†å…¬å¸å®˜æ–¹å·²è¾Ÿè°£ï¼Œè‚¡ä»·ä¿æŒç¨³å®š",
            "Weather alert: Severe storm approaching coastal areas according to national meteorological department",
            "ç¤¾äº¤åª’ä½“å¹¿æ³›æµä¼ çš„æœªè¯å®æ¶ˆæ¯å¼•å‘å…¬ä¼—å…³æ³¨ï¼Œä¸“å®¶å»ºè®®ç†æ€§å¯¹å¾…",
            "Economic indicators show positive growth trends in multiple sectors this quarter",
            "æ–°ç ”ç©¶è¡¨æ˜æ°”å€™å˜åŒ–å¯¹å…¨çƒç”Ÿæ€ç³»ç»Ÿäº§ç”Ÿæ·±è¿œå½±å“ï¼Œéœ€è¦é‡‡å–ç´§æ€¥è¡ŒåŠ¨",
            "Unverified claims about health benefits of new supplement spread online without scientific backing",
            "æ”¿åºœå‘å¸ƒå®˜æ–¹å£°æ˜æ¾„æ¸…ç½‘ç»œä¼ è¨€å¹¶æä¾›å‡†ç¡®ä¿¡æ¯å’Œæ•°æ®æ”¯æŒ",
            "False information about vaccine side effects causes public concern among healthcare professionals",
            "ä¸“å®¶å‘¼åå…¬ä¼—ç†æ€§å¯¹å¾…ç½‘ç»œä¿¡æ¯ï¼Œé¿å…ä¼ æ’­è°£è¨€å’Œè™šå‡æ¶ˆæ¯",
            "Technology companies announce new privacy policies following recent data security incidents",
            "æ•™è‚²éƒ¨å‘å¸ƒæ–°æ”¿ç­–æ”¯æŒåœ¨çº¿æ•™è‚²å‘å±•ï¼Œæé«˜æ•™å­¦è´¨é‡å’Œè¦†ç›–é¢",
            "International cooperation strengthens global efforts to combat climate change effectively",
            "ç½‘ç»œå®‰å…¨ä¸“å®¶è­¦å‘Šæ–°å‹ç½‘ç»œæ”»å‡»æ‰‹æ®µï¼Œå»ºè®®ç”¨æˆ·æé«˜é˜²èŒƒæ„è¯†",
            "Medical research shows promising results for new treatment methods in clinical trials"
        ]
        
        demo_labels = [0, 1, 2, 0, 2, 0, 1, 0, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 2, 0]
        
        # æ‰©å±•æ•°æ®
        extended_texts = demo_texts * 6
        extended_labels = demo_labels * 6
        
        # æŒ‰æ¯”ä¾‹åˆ†å‰²æ•°æ®
        total_size = len(extended_texts)
        train_size = int(0.7 * total_size)
        val_size = int(0.15 * total_size)
        
        return {
            'train': (extended_texts[:train_size], extended_labels[:train_size]),
            'val': (extended_texts[train_size:train_size+val_size], 
                   extended_labels[train_size:train_size+val_size]),
            'test': (extended_texts[train_size+val_size:], 
                    extended_labels[train_size+val_size:])
        }
    
    def create_model(self, model_key: str):
        """åˆ›å»ºé¢„è®­ç»ƒæ¨¡å‹"""
        print(f"ğŸ§  åˆ›å»ºæ¨¡å‹: {model_key}")
        
        if model_key not in self.model_configs:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_key}")
        
        config = self.model_configs[model_key]
        model_name = config['name']
        
        try:
            # åˆ›å»ºtokenizer
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            # åˆ›å»ºæ¨¡å‹
            model = PretrainedClassifier(
                model_name=model_name,
                num_classes=3,
                dropout=0.1
            ).to(self.device)
            
            self.tokenizers[model_key] = tokenizer
            self.models[model_key] = model
            
            # æ‰“å°æ¨¡å‹å‚æ•°é‡
            param_count = sum(p.numel() for p in model.parameters())
            print(f"âœ… {config['description']}: {param_count:,} å‚æ•°")
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºæ¨¡å‹å¤±è´¥: {model_key}, é”™è¯¯: {e}")
            # åˆ›å»ºå¤‡ç”¨ç®€å•æ¨¡å‹
            self._create_fallback_model(model_key)
    
    def _create_fallback_model(self, model_key: str):
        """åˆ›å»ºå¤‡ç”¨ç®€å•æ¨¡å‹"""
        print(f"ğŸ”„ åˆ›å»ºå¤‡ç”¨æ¨¡å‹: {model_key}")
        
        try:
            # ä½¿ç”¨bert-base-uncasedä½œä¸ºå¤‡ç”¨
            tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
            model = PretrainedClassifier(
                model_name='bert-base-uncased',
                num_classes=3
            ).to(self.device)
            
            self.tokenizers[model_key] = tokenizer
            self.models[model_key] = model
            
            print(f"âœ… å¤‡ç”¨æ¨¡å‹åˆ›å»ºæˆåŠŸ: {model_key}")
            
        except Exception as e:
            print(f"âŒ å¤‡ç”¨æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
    
    def train_single_model(self, model_key: str, train_loader: DataLoader, 
                          val_loader: DataLoader, epochs: int = 3, 
                          learning_rate: float = 2e-5) -> Dict[str, Any]:
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        print(f"ğŸ‹ï¸ è®­ç»ƒ {model_key} æ¨¡å‹...")
        
        model = self.models[model_key]
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        
        train_losses = []
        val_accuracies = []
        best_val_acc = 0
        best_model_state = None
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0
            
            train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]", leave=False)
            for batch in train_pbar:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                optimizer.zero_grad()
                
                logits = model(input_ids, attention_mask)
                loss = criterion(logits, labels)
                
                loss.backward()
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = torch.max(logits.data, 1)
                train_total += labels.size(0)
                train_correct += (predicted == labels).sum().item()
                
                train_pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{100*train_correct/train_total:.2f}%'
                })
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_correct = 0
            val_total = 0
            all_preds = []
            all_labels = []
            
            with torch.no_grad():
                for batch in val_loader:
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    labels = batch['labels'].to(self.device)
                    
                    logits = model(input_ids, attention_mask)
                    _, predicted = torch.max(logits.data, 1)
                    
                    val_total += labels.size(0)
                    val_correct += (predicted == labels).sum().item()
                    
                    all_preds.extend(predicted.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())
            
            train_acc = 100 * train_correct / train_total
            val_acc = 100 * val_correct / val_total
            avg_train_loss = train_loss / len(train_loader)
            
            train_losses.append(avg_train_loss)
            val_accuracies.append(val_acc)
            
            print(f"Epoch {epoch+1}/{epochs}: "
                  f"Train Loss: {avg_train_loss:.4f}, "
                  f"Train Acc: {train_acc:.2f}%, "
                  f"Val Acc: {val_acc:.2f}%")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_model_state = model.state_dict().copy()
        
        # æ¢å¤æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
        
        # è®¡ç®—æœ€ç»ˆF1åˆ†æ•°
        val_f1 = f1_score(all_labels, all_preds, average='macro')
        
        result = {
            'model_name': model_key,
            'model_description': self.model_configs[model_key]['description'],
            'best_val_accuracy': best_val_acc,
            'val_f1_score': val_f1,
            'train_losses': train_losses,
            'val_accuracies': val_accuracies,
            'epochs_trained': epochs
        }
        
        print(f"âœ… {model_key} è®­ç»ƒå®Œæˆ:")
        print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
        print(f"   éªŒè¯F1åˆ†æ•°: {val_f1:.4f}")
        
        return result
    
    def evaluate_model(self, model_key: str, test_loader: DataLoader) -> Dict[str, Any]:
        """è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½"""
        print(f"ğŸ“Š è¯„ä¼° {model_key} æ¨¡å‹...")
        
        model = self.models[model_key]
        model.eval()
        
        test_correct = 0
        test_total = 0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="Testing"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                logits = model(input_ids, attention_mask)
                _, predicted = torch.max(logits.data, 1)
                
                test_total += labels.size(0)
                test_correct += (predicted == labels).sum().item()
                
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        test_acc = 100 * test_correct / test_total
        test_f1 = f1_score(all_labels, all_preds, average='macro')
        
        # åˆ†ç±»æŠ¥å‘Š
        report = classification_report(
            all_labels, all_preds,
            target_names=list(self.label_mapping.values()),
            output_dict=True
        )
        
        result = {
            'test_accuracy': test_acc,
            'test_f1_score': test_f1,
            'classification_report': report
        }
        
        print(f"âœ… {model_key} æµ‹è¯•ç»“æœ:")
        print(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
        print(f"   æµ‹è¯•F1åˆ†æ•°: {test_f1:.4f}")
        
        return result
    
    def train_all_models(self, model_keys: Optional[List[str]] = None, 
                        epochs: int = 3, batch_size: int = 16, 
                        learning_rate: float = 2e-5, max_length: int = 512):
        """è®­ç»ƒæ‰€æœ‰æˆ–æŒ‡å®šçš„é¢„è®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒé¢„è®­ç»ƒæ¨¡å‹...")
        
        # é»˜è®¤è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        if model_keys is None:
            model_keys = list(self.model_configs.keys())
        
        # åŠ è½½æ•°æ®
        data = self.load_data()
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºæ•°æ®åŠ è½½å™¨
        for model_key in model_keys:
            print(f"\n{'='*60}")
            print(f"è®­ç»ƒæ¨¡å‹: {model_key.upper()}")
            print(f"{'='*60}")
            
            try:
                # åˆ›å»ºæ¨¡å‹
                self.create_model(model_key)
                
                if model_key not in self.models:
                    print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥ï¼Œè·³è¿‡: {model_key}")
                    continue
                
                tokenizer = self.tokenizers[model_key]
                
                # åˆ›å»ºæ•°æ®é›†
                train_dataset = PretrainedTextDataset(
                    data['train'][0], data['train'][1], tokenizer, max_length
                )
                val_dataset = PretrainedTextDataset(
                    data['val'][0], data['val'][1], tokenizer, max_length
                )
                test_dataset = PretrainedTextDataset(
                    data['test'][0], data['test'][1], tokenizer, max_length
                )
                
                # åˆ›å»ºæ•°æ®åŠ è½½å™¨
                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
                val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
                test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
                
                # è®­ç»ƒæ¨¡å‹
                train_result = self.train_single_model(
                    model_key, train_loader, val_loader, epochs, learning_rate
                )
                
                # æµ‹è¯•è¯„ä¼°
                test_result = self.evaluate_model(model_key, test_loader)
                
                # åˆå¹¶ç»“æœ
                self.results[model_key] = {**train_result, **test_result}
                
            except Exception as e:
                print(f"âŒ è®­ç»ƒæ¨¡å‹å¤±è´¥: {model_key}, é”™è¯¯: {e}")
                continue
        
        # ä¿å­˜ç»“æœ
        self.save_models_and_results()
        
        # æ˜¾ç¤ºæœ€ç»ˆå¯¹æ¯”
        self.print_model_comparison()
    
    def save_models_and_results(self):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œç»“æœ"""
        print("\nğŸ’¾ ä¿å­˜æ¨¡å‹å’Œç»“æœ...")
        
        # ä¿å­˜æ¯ä¸ªæ¨¡å‹
        for model_key, model in self.models.items():
            model_file = self.output_dir / f'{model_key}_model.pth'
            torch.save(model.state_dict(), model_file)
            print(f"âœ… ä¿å­˜æ¨¡å‹: {model_file}")
            
            # ä¿å­˜tokenizer
            if model_key in self.tokenizers:
                tokenizer_dir = self.output_dir / f'{model_key}_tokenizer'
                tokenizer_dir.mkdir(exist_ok=True)
                self.tokenizers[model_key].save_pretrained(tokenizer_dir)
                print(f"âœ… ä¿å­˜tokenizer: {tokenizer_dir}")
        
        # ä¿å­˜ç»“æœ
        results_file = self.output_dir / 'training_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"âœ… ä¿å­˜è®­ç»ƒç»“æœ: {results_file}")
    
    def print_model_comparison(self):
        """æ‰“å°æ¨¡å‹æ¯”è¾ƒç»“æœ"""
        print(f"\nğŸ“Š {'='*80}")
        print("é¢„è®­ç»ƒæ¨¡å‹æ€§èƒ½å¯¹æ¯”")
        print(f"{'='*80}")
        
        if not self.results:
            print("âš ï¸  æ²¡æœ‰è®­ç»ƒç»“æœå¯æ˜¾ç¤º")
            return
        
        print(f"{'æ¨¡å‹':<20} {'æè¿°':<25} {'éªŒè¯å‡†ç¡®ç‡':<10} {'æµ‹è¯•å‡†ç¡®ç‡':<10} {'æµ‹è¯•F1':<8}")
        print("-" * 80)
        
        # æŒ‰æµ‹è¯•F1åˆ†æ•°æ’åº
        sorted_results = sorted(self.results.items(), 
                              key=lambda x: x[1].get('test_f1_score', 0), 
                              reverse=True)
        
        for model_key, result in sorted_results:
            description = result.get('model_description', model_key)[:24]
            val_acc = result.get('best_val_accuracy', 0)
            test_acc = result.get('test_accuracy', 0)
            test_f1 = result.get('test_f1_score', 0)
            
            print(f"{model_key:<20} {description:<25} "
                  f"{val_acc:<10.2f} {test_acc:<10.2f} {test_f1:<8.4f}")
        
        if sorted_results:
            best_model = sorted_results[0]
            print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model[0]}")
            print(f"   æµ‹è¯•F1åˆ†æ•°: {best_model[1].get('test_f1_score', 0):.4f}")
            print(f"   æµ‹è¯•å‡†ç¡®ç‡: {best_model[1].get('test_accuracy', 0):.2f}%")


def main():
    """ä¸»å‡½æ•°ï¼Œæ¼”ç¤ºé¢„è®­ç»ƒæ¨¡å‹è®­ç»ƒæµç¨‹"""
    print("ğŸš€ é¢„è®­ç»ƒæ¨¡å‹è®­ç»ƒæ¼”ç¤º")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = PretrainedModelTrainer(data_dir="data")
    
    # é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
    models_to_train = [
        'bert-base-uncased',
        'roberta-base',
        # 'albert-base-v2',  # å¯é€‰æ‹©æ€§å¯ç”¨
        # 'chinese-bert-wwm'  # ä¸­æ–‡æ¨¡å‹ï¼Œå¯é€‰æ‹©æ€§å¯ç”¨
    ]
    
    # è®­ç»ƒæ¨¡å‹
    trainer.train_all_models(
        model_keys=models_to_train,
        epochs=3,           # è¾ƒå°‘çš„è®­ç»ƒè½®æ•°
        batch_size=8,       # è¾ƒå°çš„æ‰¹æ¬¡å¤§å°ï¼Œé€‚åº”æ˜¾å­˜é™åˆ¶
        learning_rate=2e-5, # æ ‡å‡†çš„é¢„è®­ç»ƒæ¨¡å‹å­¦ä¹ ç‡
        max_length=256      # è¾ƒçŸ­çš„åºåˆ—é•¿åº¦ï¼ŒåŠ å¿«è®­ç»ƒ
    )
    
    print("\nâœ… é¢„è®­ç»ƒæ¨¡å‹è®­ç»ƒæ¼”ç¤ºå®Œæˆ!")
    print(f"ğŸ“ æ¨¡å‹å’Œç»“æœå·²ä¿å­˜åˆ°: {trainer.output_dir}")


if __name__ == "__main__":
    main()