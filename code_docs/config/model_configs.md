# æ¨¡å‹é…ç½® Model Configs

> ğŸ¤– **å„ç±»æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å‚æ•°é…ç½®**

## ğŸ“‹ åŠŸèƒ½è¯´æ˜

`model_configs.yaml` å®šä¹‰äº†é¡¹ç›®æ”¯æŒçš„æ‰€æœ‰æ¨¡å‹çš„è¶…å‚æ•°é…ç½®ï¼Œä»ä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ°æœ€æ–°çš„å¤§è¯­è¨€æ¨¡å‹ã€‚

## ğŸ¯ ä¸»è¦é…ç½®å—

### traditional_models - ä¼ ç»Ÿæœºå™¨å­¦ä¹ 
- **svm**: æ”¯æŒå‘é‡æœºå‚æ•° (C, kernel, gamma)
- **random_forest**: éšæœºæ£®æ—å‚æ•° (n_estimators, max_depth)
- **naive_bayes**: æœ´ç´ è´å¶æ–¯å‚æ•°
- **logistic_regression**: é€»è¾‘å›å½’å‚æ•°

### neural_networks - åŸºç¡€ç¥ç»ç½‘ç»œ
- **textcnn**: æ–‡æœ¬CNNé…ç½® (filter_sizes, num_filters)
- **bilstm**: åŒå‘LSTMé…ç½® (hidden_dim, num_layers)
- **transformer_base**: åŸºç¡€Transformeré…ç½®

### pretrained_models - é¢„è®­ç»ƒæ¨¡å‹
- **BERTç³»åˆ—**: bert, roberta, albert, electra, deberta
- **ä¸­æ–‡æ¨¡å‹**: chinese-bert-wwm, chinese-roberta-wwm, macbert
- **å¤šè¯­è¨€æ¨¡å‹**: multilingual-bert, xlm-roberta

### multimodal_models - å¤šæ¨¡æ€æ¨¡å‹
- **CLIPç³»åˆ—**: clip-vit-b32, chinese-clip
- **BLIPç³»åˆ—**: blip-base é…ç½®
- **èåˆç­–ç•¥**: æ¨¡æ€èåˆæ–¹æ³•é…ç½®

### graph_neural_networks - å›¾ç¥ç»ç½‘ç»œ
- **åŸºç¡€GNN**: GCN, GAT, GraphSAGE, GIN
- **é«˜çº§æ¨¡å‹**: Graph Transformer, GraphBERT
- **å›¾æ± åŒ–**: å„ç§æ± åŒ–å±‚é…ç½®

### large_language_models - å¤§è¯­è¨€æ¨¡å‹
- **ChatGLM**: chatglm2-6b é…ç½®
- **Qwen**: qwen-7b-chat é…ç½®
- **ç”Ÿæˆå‚æ•°**: temperature, top_p, max_tokens

### parameter_efficient_finetuning - å‚æ•°é«˜æ•ˆå¾®è°ƒ
- **LoRA**: rank, alpha, dropout é…ç½®
- **AdaLoRA**: è‡ªé€‚åº”ranké…ç½®
- **P-Tuning**: prefix length é…ç½®

## ğŸ’¡ ä½¿ç”¨åœºæ™¯

- é€‰æ‹©å’Œé…ç½®æ¨¡å‹æ¶æ„
- è®¾ç½®æ¨¡å‹è¶…å‚æ•°
- é…ç½®é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
- è°ƒæ•´å¾®è°ƒç­–ç•¥å‚æ•°
- å¤šæ¨¡æ€èåˆé…ç½®

---

**[â¬…ï¸ æ•°æ®é…ç½®](data_configs.md) | [è®­ç»ƒé…ç½® â¡ï¸](training_configs.md)**
