# æ•°æ®åŠ è½½å™¨ Data Loaders

> ğŸ”„ **ä¸¥æ ¼éªŒè¯çš„PyTorchæ•°æ®åŠ è½½å™¨ï¼Œå¼ºåˆ¶ä½¿ç”¨çœŸå®æ•°æ®é›†**

## ğŸ“‹ æ¨¡å—æ¦‚è§ˆ

`data_loaders.py`æä¾›ä¸¥æ ¼éªŒè¯çš„æ•°æ®åŠ è½½å™¨å®ç°ï¼Œä¸æ”¯æŒæ¼”ç¤ºæ•°æ®ï¼Œæ‰€æœ‰åŠŸèƒ½éƒ½è¦æ±‚ä½¿ç”¨çœŸå®çš„MR2æ•°æ®é›†ã€‚

## ğŸš€ æ ¸å¿ƒç±»

### StrictDataLoaderConfig
ä¸¥æ ¼çš„æ•°æ®åŠ è½½å™¨é…ç½®ç±»ï¼Œè‡ªåŠ¨éªŒè¯æ•°æ®è¦æ±‚ï¼š

```python
class StrictDataLoaderConfig:
    def __init__(self):
        # è‡ªåŠ¨æ£€æŸ¥æ•°æ®è¦æ±‚
        check_data_requirements()
        self.config = self.load_config()
```

**ç‰¹ç‚¹**ï¼š
- åˆå§‹åŒ–æ—¶å¼ºåˆ¶éªŒè¯æ•°æ®å®Œæ•´æ€§
- ä»é…ç½®æ–‡ä»¶è‡ªåŠ¨åŠ è½½å‚æ•°
- æä¾›é»˜è®¤çš„å®‰å…¨é…ç½®
- é›†æˆé…ç½®ç®¡ç†å™¨

### StrictCollateFunction  
ä¸¥æ ¼çš„æ‰¹å¤„ç†å‡½æ•°ï¼Œç¡®ä¿æ•°æ®è´¨é‡ï¼š

```python
class StrictCollateFunction:
    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:
        # éªŒè¯æ‰¹æ¬¡æ•°æ®å®Œæ•´æ€§
        # å¤„ç†ç¼ºå¤±å›¾åƒæƒ…å†µ
        # åˆ›å»ºæ ‡å‡†å¼ é‡æ ¼å¼
```

**åŠŸèƒ½**ï¼š
- éªŒè¯æ‰¹æ¬¡æ•°æ®ä¸ä¸ºç©º
- æ£€æŸ¥å¿…è¦å­—æ®µå­˜åœ¨æ€§
- è‡ªåŠ¨å¤„ç†ç¼ºå¤±å›¾åƒçš„æƒ…å†µ
- åˆ›å»ºç»Ÿä¸€çš„tensoræ ¼å¼

## ğŸ”§ ä¸»è¦å‡½æ•°

### create_strict_dataloader
åˆ›å»ºå•ä¸ªä¸¥æ ¼éªŒè¯çš„æ•°æ®åŠ è½½å™¨ï¼š

```python
def create_strict_dataloader(
    split: str = 'train',
    batch_size: Optional[int] = None,
    shuffle: Optional[bool] = None,
    num_workers: int = 4
) -> DataLoader:
    """
    åˆ›å»ºä¸¥æ ¼çš„æ•°æ®åŠ è½½å™¨
    
    Args:
        split: æ•°æ®åˆ’åˆ† ('train', 'val', 'test')
        batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆNoneæ—¶ä½¿ç”¨é…ç½®é»˜è®¤å€¼ï¼‰
        shuffle: æ˜¯å¦æ‰“ä¹±ï¼ˆNoneæ—¶è®­ç»ƒé›†=Trueï¼Œå…¶ä»–=Falseï¼‰
        num_workers: å·¥ä½œè¿›ç¨‹æ•°
        
    Returns:
        ä¸¥æ ¼éªŒè¯çš„DataLoaderå¯¹è±¡
        
    Raises:
        FileNotFoundError: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨
        ValueError: æ•°æ®éªŒè¯å¤±è´¥
        RuntimeError: åˆ›å»ºå¤±è´¥
    """
```

### create_all_dataloaders
åˆ›å»ºæ‰€æœ‰æ•°æ®åŠ è½½å™¨çš„æ‰¹é‡å‡½æ•°ï¼š

```python
def create_all_dataloaders(
    batch_sizes: Optional[Dict[str, int]] = None
) -> Dict[str, DataLoader]:
    """
    åˆ›å»ºæ‰€æœ‰æ•°æ®åŠ è½½å™¨
    
    Args:
        batch_sizes: å„æ•°æ®é›†çš„æ‰¹æ¬¡å¤§å°
                    é»˜è®¤: {'train': 32, 'val': 64, 'test': 64}
    
    Returns:
        æ•°æ®åŠ è½½å™¨å­—å…¸ {'train': DataLoader, 'val': DataLoader, 'test': DataLoader}
        
    Raises:
        RuntimeError: ä»»ä½•æ•°æ®åŠ è½½å¤±è´¥éƒ½ä¼šæŠ›å‡ºè¯¦ç»†é”™è¯¯ä¿¡æ¯
    """
```

## ğŸ“Š æ‰¹å¤„ç†æ•°æ®æ ¼å¼

### æ‰¹å¤„ç†è¾“å‡ºæ ¼å¼
```python
# StrictCollateFunctionè¾“å‡ºçš„æ ‡å‡†æ ¼å¼
batch_data = {
    # åŸºç¡€å­—æ®µ
    'item_id': List[str],          # æ•°æ®é¡¹IDåˆ—è¡¨
    'text': List[str],             # æ–‡æœ¬åˆ—è¡¨
    'caption': List[str],          # å…¼å®¹æ€§æ–‡æœ¬å­—æ®µ
    'label': List[int],            # åŸå§‹æ ‡ç­¾åˆ—è¡¨
    
    # å¼ é‡å­—æ®µ  
    'labels': torch.Tensor,        # æ ‡ç­¾å¼ é‡ (batch_size,)
    'images': torch.Tensor,        # å›¾åƒå¼ é‡ (batch_size, 3, H, W)
    
    # å…ƒæ•°æ®
    'has_image': List[bool],       # å›¾åƒæœ‰æ•ˆæ€§åˆ—è¡¨
    'image_path': List[str],       # å›¾åƒè·¯å¾„åˆ—è¡¨
    'text_length': List[int],      # æ–‡æœ¬é•¿åº¦åˆ—è¡¨
    'token_count': List[int]       # è¯æ•°ç»Ÿè®¡åˆ—è¡¨
}
```

### ç¼ºå¤±å›¾åƒå¤„ç†
```python
# å½“å›¾åƒä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥æ—¶
if 'image' not in item or item['image'] is None:
    # è‡ªåŠ¨åˆ›å»ºé›¶å¼ é‡
    images.append(torch.zeros(3, 224, 224))
else:
    images.append(item['image'])

# æ‰¹å¤„ç†æ—¶å †å æ‰€æœ‰å›¾åƒ
batch_data['images'] = torch.stack(images)
```

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### åˆ›å»ºå•ä¸ªæ•°æ®åŠ è½½å™¨
```python
from data_utils.data_loaders import create_strict_dataloader

try:
    # åˆ›å»ºè®­ç»ƒé›†æ•°æ®åŠ è½½å™¨
    train_loader = create_strict_dataloader(
        split='train',
        batch_size=32,
        shuffle=True,
        num_workers=4
    )
    
    print(f"âœ… è®­ç»ƒé›†åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
    print(f"   æ•°æ®é›†å¤§å°: {len(train_loader.dataset)}")
    print(f"   æ‰¹æ¬¡æ•°é‡: {len(train_loader)}")
    print(f"   æ‰¹æ¬¡å¤§å°: {train_loader.batch_size}")
    
except FileNotFoundError as e:
    print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {e}")
except ValueError as e:
    print(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: {e}")
```

### åˆ›å»ºæ‰€æœ‰æ•°æ®åŠ è½½å™¨
```python
from data_utils.data_loaders import create_all_dataloaders

try:
    # åˆ›å»ºæ‰€æœ‰æ•°æ®åŠ è½½å™¨
    dataloaders = create_all_dataloaders(
        batch_sizes={'train': 16, 'val': 32, 'test': 32}
    )
    
    # è®¿é—®å„ä¸ªæ•°æ®åŠ è½½å™¨
    train_loader = dataloaders['train']
    val_loader = dataloaders['val']
    test_loader = dataloaders['test']
    
    print("âœ… æ‰€æœ‰æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
    for split, loader in dataloaders.items():
        print(f"   {split}: {len(loader.dataset)} æ ·æœ¬, æ‰¹æ¬¡å¤§å° {loader.batch_size}")
        
except RuntimeError as e:
    print(f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
    # é”™è¯¯ä¿¡æ¯åŒ…å«è¯¦ç»†çš„è§£å†³æ–¹æ¡ˆ
```

### æ‰¹æ¬¡æ•°æ®è¿­ä»£
```python
# è¿­ä»£æ•°æ®åŠ è½½å™¨
for batch_idx, batch in enumerate(train_loader):
    print(f"æ‰¹æ¬¡ {batch_idx}:")
    print(f"  æ•°æ®é”®: {list(batch.keys())}")
    print(f"  æ‰¹æ¬¡å¤§å°: {len(batch['labels'])}")
    print(f"  æ ‡ç­¾: {batch['labels']}")
    print(f"  æ–‡æœ¬æ ·ä¾‹: {batch['text'][0][:50]}...")
    print(f"  å›¾åƒå½¢çŠ¶: {batch['images'].shape}")
    print(f"  æœ‰æ•ˆå›¾åƒæ•°: {sum(batch.get('has_image', []))}")
    
    if batch_idx >= 2:  # åªæ˜¾ç¤ºå‰3ä¸ªæ‰¹æ¬¡
        break
```

## ğŸ§ª æµ‹è¯•å’ŒéªŒè¯

### test_dataloader å‡½æ•°
å†…ç½®çš„æ•°æ®åŠ è½½å™¨æµ‹è¯•å‡½æ•°ï¼š

```python
from data_utils.data_loaders import test_dataloader

# æµ‹è¯•æ•°æ®åŠ è½½å™¨
test_dataloader(train_loader, max_batches=3)
```

**éªŒè¯å†…å®¹**ï¼š
- æ‰¹æ¬¡æ•°æ®ç±»å‹æ£€æŸ¥
- å¿…è¦å­—æ®µå­˜åœ¨æ€§éªŒè¯
- å¼ é‡å½¢çŠ¶å’Œç±»å‹éªŒè¯
- æ‰¹æ¬¡å¤§å°ä¸€è‡´æ€§æ£€æŸ¥
- æ•°æ®å†…å®¹åˆç†æ€§éªŒè¯

### æµ‹è¯•è¾“å‡ºç¤ºä¾‹
```
ğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½å™¨ (æœ€å¤š 3 ä¸ªæ‰¹æ¬¡)
  æ‰¹æ¬¡ 0:
    æ•°æ®é”®: ['item_id', 'text', 'caption', 'label', 'labels', 'images', 'has_image']
    æ ‡ç­¾: tensor([1, 0, 2, 1, 0, 2, 1, 0])
    æ‰¹æ¬¡å¤§å°: 8
    æ–‡æœ¬æ ·ä¾‹: è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ This is a test text...
    å›¾åƒå½¢çŠ¶: torch.Size([8, 3, 224, 224])
  æ‰¹æ¬¡ 1:
    ...
âœ… æ•°æ®åŠ è½½å™¨æµ‹è¯•é€šè¿‡
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### é…ç½®ä¼˜åŒ–å»ºè®®
```python
# æ ¹æ®ç¡¬ä»¶è°ƒæ•´å‚æ•°
optimal_config = {
    # CPUæ ¸å¿ƒæ•°å†³å®šworkeræ•°é‡
    'num_workers': min(4, os.cpu_count()),
    
    # GPUå†…å­˜å†³å®šæ‰¹æ¬¡å¤§å°
    'batch_size': 32 if torch.cuda.is_available() else 16,
    
    # å¯ç”¨å†…å­˜å›ºå®šï¼ˆGPUè®­ç»ƒæ—¶ï¼‰
    'pin_memory': torch.cuda.is_available(),
    
    # æŒä¹…åŒ–workersï¼ˆå‡å°‘è¿›ç¨‹åˆ›å»ºå¼€é”€ï¼‰
    'persistent_workers': True
}
```

### å†…å­˜ç®¡ç†
```python
# å†…å­˜ä¸è¶³æ—¶çš„ä¼˜åŒ–ç­–ç•¥
low_memory_config = {
    'batch_size': 8,          # å‡å°æ‰¹æ¬¡å¤§å°
    'num_workers': 1,         # å‡å°‘workerè¿›ç¨‹
    'pin_memory': False,      # å…³é—­å†…å­˜å›ºå®š
    'drop_last': True         # ä¸¢å¼ƒä¸å®Œæ•´æ‰¹æ¬¡
}
```

## ğŸ”§ è‡ªå®šä¹‰é…ç½®

### è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°
```python
from data_utils.data_loaders import StrictCollateFunction

class CustomCollateFunction(StrictCollateFunction):
    def __call__(self, batch):
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•è·å–åŸºç¡€æ‰¹å¤„ç†ç»“æœ
        batch_data = super().__call__(batch)
        
        # æ·»åŠ è‡ªå®šä¹‰å­—æ®µ
        batch_data['custom_field'] = [item.get('custom', None) for item in batch]
        
        return batch_data

# ä½¿ç”¨è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°
custom_loader = DataLoader(
    dataset,
    batch_size=32,
    collate_fn=CustomCollateFunction()
)
```

### é…ç½®æ–‡ä»¶é›†æˆ
```python
# ä»é…ç½®æ–‡ä»¶è·å–å‚æ•°
from utils.config_manager import get_data_config

data_config = get_data_config()
dataloader_config = data_config.get('dataloader', {})

train_config = dataloader_config.get('train', {})
batch_size = train_config.get('batch_size', 32)
num_workers = train_config.get('num_workers', 4)
```

## ğŸš¨ é”™è¯¯å¤„ç†

### å¸¸è§é”™è¯¯ç±»å‹
```python
# 1. æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨
FileNotFoundError: "âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: data/dataset_items_train.json"
# è§£å†³: ä¸‹è½½MR2æ•°æ®é›†å¹¶è§£å‹åˆ°æ­£ç¡®ä½ç½®

# 2. æ•°æ®éªŒè¯å¤±è´¥  
ValueError: "âŒ train æ•°æ®é›†æ ·æœ¬æ•°ä¸è¶³: 5 < 10"
# è§£å†³: ç¡®ä¿æ•°æ®é›†åŒ…å«è¶³å¤Ÿçš„æ ·æœ¬

# 3. æ‰¹æ¬¡æ•°æ®ä¸ºç©º
ValueError: "âŒ æ‰¹æ¬¡æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæ‰¹å¤„ç†"
# è§£å†³: æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ­£ç¡®åŠ è½½

# 4. æ ‡ç­¾è½¬æ¢å¤±è´¥
ValueError: "âŒ æ ‡ç­¾è½¬æ¢å¤±è´¥: invalid literal for int()"
# è§£å†³: æ£€æŸ¥æ ‡ç­¾å­—æ®µæ ¼å¼
```

### å®Œæ•´é”™è¯¯å¤„ç†ç¤ºä¾‹
```python
try:
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloaders = create_all_dataloaders()
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    for split, loader in dataloaders.items():
        test_dataloader(loader, max_batches=1)
    
    print("âœ… æ‰€æœ‰æ•°æ®åŠ è½½å™¨éªŒè¯é€šè¿‡")
    
except FileNotFoundError as e:
    print(f"âŒ æ–‡ä»¶é”™è¯¯: {e}")
    print("è§£å†³æ–¹æ¡ˆ:")
    print("1. ä¸‹è½½MR2æ•°æ®é›†")
    print("2. è§£å‹åˆ°é¡¹ç›®æ ¹ç›®å½•/data")
    print("3. ç¡®ä¿åŒ…å«æ‰€æœ‰JSONæ–‡ä»¶")
    
except ValueError as e:
    print(f"âŒ éªŒè¯é”™è¯¯: {e}")
    print("è§£å†³æ–¹æ¡ˆ:")
    print("1. æ£€æŸ¥æ•°æ®æ–‡ä»¶å®Œæ•´æ€§")
    print("2. éªŒè¯JSONæ ¼å¼æ­£ç¡®æ€§")
    print("3. ç¡®ä¿æ ·æœ¬æ•°é‡è¶³å¤Ÿ")
    
except RuntimeError as e:
    print(f"âŒ è¿è¡Œæ—¶é”™è¯¯: {e}")
    print("è§£å†³æ–¹æ¡ˆ:")
    print("1. æ£€æŸ¥ç³»ç»Ÿå†…å­˜")
    print("2. å‡å°‘batch_sizeå’Œnum_workers")
    print("3. é‡å¯Pythonè¿›ç¨‹")
```

## ğŸ”„ å‘åå…¼å®¹

### å…¼å®¹æ€§å‡½æ•°
```python
# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸæœ‰å‡½æ•°å
def create_simple_dataloader(*args, **kwargs):
    """å‘åå…¼å®¹çš„å‡½æ•°å"""
    return create_strict_dataloader(*args, **kwargs)

def create_mr2_dataloaders(*args, **kwargs):
    """å‘åå…¼å®¹çš„å‡½æ•°å"""
    return create_all_dataloaders(*args, **kwargs)
```

### æ¨¡å—å¯¼å…¥å…¼å®¹æ€§
```python
# __init__.py ä¸­çš„æ™ºèƒ½å¯¼å…¥
try:
    from .data_loaders import create_all_dataloaders, StrictDataLoaderConfig
    create_mr2_dataloaders = create_all_dataloaders
    DataLoaderFactory = StrictDataLoaderConfig
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
    create_all_dataloaders = None
```

## ğŸ’¡ æœ€ä½³å®è·µ

### å¼€å‘é˜¶æ®µ
```python
# å¼€å‘æ—¶ä½¿ç”¨è¾ƒå°çš„é…ç½®å¿«é€Ÿè¿­ä»£
dev_dataloaders = create_all_dataloaders(
    batch_sizes={'train': 8, 'val': 16, 'test': 16}
)
```

### ç”Ÿäº§é˜¶æ®µ  
```python
# ç”Ÿäº§æ—¶ä½¿ç”¨ä¼˜åŒ–çš„é…ç½®
prod_dataloaders = create_all_dataloaders(
    batch_sizes={'train': 32, 'val': 64, 'test': 64}
)
```

### è°ƒè¯•æŠ€å·§
```python
# 1. å…ˆæµ‹è¯•å•ä¸ªæ ·æœ¬
dataset = MR2Dataset(data_dir='data', split='train')
sample = dataset[0]
print("å•ä¸ªæ ·æœ¬æµ‹è¯•é€šè¿‡")

# 2. å†æµ‹è¯•å°æ‰¹æ¬¡
small_loader = create_strict_dataloader(split='train', batch_size=2)
batch = next(iter(small_loader))
print("å°æ‰¹æ¬¡æµ‹è¯•é€šè¿‡")

# 3. æœ€åæµ‹è¯•å®Œæ•´é…ç½®
full_loader = create_strict_dataloader(split='train', batch_size=32)
test_dataloader(full_loader, max_batches=3)
```

---

**[â¬…ï¸ MR2æ•°æ®é›†](code_docs/data_loaders/mr2_dataset.md) | [æ•°æ®åˆ†æ â¡ï¸](code_docs/data_loaders/mr2_analysis.md)**