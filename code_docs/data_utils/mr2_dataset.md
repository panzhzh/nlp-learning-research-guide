# MR2æ•°æ®é›†ç±» MR2Dataset

> ğŸ“š **ä¸¥æ ¼éªŒè¯çš„PyTorchæ•°æ®é›†ç±»ï¼Œä¸“ä¸ºMR2å¤šæ¨¡æ€è°£è¨€æ£€æµ‹è®¾è®¡**

## ğŸ“‹ ç±»æ¦‚è§ˆ

`MR2Dataset`æ˜¯ä¸¥æ ¼éªŒè¯çš„PyTorch Datasetå®ç°ï¼Œåªæ”¯æŒçœŸå®æ•°æ®é›†ï¼Œæä¾›å®Œæ•´çš„å¤šæ¨¡æ€æ•°æ®å¤„ç†åŠŸèƒ½ã€‚

```python
from data_utils import MR2Dataset

# ä¸¥æ ¼æ¨¡å¼ï¼šå¿…é¡»ä½¿ç”¨çœŸå®æ•°æ®é›†
dataset = MR2Dataset(
    data_dir='data',
    split='train',
    transform_type='train',
    load_images=True
)
```

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½

### åˆå§‹åŒ–å‚æ•°
```python
def __init__(self, 
             data_dir: Union[str, Path],
             split: str = 'train',
             transform_type: str = 'train', 
             target_size: Tuple[int, int] = (224, 224),
             load_images: bool = True):
```

| å‚æ•° | ç±»å‹ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|------|--------|
| `data_dir` | str/Path | æ•°æ®ç›®å½•è·¯å¾„ | å¿…éœ€ |
| `split` | str | æ•°æ®åˆ’åˆ† ('train', 'val', 'test') | 'train' |
| `transform_type` | str | å›¾åƒå˜æ¢ç±»å‹ ('train', 'val') | 'train' |
| `target_size` | tuple | ç›®æ ‡å›¾åƒå°ºå¯¸ (H, W) | (224, 224) |
| `load_images` | bool | æ˜¯å¦åŠ è½½å›¾åƒ | True |

### ä¸¥æ ¼éªŒè¯æœºåˆ¶

#### æ•°æ®è¦æ±‚éªŒè¯
```python
# è‡ªåŠ¨è°ƒç”¨çš„éªŒè¯æµç¨‹
def __init__(self):
    # 1. æ£€æŸ¥æ•°æ®è¦æ±‚
    check_data_requirements()
    
    # 2. è®¾ç½®é…ç½®
    self.setup_config()
    
    # 3. è®¾ç½®å›¾åƒå˜æ¢
    self.setup_transforms()
    
    # 4. åŠ è½½æ•°æ®é›†
    self.load_dataset()
    
    # 5. éªŒè¯æ•°æ®é›†
    self.validate_dataset()
```

#### æ–‡ä»¶éªŒè¯
```python
# å¿…éœ€æ–‡ä»¶æ£€æŸ¥
required_files = [
    f'dataset_items_{split}.json'  # å¯¹åº”splitçš„JSONæ–‡ä»¶
]

# æ•°æ®æ ¼å¼éªŒè¯
required_fields = {
    'caption': str,    # æ–‡æœ¬å†…å®¹ï¼ˆå¿…éœ€ï¼‰
    'label': int,      # æ ‡ç­¾ï¼ˆå¿…éœ€ï¼‰
    'image_path': str  # å›¾åƒè·¯å¾„ï¼ˆå¯é€‰ï¼‰
}
```

## ğŸ–¼ï¸ å›¾åƒå¤„ç†

### å›¾åƒå˜æ¢é…ç½®
```python
# è®­ç»ƒæ—¶å˜æ¢ï¼ˆæ•°æ®å¢å¼ºï¼‰
if self.transform_type == 'train':
    self.image_transforms = transforms.Compose([
        transforms.Resize(self.target_size),
        transforms.RandomHorizontalFlip(p=0.3),  # é™ä½éšæœºæ€§
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])  # ImageNetæ ‡å‡†
    ])

# éªŒè¯/æµ‹è¯•æ—¶å˜æ¢ï¼ˆæ— å¢å¼ºï¼‰
else:
    self.image_transforms = transforms.Compose([
        transforms.Resize(self.target_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
```

### å®‰å…¨å›¾åƒåŠ è½½
```python
def load_image_safe(self, image_path: str) -> Dict[str, Any]:
    """
    å®‰å…¨åŠ è½½å›¾åƒï¼Œå¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ
    
    Returns:
        {
            'image': torch.Tensor,      # å›¾åƒå¼ é‡ (3, H, W)
            'has_image': bool,          # æ˜¯å¦æˆåŠŸåŠ è½½
            'image_path': str,          # å›¾åƒè·¯å¾„
            'image_size': tuple         # åŸå§‹å°ºå¯¸ (å¯é€‰)
        }
    """
```

## ğŸ“Š æ•°æ®é¡¹æ ¼å¼

### è¿”å›æ•°æ®ç»“æ„
```python
def __getitem__(self, idx: int) -> Dict[str, Any]:
    """
    è¿”å›å•ä¸ªæ•°æ®æ ·æœ¬
    
    Returns:
        {
            'item_id': str,           # æ•°æ®é¡¹ID
            'text': str,              # ä¸»è¦æ–‡æœ¬å­—æ®µ
            'caption': str,           # å…¼å®¹æ€§å­—æ®µï¼ˆåŒtextï¼‰
            'label': int,             # æ ‡ç­¾ (0, 1, 2)
            'language': str,          # è¯­è¨€ç±»å‹
            'text_length': int,       # æ–‡æœ¬é•¿åº¦
            'token_count': int,       # è¯æ•°ç»Ÿè®¡
            'image': torch.Tensor,    # å›¾åƒå¼ é‡ (3, H, W)
            'has_image': bool,        # æ˜¯å¦æœ‰æœ‰æ•ˆå›¾åƒ
            'image_path': str,        # å›¾åƒè·¯å¾„
            'image_size': tuple       # å›¾åƒå°ºå¯¸ï¼ˆå¦‚æœæœ‰ï¼‰
        }
    """
```

### æ ‡ç­¾æ˜ å°„
```python
# ä»é…ç½®ç®¡ç†å™¨è·å–çš„æ ‡ç­¾æ˜ å°„
self.label_mapping = {
    0: "Non-rumor",     # éè°£è¨€
    1: "Rumor",         # è°£è¨€  
    2: "Unverified"     # æœªéªŒè¯
}
```

## ğŸ”§ å®ç”¨æ–¹æ³•

### ç»Ÿè®¡ä¿¡æ¯è·å–
```python
# è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
stats = dataset.get_statistics()
"""
è¿”å›:
{
    'total_samples': int,                    # æ€»æ ·æœ¬æ•°
    'label_distribution': Dict[str, int],    # æ ‡ç­¾åˆ†å¸ƒ
    'has_image_count': int,                  # æœ‰æ•ˆå›¾åƒæ•°é‡
    'text_length_stats': {                   # æ–‡æœ¬é•¿åº¦ç»Ÿè®¡
        'min': int,
        'max': int, 
        'mean': float,
        'std': float
    }
}
"""

# è·å–æ ‡ç­¾åˆ†å¸ƒ
label_dist = dataset.get_label_distribution()
# è¿”å›: {'Non-rumor': 150, 'Rumor': 120, 'Unverified': 30}
```

### æ ·æœ¬æŸ¥è¯¢
```python
# æ ¹æ®IDè·å–æ ·æœ¬
sample = dataset.get_sample_by_id('item_123')

# æ‰“å°æ ·æœ¬ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
dataset.print_sample_info(idx=0)
"""
è¾“å‡ºç¤ºä¾‹:
ğŸ” æ ·æœ¬ 0 ä¿¡æ¯:
   ID: item_001
   æ–‡æœ¬: è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ This is a test...
   æ ‡ç­¾: 1 (Rumor)
   æ–‡æœ¬é•¿åº¦: 45
   æœ‰å›¾åƒ: True
   å›¾åƒè·¯å¾„: data/train/img/item_001.jpg
   å›¾åƒå¼ é‡å½¢çŠ¶: torch.Size([3, 224, 224])
"""
```

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨
```python
from data_utils import MR2Dataset

# åˆ›å»ºè®­ç»ƒé›†
train_dataset = MR2Dataset(
    data_dir='data',
    split='train',
    transform_type='train',
    load_images=True
)

print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
print(f"æ ‡ç­¾åˆ†å¸ƒ: {train_dataset.get_label_distribution()}")

# è·å–å•ä¸ªæ ·æœ¬
sample = train_dataset[0]
print(f"æ ·æœ¬ID: {sample['item_id']}")
print(f"æ–‡æœ¬: {sample['text'][:50]}...")
print(f"æ ‡ç­¾: {sample['label']} ({train_dataset.label_mapping[sample['label']]})")
print(f"å›¾åƒå½¢çŠ¶: {sample['image'].shape}")
```

### éªŒè¯é›†åˆ›å»º
```python
# åˆ›å»ºéªŒè¯é›†ï¼ˆæ— æ•°æ®å¢å¼ºï¼‰
val_dataset = MR2Dataset(
    data_dir='data',
    split='val',
    transform_type='val',  # æ— å¢å¼ºå˜æ¢
    load_images=True
)

# å¯¹æ¯”è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„å˜æ¢
print("è®­ç»ƒé›†å˜æ¢:", train_dataset.image_transforms)
print("éªŒè¯é›†å˜æ¢:", val_dataset.image_transforms)
```

### é”™è¯¯å¤„ç†ç¤ºä¾‹
```python
try:
    # å°è¯•åˆ›å»ºæ•°æ®é›†
    dataset = MR2Dataset(data_dir='data', split='train')
    
    # éªŒè¯æ•°æ®é›†åŸºæœ¬ä¿¡æ¯
    if len(dataset) == 0:
        raise ValueError("æ•°æ®é›†ä¸ºç©º")
        
    # æµ‹è¯•æ ·æœ¬è®¿é—®
    sample = dataset[0]
    print("âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸ")
    
except FileNotFoundError as e:
    print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {e}")
    print("è§£å†³æ–¹æ¡ˆ:")
    print("1. ä¸‹è½½MR2æ•°æ®é›†")
    print("2. è§£å‹åˆ°é¡¹ç›®æ ¹ç›®å½•çš„dataæ–‡ä»¶å¤¹")
    print("3. ç¡®ä¿åŒ…å«æ‰€æœ‰å¿…éœ€çš„JSONæ–‡ä»¶")
    
except ValueError as e:
    print(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: {e}")
    print("è§£å†³æ–¹æ¡ˆ:")
    print("1. æ£€æŸ¥æ•°æ®æ–‡ä»¶æ ¼å¼")
    print("2. ç¡®ä¿æœ€å°æ ·æœ¬æ•°è¦æ±‚")
    print("3. éªŒè¯æ ‡ç­¾å’Œå­—æ®µå®Œæ•´æ€§")
    
except Exception as e:
    print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
```

### ä¸DataLoaderç»“åˆä½¿ç”¨
```python
from torch.utils.data import DataLoader

# åˆ›å»ºæ•°æ®é›†
dataset = MR2Dataset(data_dir='data', split='train')

# åˆ›å»ºDataLoader
dataloader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4,
    collate_fn=None  # ä½¿ç”¨é»˜è®¤collate_fnæˆ–è‡ªå®šä¹‰
)

# è¿­ä»£æ•°æ®
for batch_idx, batch in enumerate(dataloader):
    print(f"æ‰¹æ¬¡ {batch_idx}:")
    print(f"  æ–‡æœ¬æ•°é‡: {len(batch['text'])}")
    print(f"  æ ‡ç­¾å½¢çŠ¶: {batch['labels'].shape if 'labels' in batch else 'N/A'}")
    print(f"  å›¾åƒå½¢çŠ¶: {batch['images'].shape if 'images' in batch else 'N/A'}")
    
    if batch_idx >= 2:  # åªæ˜¾ç¤ºå‰3ä¸ªæ‰¹æ¬¡
        break
```

## âš™ï¸ é…ç½®é›†æˆ

### é…ç½®ç®¡ç†å™¨é›†æˆ
```python
# æ•°æ®é›†è‡ªåŠ¨ä½¿ç”¨é…ç½®ç®¡ç†å™¨
from utils.config_manager import get_data_config, get_label_mapping

# é…ç½®è‡ªåŠ¨åŠ è½½
self.label_mapping = get_label_mapping()
data_config = get_data_config()
self.dataset_config = data_config.get('dataset', {})
```

### é¢„å¤„ç†é…ç½®
```python
# ä»é…ç½®æ–‡ä»¶è·å–å›¾åƒå¤„ç†å‚æ•°
processing_config = data_config.get('processing', {}).get('image', {})
self.normalize_mean = processing_config.get('normalize_mean', [0.485, 0.456, 0.406])
self.normalize_std = processing_config.get('normalize_std', [0.229, 0.224, 0.225])
```

## ğŸ” è°ƒè¯•å’Œæ€§èƒ½

### è°ƒè¯•åŠŸèƒ½
```python
# æ‰“å°è¯¦ç»†çš„æ ·æœ¬ä¿¡æ¯
dataset.print_sample_info(0)

# è·å–ç»Ÿè®¡ä¿¡æ¯è¿›è¡Œè°ƒè¯•
stats = dataset.get_statistics()
print(f"æ•°æ®é›†ç»Ÿè®¡: {stats}")

# æ£€æŸ¥å›¾åƒåŠ è½½æƒ…å†µ
sample = dataset[0]
if sample['has_image']:
    print(f"âœ… å›¾åƒåŠ è½½æˆåŠŸ: {sample['image'].shape}")
else:
    print("âŒ å›¾åƒåŠ è½½å¤±è´¥")
```

### æ€§èƒ½ä¼˜åŒ–
- **å›¾åƒé¢„åŠ è½½**: è®¾ç½®`load_images=False`å¯ä»¥è·³è¿‡å›¾åƒåŠ è½½
- **å˜æ¢ä¼˜åŒ–**: éªŒè¯æ—¶ä½¿ç”¨`transform_type='val'`é¿å…æ•°æ®å¢å¼º
- **å†…å­˜ç®¡ç†**: åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„æ ·æœ¬å¼•ç”¨

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

### æ•°æ®è¦æ±‚
- **çœŸå®æ•°æ®é›†**: å¿…é¡»ä½¿ç”¨å®Œæ•´çš„MR2æ•°æ®é›†ï¼Œä¸æ”¯æŒæ¼”ç¤ºæ•°æ®
- **æ–‡ä»¶å®Œæ•´æ€§**: æ‰€æœ‰JSONæ–‡ä»¶å¿…é¡»å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®
- **æœ€å°æ ·æœ¬æ•°**: æ¯ä¸ªsplitéœ€è¦è‡³å°‘10ä¸ªæœ‰æ•ˆæ ·æœ¬

### å…¼å®¹æ€§
- **å‘åå…¼å®¹**: æä¾›`SimpleMR2Dataset`åˆ«å
- **å­—æ®µå…¼å®¹**: åŒæ—¶æä¾›`text`å’Œ`caption`å­—æ®µ
- **æ ‡ç­¾å…¼å®¹**: æ”¯æŒæ•´æ•°æ ‡ç­¾å’Œå­—ç¬¦ä¸²æ ‡ç­¾æ˜ å°„

### é”™è¯¯æ¢å¤
- **å›¾åƒé”™è¯¯**: å›¾åƒåŠ è½½å¤±è´¥æ—¶è‡ªåŠ¨åˆ›å»ºé›¶å¼ é‡
- **æ ¼å¼é”™è¯¯**: æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œè§£å†³å»ºè®®
- **è·¯å¾„é—®é¢˜**: è‡ªåŠ¨å¤„ç†ç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„

---

**[â¬…ï¸ æ•°æ®å·¥å…·æ¦‚è§ˆ](README.md) | [æ•°æ®åŠ è½½å™¨ â¡ï¸](data_loaders.md)**