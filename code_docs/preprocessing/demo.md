# æ¼”ç¤ºè„šæœ¬ Demo Script

> ğŸ“ **é¢„å¤„ç†åŠŸèƒ½çš„å¿«é€Ÿæ¼”ç¤ºå’Œæµ‹è¯•è„šæœ¬**

## ğŸ“‹ åŠŸèƒ½è¯´æ˜

`demo.py` æä¾›é¢„å¤„ç†æ¨¡å—çš„å¿«é€Ÿæ¼”ç¤ºè„šæœ¬ï¼Œå±•ç¤ºæ–‡æœ¬å’Œå›¾åƒé¢„å¤„ç†åŠŸèƒ½çš„å®Œæ•´ä½¿ç”¨æµç¨‹ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•å’ŒåŠŸèƒ½éªŒè¯ã€‚

## ğŸ¯ ä¸»è¦åŠŸèƒ½

### æ¼”ç¤ºåŠŸèƒ½
- **æ–‡æœ¬å¤„ç†æ¼”ç¤º**: å±•ç¤ºæ–‡æœ¬æ¸…æ´—ã€åˆ†è¯ã€ç‰¹å¾æå–
- **å›¾åƒå¤„ç†æ¼”ç¤º**: å±•ç¤ºå›¾åƒåŠ è½½ã€é¢„å¤„ç†ã€æ‰¹é‡å¤„ç†
- **åŠŸèƒ½å¯¹æ¯”**: å±•ç¤ºä¸åŒå‚æ•°é…ç½®çš„æ•ˆæœ
- **æ€§èƒ½æµ‹è¯•**: ç®€å•çš„å¤„ç†é€Ÿåº¦å’Œè´¨é‡è¯„ä¼°

### æµ‹è¯•åŠŸèƒ½
- **ä¾èµ–åº“æ£€æŸ¥**: éªŒè¯æ‰€éœ€åº“æ˜¯å¦æ­£ç¡®å®‰è£…
- **æ•°æ®æ–‡ä»¶æ£€æŸ¥**: æ£€æŸ¥æµ‹è¯•æ•°æ®æ˜¯å¦å­˜åœ¨
- **åŠŸèƒ½å®Œæ•´æ€§æµ‹è¯•**: éªŒè¯å„é¡¹åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
- **é”™è¯¯å¤„ç†æµ‹è¯•**: æµ‹è¯•å¼‚å¸¸æƒ…å†µçš„å¤„ç†èƒ½åŠ›

## ğŸš€ è„šæœ¬ç»“æ„

### main() å‡½æ•°
ä¸»è¦æ¼”ç¤ºæµç¨‹:
1. æ–‡æœ¬å¤„ç†å™¨åŠŸèƒ½æ¼”ç¤º
2. å›¾åƒå¤„ç†å™¨åŠŸèƒ½æ¼”ç¤º
3. æ€§èƒ½å’Œè´¨é‡å±•ç¤º
4. é”™è¯¯å¤„ç†ç¤ºä¾‹

### æ¼”ç¤ºå†…å®¹
- **æ–‡æœ¬å¤„ç†**: å¤šè¯­è¨€æ–‡æœ¬å¤„ç†ç¤ºä¾‹
- **å›¾åƒå¤„ç†**: å•å¼ å’Œæ‰¹é‡å›¾åƒå¤„ç†
- **ç‰¹å¾æå–**: æ–‡æœ¬å’Œå›¾åƒç‰¹å¾å±•ç¤º
- **é…ç½®å¯¹æ¯”**: ä¸åŒå‚æ•°è®¾ç½®çš„æ•ˆæœ

## ğŸ’¡ ä½¿ç”¨æ–¹æ³•

### ç›´æ¥è¿è¡Œ
```bash
# è¿›å…¥preprocessingç›®å½•
cd preprocessing

# è¿è¡Œæ¼”ç¤ºè„šæœ¬
python demo.py
```

### åˆ†æ¨¡å—æ¼”ç¤º
```python
from preprocessing.demo import main

# è¿è¡Œå®Œæ•´æ¼”ç¤º
main()
```

### è‡ªå®šä¹‰æ¼”ç¤º
```python
from preprocessing import TextProcessor, ImageProcessor

# åˆ›å»ºå¤„ç†å™¨
text_processor = TextProcessor(language='mixed')
image_processor = ImageProcessor(target_size=(224, 224))

# è‡ªå®šä¹‰æµ‹è¯•
test_texts = ["Your test texts here"]
test_images = ["path/to/test/image.jpg"]
```

## ğŸ“Š æ¼”ç¤ºè¾“å‡º

### æ–‡æœ¬å¤„ç†æ¼”ç¤ºè¾“å‡º
```
ğŸ“ æ–‡æœ¬å¤„ç†æ¼”ç¤º:

æµ‹è¯• 1: è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ This is a test text!
  è¯­è¨€: mixed
  æ¸…æ´—å: è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ This is a test text
  åˆ†è¯ç»“æœ: ['è¿™æ˜¯', 'ä¸€ä¸ª', 'æµ‹è¯•', 'æ–‡æœ¬', 'this', 'test', 'text']
  ç‰¹å¾: é•¿åº¦=42, è¯æ•°=7, è¯­è¨€=mixed

æµ‹è¯• 2: ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œé€‚åˆå‡ºé—¨æ¸¸ç©ã€‚
  è¯­è¨€: chinese
  æ¸…æ´—å: ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œé€‚åˆå‡ºé—¨æ¸¸ç©ã€‚
  åˆ†è¯ç»“æœ: ['ä»Šå¤©', 'å¤©æ°”', 'ä¸é”™', 'é€‚åˆ', 'å‡ºé—¨', 'æ¸¸ç©']
  ç‰¹å¾: é•¿åº¦=14, è¯æ•°=6, è¯­è¨€=chinese
```

### å›¾åƒå¤„ç†æ¼”ç¤ºè¾“å‡º
```
ğŸ–¼ï¸ å›¾åƒå¤„ç†æ¼”ç¤º:

æµ‹è¯•å›¾åƒ: data/train/img/example.jpg
å›¾åƒä¿¡æ¯: {'width': 640, 'height': 480, 'format': 'JPEG', 'file_size_mb': 0.15}
å¤„ç†ç»“æœtensorå½¢çŠ¶: torch.Size([3, 224, 224])
å›¾åƒç‰¹å¾: {'brightness': 128.5, 'contrast': 45.2, 'edge_density': 0.12}

æ‰¹é‡å¤„ç†: å·²å¤„ç† 50 å¼ å›¾åƒ
```

## ğŸ”§ æ¼”ç¤ºé…ç½®

### æµ‹è¯•æ–‡æœ¬æ ·ä¾‹
```python
test_texts = [
    "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ This is a test text!",
    "ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œé€‚åˆå‡ºé—¨æ¸¸ç©ã€‚",
    "Breaking news: AI technology advances rapidly!",
    "æ··åˆè¯­è¨€æ–‡æœ¬ with English words and ä¸­æ–‡å­—ç¬¦",
    "åŒ…å«URLçš„æ–‡æœ¬ https://example.com å’Œ@usernameæåŠ",
    "å¸¦æœ‰emojiçš„æ–‡æœ¬ ğŸ˜Š å’Œ #hashtag æ ‡ç­¾"
]
```

### å¤„ç†å™¨é…ç½®
```python
# æ–‡æœ¬å¤„ç†å™¨é…ç½®
text_processor = TextProcessor(language='mixed')
text_processor.remove_urls = True
text_processor.remove_mentions = True

# å›¾åƒå¤„ç†å™¨é…ç½®
image_processor = ImageProcessor(target_size=(224, 224))
```

## ğŸ“ ç¤ºä¾‹ä»£ç 

### æ–‡æœ¬å¤„ç†æ¼”ç¤º
```python
def demo_text_processing():
    """æ¼”ç¤ºæ–‡æœ¬å¤„ç†åŠŸèƒ½"""
    print("ğŸ“ æ–‡æœ¬å¤„ç†æ¼”ç¤º:")
    
    processor = TextProcessor(language='mixed')
    
    test_texts = [
        "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ This is a test!",
        "åŒ…å«URLçš„æ–‡æœ¬ https://example.com å’Œ@username",
        "æ··åˆè¯­è¨€æ–‡æœ¬ with English words ä¸­æ–‡å­—ç¬¦"
    ]
    
    for i, text in enumerate(test_texts, 1):
        print(f"\næµ‹è¯• {i}: {text}")
        
        # è¯­è¨€æ£€æµ‹
        language = processor.detect_language(text)
        print(f"  è¯­è¨€: {language}")
        
        # æ–‡æœ¬æ¸…æ´—
        cleaned = processor.clean_text(text)
        print(f"  æ¸…æ´—å: {cleaned}")
        
        # åˆ†è¯
        tokens = processor.tokenize(text)
        print(f"  åˆ†è¯ç»“æœ: {tokens[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
        
        # ç‰¹å¾æå–
        features = processor.extract_features(text)
        print(f"  ç‰¹å¾: é•¿åº¦={features['text_length']}, "
              f"è¯æ•°={features['token_count']}, "
              f"è¯­è¨€={features['language']}")
```

### å›¾åƒå¤„ç†æ¼”ç¤º
```python
def demo_image_processing():
    """æ¼”ç¤ºå›¾åƒå¤„ç†åŠŸèƒ½"""
    print("ğŸ–¼ï¸ å›¾åƒå¤„ç†æ¼”ç¤º:")
    
    processor = ImageProcessor(target_size=(224, 224))
    
    # å¤„ç†å•å¼ å›¾åƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    test_image_dir = Path("../data/train/img")
    if test_image_dir.exists():
        image_files = list(test_image_dir.glob("*.jpg"))
        if image_files:
            test_image = image_files[0]
            print(f"æµ‹è¯•å›¾åƒ: {test_image}")
            
            # è·å–å›¾åƒä¿¡æ¯
            img_info = processor.get_image_info(test_image)
            print(f"å›¾åƒä¿¡æ¯: {img_info}")
            
            # å¤„ç†å›¾åƒ
            tensor = processor.process_single_image(test_image)
            if tensor is not None:
                print(f"å¤„ç†ç»“æœtensorå½¢çŠ¶: {tensor.shape}")
```

## ğŸ§ª æµ‹è¯•åŠŸèƒ½

### ä¾èµ–åº“æ£€æŸ¥
```python
def check_dependencies():
    """æ£€æŸ¥ä¾èµ–åº“æ˜¯å¦å®‰è£…"""
    try:
        import jieba
        import nltk
        import PIL
        import cv2
        import torch
        import torchvision
        print("âœ… æ‰€æœ‰ä¾èµ–åº“æ£€æŸ¥é€šè¿‡")
        return True
    except ImportError as e:
        print(f"âŒ ä¾èµ–åº“ç¼ºå¤±: {e}")
        return False
```

### åŠŸèƒ½å®Œæ•´æ€§æµ‹è¯•
```python
def test_functionality():
    """æµ‹è¯•åŠŸèƒ½å®Œæ•´æ€§"""
    # æ–‡æœ¬å¤„ç†æµ‹è¯•
    text_processor = TextProcessor()
    test_text = "æµ‹è¯•æ–‡æœ¬ test text"
    
    try:
        cleaned = text_processor.clean_text(test_text)
        tokens = text_processor.tokenize(test_text)
        features = text_processor.extract_features(test_text)
        print("âœ… æ–‡æœ¬å¤„ç†åŠŸèƒ½æ­£å¸¸")
    except Exception as e:
        print(f"âŒ æ–‡æœ¬å¤„ç†é”™è¯¯: {e}")
    
    # å›¾åƒå¤„ç†æµ‹è¯•
    image_processor = ImageProcessor()
    try:
        # åˆ›å»ºæµ‹è¯•å›¾åƒå¼ é‡
        test_tensor = torch.zeros(3, 224, 224)
        print("âœ… å›¾åƒå¤„ç†åŠŸèƒ½æ­£å¸¸")
    except Exception as e:
        print(f"âŒ å›¾åƒå¤„ç†é”™è¯¯: {e}")
```

## ğŸ“ˆ æ€§èƒ½æµ‹è¯•

### å¤„ç†é€Ÿåº¦æµ‹è¯•
```python
def benchmark_performance():
    """ç®€å•çš„æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    import time
    
    # æ–‡æœ¬å¤„ç†é€Ÿåº¦
    processor = TextProcessor()
    test_texts = ["æµ‹è¯•æ–‡æœ¬"] * 100
    
    start_time = time.time()
    for text in test_texts:
        processor.tokenize(text)
    text_time = time.time() - start_time
    
    print(f"æ–‡æœ¬å¤„ç†é€Ÿåº¦: {len(test_texts)/text_time:.1f} æ–‡æœ¬/ç§’")
```

## âš ï¸ ä½¿ç”¨è¯´æ˜

### è¿è¡Œç¯å¢ƒè¦æ±‚
- Python 3.7+
- å·²å®‰è£…é¡¹ç›®ä¾èµ–åº“
- è¶³å¤Ÿçš„å†…å­˜è¿›è¡Œå›¾åƒå¤„ç†

### æµ‹è¯•æ•°æ®è¦æ±‚
- æ–‡æœ¬æµ‹è¯•: å†…ç½®æµ‹è¯•æ–‡æœ¬ï¼Œæ— éœ€é¢å¤–æ•°æ®
- å›¾åƒæµ‹è¯•: éœ€è¦MR2æ•°æ®é›†æˆ–æµ‹è¯•å›¾åƒæ–‡ä»¶
- æ‰¹é‡æµ‹è¯•: éœ€è¦å®Œæ•´çš„æ•°æ®ç›®å½•ç»“æ„

### å¸¸è§é—®é¢˜
1. **ä¾èµ–åº“ç¼ºå¤±**: è¿è¡Œå‰æ£€æŸ¥å¹¶å®‰è£…æ‰€éœ€åº“
2. **æ•°æ®æ–‡ä»¶ç¼ºå¤±**: å›¾åƒæ¼”ç¤ºéœ€è¦æµ‹è¯•å›¾åƒæ–‡ä»¶
3. **å†…å­˜ä¸è¶³**: å¤§æ‰¹é‡å¤„ç†æ—¶æ³¨æ„å†…å­˜ä½¿ç”¨
4. **ç¼–ç é—®é¢˜**: ç¡®ä¿æ–‡æœ¬æ–‡ä»¶ä½¿ç”¨UTF-8ç¼–ç 

### æ•…éšœæ’é™¤
- æ£€æŸ¥Pythonç‰ˆæœ¬å’Œä¾èµ–åº“ç‰ˆæœ¬
- ç¡®è®¤æ•°æ®æ–‡ä»¶è·¯å¾„æ­£ç¡®
- æŸ¥çœ‹æ§åˆ¶å°é”™è¯¯ä¿¡æ¯
- è°ƒæ•´æ‰¹å¤„ç†å¤§å°å‡å°‘å†…å­˜å ç”¨

---

**[â¬…ï¸ å›¾åƒå¤„ç†](image_processing.md) | [æ¨¡å‹åº“æ¨¡å— â¡ï¸](../models/README.md)**
