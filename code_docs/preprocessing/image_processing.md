# å›¾åƒå¤„ç†å™¨ Image Processing

> ğŸ–¼ï¸ **ä¸“ä¸šçš„å›¾åƒé¢„å¤„ç†ã€ç‰¹å¾æå–å’Œæ‰¹é‡å¤„ç†å·¥å…·ï¼Œä¸“ä¸ºMR2æ•°æ®é›†ä¼˜åŒ–**

## ğŸ“‹ åŠŸèƒ½æ¦‚è§ˆ

`ImageProcessor`æ˜¯ä¸“é—¨ä¸ºMR2æ•°æ®é›†è®¾è®¡çš„å›¾åƒå¤„ç†å™¨ï¼Œæä¾›å›¾åƒåŠ è½½ã€é¢„å¤„ç†ã€ç‰¹å¾æå–å’Œæ‰¹é‡å¤„ç†åŠŸèƒ½ï¼Œæ”¯æŒå¤šç§æ•°æ®å¢å¼ºç­–ç•¥ã€‚

## ğŸš€ æ ¸å¿ƒç±»

### ImageProcessor
ä¸»è¦å›¾åƒå¤„ç†ç±»ï¼Œé›†æˆé…ç½®ç®¡ç†å’Œæ‰¹é‡å¤„ç†ï¼š

```python
from preprocessing import ImageProcessor

# åˆ›å»ºå›¾åƒå¤„ç†å™¨
processor = ImageProcessor(target_size=(224, 224))
```

#### åˆå§‹åŒ–å‚æ•°
```python
def __init__(self, target_size: Tuple[int, int] = (224, 224)):
    """
    åˆå§‹åŒ–å›¾åƒå¤„ç†å™¨
    
    Args:
        target_size: ç›®æ ‡å›¾åƒå°ºå¯¸ (height, width)
                    é»˜è®¤(224, 224)é€‚é…å¤§å¤šæ•°é¢„è®­ç»ƒæ¨¡å‹
    """
```

### è‡ªåŠ¨é…ç½®é›†æˆ
```python
# è‡ªåŠ¨ä»é…ç½®ç®¡ç†å™¨åŠ è½½å‚æ•°
if USE_CONFIG:
    config = get_data_config()
    self.processing_config = config.get('processing', {}).get('image', {})
    self.data_dir = get_data_dir()
else:
    self.processing_config = {}
    self.data_dir = Path('data')

# è®¾ç½®å¤„ç†å‚æ•°
self.normalize_mean = self.processing_config.get('normalize_mean', [0.485, 0.456, 0.406])
self.normalize_std = self.processing_config.get('normalize_std', [0.229, 0.224, 0.225])
self.quality_threshold = self.processing_config.get('quality_threshold', 0.3)
```

## ğŸ”§ å›¾åƒå˜æ¢é…ç½®

### è‡ªåŠ¨å˜æ¢è®¾ç½®
```python
def setup_transforms(self):
    """è®¾ç½®ä¸‰ç§ç±»å‹çš„å›¾åƒå˜æ¢"""
    
    # 1. è®­ç»ƒå˜æ¢ï¼ˆåŒ…å«æ•°æ®å¢å¼ºï¼‰
    self.train_transforms = transforms.Compose([
        transforms.Resize(self.target_size),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, 
                              saturation=0.2, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=self.normalize_mean, 
                           std=self.normalize_std)
    ])
    
    # 2. éªŒè¯å˜æ¢ï¼ˆæ— å¢å¼ºï¼‰
    self.val_transforms = transforms.Compose([
        transforms.Resize(self.target_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=self.normalize_mean, 
                           std=self.normalize_std)
    ])
    
    # 3. å¯è§†åŒ–å˜æ¢ï¼ˆæ— å½’ä¸€åŒ–ï¼‰
    self.visual_transforms = transforms.Compose([
        transforms.Resize(self.target_size),
        transforms.ToTensor()
    ])
```

### å˜æ¢ç±»å‹è¯´æ˜
| å˜æ¢ç±»å‹ | ç”¨é€” | ç‰¹ç‚¹ |
|----------|------|------|
| `train` | è®­ç»ƒé˜¶æ®µ | åŒ…å«éšæœºå¢å¼ºï¼Œæé«˜æ³›åŒ–èƒ½åŠ› |
| `val` | éªŒè¯/æµ‹è¯• | æ— éšæœºæ€§ï¼Œä¿è¯ç»“æœä¸€è‡´æ€§ |
| `visual` | å¯è§†åŒ– | æ— å½’ä¸€åŒ–ï¼Œä¾¿äºå›¾åƒæ˜¾ç¤º |

## ğŸ”„ æ ¸å¿ƒå¤„ç†æ–¹æ³•

### å®‰å…¨å›¾åƒåŠ è½½
```python
def load_image(self, image_path: Union[str, Path], mode: str = 'RGB') -> Optional[Image.Image]:
    """
    å®‰å…¨åŠ è½½å›¾åƒæ–‡ä»¶
    
    Args:
        image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
        mode: å›¾åƒæ¨¡å¼ ('RGB', 'RGBA', 'L')
        
    Returns:
        PIL Imageå¯¹è±¡ï¼Œå¤±è´¥è¿”å›None
        
    å®‰å…¨ç‰¹æ€§:
    - æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥
    - æ ¼å¼è‡ªåŠ¨è½¬æ¢
    - è´¨é‡éªŒè¯
    - å¼‚å¸¸å¤„ç†
    """
```

### å›¾åƒè´¨é‡éªŒè¯
```python
def validate_image(self, image: Image.Image) -> bool:
    """
    éªŒè¯å›¾åƒè´¨é‡
    
    éªŒè¯é¡¹ç›®:
    - æœ€å°å°ºå¯¸æ£€æŸ¥ (50x50)
    - å›¾åƒæ•°æ®å®Œæ•´æ€§
    - æ ¼å¼æœ‰æ•ˆæ€§
    
    Returns:
        æ˜¯å¦é€šè¿‡éªŒè¯
    """
```

### å•å¼ å›¾åƒå¤„ç†
```python
def process_single_image(self, 
                        image_path: Union[str, Path], 
                        transform_type: str = 'val',
                        apply_augment: bool = False,
                        augment_type: str = 'light') -> Optional[torch.Tensor]:
    """
    å¤„ç†å•å¼ å›¾åƒçš„å®Œæ•´æµç¨‹
    
    Args:
        image_path: å›¾åƒè·¯å¾„
        transform_type: å˜æ¢ç±»å‹ ('train', 'val', 'visual')
        apply_augment: æ˜¯å¦åº”ç”¨é¢å¤–å¢å¼º
        augment_type: å¢å¼ºçº§åˆ« ('light', 'medium', 'heavy')
        
    Returns:
        å¤„ç†åçš„tensor (3, H, W)ï¼Œå¤±è´¥è¿”å›None
    """
```

## ğŸ¨ æ•°æ®å¢å¼ºç­–ç•¥

### apply_augmentation æ–¹æ³•
```python
def apply_augmentation(self, image: Image.Image, augment_type: str = 'light') -> Image.Image:
    """åº”ç”¨ä¸åŒçº§åˆ«çš„æ•°æ®å¢å¼º"""
```

#### è½»åº¦å¢å¼º (light)
```python
if augment_type == 'light':
    if np.random.random() > 0.5:
        image = F.hflip(image)  # 50%æ¦‚ç‡æ°´å¹³ç¿»è½¬
    if np.random.random() > 0.7:
        angle = np.random.uniform(-5, 5)  # Â±5åº¦éšæœºæ—‹è½¬
        image = F.rotate(image, angle)
```

#### ä¸­åº¦å¢å¼º (medium)  
```python
elif augment_type == 'medium':
    if np.random.random() > 0.5:
        image = F.hflip(image)  # æ°´å¹³ç¿»è½¬
    if np.random.random() > 0.6:
        angle = np.random.uniform(-10, 10)  # Â±10åº¦æ—‹è½¬
        image = F.rotate(image, angle)
    if np.random.random() > 0.6:
        # äº®åº¦è°ƒæ•´ (0.8-1.2å€)
        enhancer = ImageEnhance.Brightness(image)
        factor = np.random.uniform(0.8, 1.2)
        image = enhancer.enhance(factor)
```

#### é‡åº¦å¢å¼º (heavy)
```python
elif augment_type == 'heavy':
    # æ°´å¹³ç¿»è½¬ (50%æ¦‚ç‡)
    if np.random.random() > 0.5:
        image = F.hflip(image)
        
    # æ—‹è½¬ (Â±15åº¦)
    if np.random.random() > 0.5:
        angle = np.random.uniform(-15, 15)
        image = F.rotate(image, angle)
        
    # äº®åº¦è°ƒæ•´ (0.7-1.3å€)
    if np.random.random() > 0.5:
        enhancer = ImageEnhance.Brightness(image)
        factor = np.random.uniform(0.7, 1.3)
        image = enhancer.enhance(factor)
        
    # å¯¹æ¯”åº¦è°ƒæ•´ (0.8-1.2å€)
    if np.random.random() > 0.5:
        enhancer = ImageEnhance.Contrast(image)
        factor = np.random.uniform(0.8, 1.2)
        image = enhancer.enhance(factor)
        
    # è½»å¾®é«˜æ–¯æ¨¡ç³Š
    if np.random.random() > 0.3:
        image = image.filter(ImageFilter.GaussianBlur(radius=0.5))
```

## ğŸ“Š ç‰¹å¾æå–

### extract_image_features æ–¹æ³•
```python
def extract_image_features(self, image: Image.Image) -> Dict[str, float]:
    """
    æå–å…¨é¢çš„å›¾åƒç‰¹å¾
    
    ç‰¹å¾ç±»åˆ«:
    - å‡ ä½•ç‰¹å¾: å®½åº¦ã€é«˜åº¦ã€çºµæ¨ªæ¯”ã€åƒç´ æ€»æ•°
    - é¢œè‰²ç‰¹å¾: RGBå‡å€¼å’Œæ ‡å‡†å·®
    - è´¨é‡ç‰¹å¾: äº®åº¦ã€å¯¹æ¯”åº¦
    - çº¹ç†ç‰¹å¾: è¾¹ç¼˜å¯†åº¦
    
    Returns:
        å›¾åƒç‰¹å¾å­—å…¸
    """
```

### å®Œæ•´ç‰¹å¾è¯´æ˜
```python
features = {
    # === å‡ ä½•ç‰¹å¾ ===
    'width': 640.0,                    # å›¾åƒå®½åº¦
    'height': 480.0,                   # å›¾åƒé«˜åº¦  
    'aspect_ratio': 1.33,              # å®½é«˜æ¯”
    'total_pixels': 307200.0,          # æ€»åƒç´ æ•°
    
    # === RGBé¢œè‰²ç‰¹å¾ ===
    'mean_r': 128.5,                   # çº¢è‰²é€šé“å‡å€¼
    'mean_g': 132.1,                   # ç»¿è‰²é€šé“å‡å€¼
    'mean_b': 125.8,                   # è“è‰²é€šé“å‡å€¼
    'std_r': 45.2,                     # çº¢è‰²é€šé“æ ‡å‡†å·®
    'std_g': 48.1,                     # ç»¿è‰²é€šé“æ ‡å‡†å·®
    'std_b': 42.9,                     # è“è‰²é€šé“æ ‡å‡†å·®
    
    # === å›¾åƒè´¨é‡ç‰¹å¾ ===
    'brightness': 128.8,               # æ•´ä½“äº®åº¦(0-255)
    'contrast': 45.4,                  # å¯¹æ¯”åº¦(æ ‡å‡†å·®)
    'edge_density': 0.12               # è¾¹ç¼˜å¯†åº¦(0-1)
}
```

### è¾¹ç¼˜å¯†åº¦è®¡ç®—
```python
# ä½¿ç”¨OpenCV Cannyè¾¹ç¼˜æ£€æµ‹
try:
    edges = cv2.Canny(gray.astype(np.uint8), 50, 150)
    features['edge_density'] = float(np.sum(edges > 0) / edges.size)
except:
    features['edge_density'] = 0.0
```

## ğŸ“¦ æ‰¹é‡å¤„ç†åŠŸèƒ½

### MR2æ•°æ®é›†ä¸“ç”¨æ‰¹é‡å¤„ç†
```python
def process_mr2_dataset(self, 
                       splits: List[str] = ['train', 'val', 'test'], 
                       save_features: bool = True) -> Dict[str, Dict]:
    """
    å¤„ç†å®Œæ•´çš„MR2æ•°æ®é›†
    
    Args:
        splits: è¦å¤„ç†çš„æ•°æ®åˆ’åˆ†
        save_features: æ˜¯å¦ä¿å­˜ç‰¹å¾åˆ°æ–‡ä»¶
        
    Returns:
        å¤„ç†ç»“æœå­—å…¸:
        {
            'train': {
                'total_items': int,
                'processed_images': int,
                'failed_images': int,
                'image_info': Dict,      # å›¾åƒåŸºæœ¬ä¿¡æ¯
                'image_features': Dict   # æå–çš„ç‰¹å¾
            }
        }
    """
```

### æ‰¹é‡å¤„ç†æµç¨‹
```python
# å®Œæ•´çš„æ‰¹é‡å¤„ç†æµç¨‹
for split in splits:
    print(f"\nğŸ“‚ å¤„ç† {split} æ•°æ®é›†")
    
    # 1. åŠ è½½æ•°æ®é›†ä¿¡æ¯
    dataset_file = self.data_dir / f'dataset_items_{split}.json'
    with open(dataset_file, 'r') as f:
        dataset_items = json.load(f)
    
    # 2. å¤„ç†æ¯ä¸ªæ•°æ®é¡¹
    for item_id, item_data in dataset_items.items():
        if 'image_path' not in item_data:
            continue
            
        image_path = self.data_dir / item_data['image_path']
        
        try:
            # è·å–å›¾åƒä¿¡æ¯
            img_info = self.get_image_info(image_path)
            split_results['image_info'][item_id] = img_info
            
            # æå–å›¾åƒç‰¹å¾
            image = self.load_image(image_path)
            if image is not None:
                features = self.extract_image_features(image)
                split_results['image_features'][item_id] = features
                split_results['processed_images'] += 1
            else:
                split_results['failed_images'] += 1
                
        except Exception as e:
            logger.error(f"å¤„ç†å›¾åƒå¤±è´¥ {image_path}: {e}")
            split_results['failed_images'] += 1
    
    # 3. ä¿å­˜ç‰¹å¾(å¯é€‰)
    if save_features:
        self.save_image_features(split_results, split)
```

### ç‰¹å¾ä¿å­˜
```python
def save_image_features(self, features_data: Dict, split: str):
    """
    ä¿å­˜å›¾åƒç‰¹å¾åˆ°æ–‡ä»¶
    
    ä¿å­˜ä½ç½®:
    - data/processed/{split}_image_info.json     # å›¾åƒåŸºæœ¬ä¿¡æ¯
    - data/processed/{split}_image_features.json # æå–çš„ç‰¹å¾
    """
    processed_dir = self.data_dir / 'processed'
    processed_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜å›¾åƒä¿¡æ¯
    info_file = processed_dir / f'{split}_image_info.json'
    with open(info_file, 'w', encoding='utf-8') as f:
        json.dump(features_data['image_info'], f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜å›¾åƒç‰¹å¾
    features_file = processed_dir / f'{split}_image_features.json'
    with open(features_file, 'w', encoding='utf-8') as f:
        json.dump(features_data['image_features'], f, indent=2, ensure_ascii=False)
```

## ğŸ“ˆ ç»Ÿè®¡åˆ†æ

### create_image_statistics æ–¹æ³•
```python
def create_image_statistics(self, results: Dict) -> Dict[str, Any]:
    """
    åˆ›å»ºå›¾åƒå¤„ç†ç»Ÿè®¡ä¿¡æ¯
    
    ç»Ÿè®¡å†…å®¹:
    - å›¾åƒæ•°é‡ç»Ÿè®¡
    - å°ºå¯¸åˆ†å¸ƒåˆ†æ
    - æ ¼å¼åˆ†å¸ƒç»Ÿè®¡
    - æ–‡ä»¶å¤§å°åˆ†æ
    - å¤„ç†æˆåŠŸç‡
    """
```

### ç»Ÿè®¡ä¿¡æ¯ç¤ºä¾‹
```python
statistics = {
    'total_images': 900,               # æ€»å›¾åƒæ•°
    'successful_images': 810,          # æˆåŠŸå¤„ç†æ•°
    'failed_images': 90,               # å¤„ç†å¤±è´¥æ•°
    'success_rate': 0.9,               # æˆåŠŸç‡
    
    'size_distribution': {
        'avg_width': 640.5,            # å¹³å‡å®½åº¦
        'avg_height': 480.2,           # å¹³å‡é«˜åº¦
        'max_width': 1920,             # æœ€å¤§å®½åº¦
        'min_width': 128,              # æœ€å°å®½åº¦
    },
    
    'format_distribution': {
        'JPEG': 720,                   # JPEGæ ¼å¼æ•°é‡
        'PNG': 90                      # PNGæ ¼å¼æ•°é‡
    },
    
    'avg_file_size': 125.6,            # å¹³å‡æ–‡ä»¶å¤§å°(KB)
    'total_file_size': 112.8           # æ€»æ–‡ä»¶å¤§å°(MB)
}
```

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨
```python
from preprocessing import ImageProcessor

# åˆ›å»ºå¤„ç†å™¨
processor = ImageProcessor(target_size=(224, 224))

# å¤„ç†å•å¼ å›¾åƒ
image_path = 'data/train/img/example.jpg'

# 1. è·å–å›¾åƒä¿¡æ¯
info = processor.get_image_info(image_path)
print(f"å›¾åƒä¿¡æ¯:")
print(f"  å°ºå¯¸: {info['width']} x {info['height']}")
print(f"  æ ¼å¼: {info['format']}")
print(f"  æ–‡ä»¶å¤§å°: {info['file_size_mb']:.2f} MB")

# 2. å¤„ç†å›¾åƒ
tensor = processor.process_single_image(
    image_path, 
    transform_type='train'  # åŒ…å«æ•°æ®å¢å¼º
)
print(f"å¤„ç†åå¼ é‡å½¢çŠ¶: {tensor.shape}")  # torch.Size([3, 224, 224])

# 3. æå–ç‰¹å¾
image = processor.load_image(image_path)
features = processor.extract_image_features(image)
print(f"å›¾åƒç‰¹å¾:")
print(f"  äº®åº¦: {features['brightness']:.2f}")
print(f"  å¯¹æ¯”åº¦: {features['contrast']:.2f}")
print(f"  è¾¹ç¼˜å¯†åº¦: {features['edge_density']:.3f}")
```

### æ•°æ®å¢å¼ºæµ‹è¯•
```python
# æµ‹è¯•ä¸åŒçº§åˆ«çš„æ•°æ®å¢å¼º
image = processor.load_image('data/train/img/example.jpg')

# è½»åº¦å¢å¼º
light_aug = processor.apply_augmentation(image, 'light')
processor.save_image_with_info(light_aug, 'outputs/light_augmented.jpg')

# ä¸­åº¦å¢å¼º
medium_aug = processor.apply_augmentation(image, 'medium')
processor.save_image_with_info(medium_aug, 'outputs/medium_augmented.jpg')

# é‡åº¦å¢å¼º
heavy_aug = processor.apply_augmentation(image, 'heavy')
processor.save_image_with_info(heavy_aug, 'outputs/heavy_augmented.jpg')
```

### æ‰¹é‡å¤„ç†MR2æ•°æ®é›†
```python
# å¤„ç†å®Œæ•´æ•°æ®é›†
print("ğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç†MR2æ•°æ®é›†...")

results = processor.process_mr2_dataset(
    splits=['train', 'val', 'test'],
    save_features=True
)

# æŸ¥çœ‹å¤„ç†ç»“æœ
for split, stats in results.items():
    success_rate = stats['processed_images'] / stats['total_items'] * 100
    print(f"{split.upper()}:")
    print(f"  æ€»æ•°: {stats['total_items']}")
    print(f"  æˆåŠŸ: {stats['processed_images']}")
    print(f"  å¤±è´¥: {stats['failed_images']}")
    print(f"  æˆåŠŸç‡: {success_rate:.1f}%")

# åˆ›å»ºç»Ÿè®¡ä¿¡æ¯
statistics = processor.create_image_statistics(results)
print(f"\nğŸ“Š æ•´ä½“ç»Ÿè®¡:")
print(f"æ€»å›¾åƒæ•°: {statistics['total_images']}")
print(f"æˆåŠŸå¤„ç†: {statistics['successful_images']}")
print(f"å¹³å‡å°ºå¯¸: {statistics.get('avg_width', 0):.0f} x {statistics.get('avg_height', 0):.0f}")
print(f"æ ¼å¼åˆ†å¸ƒ: {statistics.get('format_distribution', {})}")
```

### è‡ªå®šä¹‰å˜æ¢é…ç½®
```python
import torchvision.transforms as transforms

# åˆ›å»ºè‡ªå®šä¹‰å˜æ¢
custom_transforms = transforms.Compose([
    transforms.Resize((256, 256)),           # å…ˆæ”¾å¤§
    transforms.RandomCrop((224, 224)),       # éšæœºè£å‰ª
    transforms.RandomHorizontalFlip(p=0.7),  # æ›´é«˜ç¿»è½¬æ¦‚ç‡
    transforms.ColorJitter(
        brightness=0.3, 
        contrast=0.3, 
        saturation=0.2, 
        hue=0.1
    ),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406], 
        std=[0.229, 0.224, 0.225]
    )
])

# åº”ç”¨è‡ªå®šä¹‰å˜æ¢
processor.train_transforms = custom_transforms

# å¤„ç†å›¾åƒ
tensor = processor.process_single_image(
    'data/train/img/example.jpg', 
    transform_type='train'
)
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### æ‰¹é‡å¤„ç†ä¼˜åŒ–
```python
def batch_process_images(self, 
                        image_paths: List[Union[str, Path]], 
                        transform_type: str = 'val',
                        batch_size: int = 32) -> List[torch.Tensor]:
    """
    é«˜æ•ˆçš„æ‰¹é‡å›¾åƒå¤„ç†
    
    ä¼˜åŒ–ç­–ç•¥:
    - åˆ†æ‰¹å¤„ç†å‡å°‘å†…å­˜å ç”¨
    - å¹¶è¡ŒåŠ è½½å’Œé¢„å¤„ç†
    - è‡ªåŠ¨é”™è¯¯æ¢å¤
    """
    processed_tensors = []
    
    for i in range(0, len(image_paths), batch_size):
        batch_paths = image_paths[i:i+batch_size]
        batch_tensors = []
        
        for path in batch_paths:
            tensor = self.process_single_image(path, transform_type)
            if tensor is not None:
                batch_tensors.append(tensor)
        
        if batch_tensors:
            # å †å tensorä»¥èŠ‚çœå†…å­˜
            batch_tensor = torch.stack(batch_tensors)
            processed_tensors.append(batch_tensor)
            
        # å¯é€‰ï¼šæ˜¾ç¤ºè¿›åº¦
        if (i // batch_size) % 10 == 0:
            print(f"å·²å¤„ç†: {i + len(batch_paths)}/{len(image_paths)}")
    
    return processed_tensors
```

### å†…å­˜ä¼˜åŒ–ç­–ç•¥
```python
# 1. åŠæ—¶é‡Šæ”¾å›¾åƒå¯¹è±¡
def process_with_memory_optimization(self, image_path):
    try:
        image = self.load_image(image_path)
        
        # æå–ç‰¹å¾
        features = self.extract_image_features(image)
        
        # å¤„ç†å›¾åƒ
        tensor = self.image_transforms(image)
        
        # åŠæ—¶åˆ é™¤imageå¯¹è±¡
        del image
        
        return tensor, features
    except Exception as e:
        return None, {}

# 2. åˆ†å—å¤„ç†å¤§æ•°æ®é›†
def process_large_dataset_chunked(self, dataset_size, chunk_size=1000):
    """åˆ†å—å¤„ç†å¤§æ•°æ®é›†ä»¥é¿å…å†…å­˜æº¢å‡º"""
    for chunk_start in range(0, dataset_size, chunk_size):
        chunk_end = min(chunk_start + chunk_size, dataset_size)
        
        # å¤„ç†å½“å‰å—
        self._process_chunk(chunk_start, chunk_end)
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
```

### è´¨é‡æ§åˆ¶ä¼˜åŒ–
```python
def enhanced_quality_control(self, image_path):
    """å¢å¼ºçš„å›¾åƒè´¨é‡æ§åˆ¶"""
    
    # 1. æ–‡ä»¶çº§æ£€æŸ¥
    if not os.path.exists(image_path):
        return False, "æ–‡ä»¶ä¸å­˜åœ¨"
    
    file_size = os.path.getsize(image_path)
    if file_size < 1024:  # å°äº1KB
        return False, "æ–‡ä»¶è¿‡å°"
    
    # 2. å›¾åƒçº§æ£€æŸ¥
    try:
        with Image.open(image_path) as image:
            # å°ºå¯¸æ£€æŸ¥
            if image.width < 50 or image.height < 50:
                return False, "å›¾åƒå°ºå¯¸è¿‡å°"
            
            # å®½é«˜æ¯”æ£€æŸ¥
            aspect_ratio = image.width / image.height
            if aspect_ratio > 10 or aspect_ratio < 0.1:
                return False, "å®½é«˜æ¯”å¼‚å¸¸"
            
            # é¢œè‰²é€šé“æ£€æŸ¥
            if len(image.getbands()) not in [1, 3, 4]:
                return False, "é¢œè‰²é€šé“å¼‚å¸¸"
                
        return True, "è´¨é‡æ£€æŸ¥é€šè¿‡"
        
    except Exception as e:
        return False, f"å›¾åƒæŸå: {e}"
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### å›¾åƒå°ºå¯¸æ™ºèƒ½è°ƒæ•´
```python
def resize_image(self, 
                image: Image.Image, 
                size: Optional[Tuple[int, int]] = None, 
                method: str = 'lanczos') -> Image.Image:
    """
    æ™ºèƒ½å›¾åƒå°ºå¯¸è°ƒæ•´
    
    Args:
        image: PIL Imageå¯¹è±¡
        size: ç›®æ ‡å°ºå¯¸ï¼ŒNoneæ—¶ä½¿ç”¨self.target_size
        method: é‡é‡‡æ ·æ–¹æ³•
                'lanczos' - é«˜è´¨é‡(é»˜è®¤)
                'bilinear' - å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦
                'bicubic' - é«˜è´¨é‡æ…¢é€Ÿ
    """
    if size is None:
        size = self.target_size
    
    # é€‰æ‹©é‡é‡‡æ ·æ–¹æ³•
    resample_methods = {
        'lanczos': Image.Resampling.LANCZOS,
        'bilinear': Image.Resampling.BILINEAR,
        'bicubic': Image.Resampling.BICUBIC
    }
    resample = resample_methods.get(method, Image.Resampling.LANCZOS)
    
    return image.resize(size, resample)
```

### å›¾åƒä¿¡æ¯è·å–å¢å¼º
```python
def get_image_info(self, image_path: Union[str, Path]) -> Dict[str, Any]:
    """
    è·å–è¯¦ç»†çš„å›¾åƒä¿¡æ¯
    
    Returns:
        {
            'path': str,              # å›¾åƒè·¯å¾„
            'filename': str,          # æ–‡ä»¶å
            'format': str,            # å›¾åƒæ ¼å¼
            'mode': str,              # é¢œè‰²æ¨¡å¼
            'size': tuple,            # å°ºå¯¸ (width, height)
            'width': int,             # å®½åº¦
            'height': int,            # é«˜åº¦
            'file_size': int,         # æ–‡ä»¶å¤§å°(å­—èŠ‚)
            'file_size_mb': float,    # æ–‡ä»¶å¤§å°(MB)
            'aspect_ratio': float,    # å®½é«˜æ¯”
            'color_channels': int,    # é¢œè‰²é€šé“æ•°
            'has_transparency': bool, # æ˜¯å¦æœ‰é€æ˜é€šé“
            'creation_time': str      # åˆ›å»ºæ—¶é—´(å¦‚æœæœ‰)
        }
    """
    try:
        with Image.open(image_path) as image:
            file_size = os.path.getsize(image_path)
            
            # åŸºç¡€ä¿¡æ¯
            info = {
                'path': str(image_path),
                'filename': os.path.basename(image_path),
                'format': image.format,
                'mode': image.mode,
                'size': image.size,
                'width': image.width,
                'height': image.height,
                'file_size': file_size,
                'file_size_mb': round(file_size / (1024 * 1024), 2),
                'aspect_ratio': round(image.width / image.height, 2)
            }
            
            # é«˜çº§ä¿¡æ¯
            info['color_channels'] = len(image.getbands())
            info['has_transparency'] = 'transparency' in image.info or image.mode in ('RGBA', 'LA')
            
            # å°è¯•è·å–EXIFä¿¡æ¯
            try:
                exif = image._getexif()
                if exif and 306 in exif:  # DateTimeæ ‡ç­¾
                    info['creation_time'] = exif[306]
            except:
                pass
                
            return info
            
    except Exception as e:
        logger.error(f"è·å–å›¾åƒä¿¡æ¯å¤±è´¥ {image_path}: {e}")
        return {}
```

## ğŸš¨ é”™è¯¯å¤„ç†å’Œæ—¥å¿—

### å®Œå–„çš„é”™è¯¯å¤„ç†
```python
def load_image_safe(self, image_path: str) -> Dict[str, Any]:
    """
    å®‰å…¨çš„å›¾åƒåŠ è½½ï¼ŒåŒ…å«å®Œæ•´é”™è¯¯å¤„ç†
    """
    full_image_path = self.data_dir / image_path
    
    try:
        # 1. æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥
        if not full_image_path.exists():
            logger.warning(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {full_image_path}")
            return self.create_empty_image_result(str(full_image_path))
        
        # 2. æ–‡ä»¶å¤§å°æ£€æŸ¥
        file_size = full_image_path.stat().st_size
        if file_size == 0:
            logger.warning(f"å›¾åƒæ–‡ä»¶ä¸ºç©º: {full_image_path}")
            return self.create_empty_image_result(str(full_image_path))
        
        # 3. å°è¯•åŠ è½½å›¾åƒ
        with Image.open(full_image_path) as image:
            # è½¬æ¢ä¸ºRGBæ¨¡å¼
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # è´¨é‡æ£€æŸ¥
            if not self.validate_image(image):
                logger.warning(f"å›¾åƒè´¨é‡æ£€æŸ¥æœªé€šè¿‡: {full_image_path}")
                return self.create_empty_image_result(str(full_image_path))
            
            # åº”ç”¨å˜æ¢
            image_tensor = self.image_transforms(image)
            
            return {
                'image': image_tensor,
                'has_image': True,
                'image_path': str(full_image_path),
                'image_size': image.size,
                'load_status': 'success'
            }
            
    except OSError as e:
        logger.error(f"æ–‡ä»¶ç³»ç»Ÿé”™è¯¯ {full_image_path}: {e}")
        return self.create_empty_image_result(str(full_image_path))
    except Image.UnidentifiedImageError as e:
        logger.error(f"æ— æ³•è¯†åˆ«çš„å›¾åƒæ ¼å¼ {full_image_path}: {e}")
        return self.create_empty_image_result(str(full_image_path))
    except Exception as e:
        logger.error(f"å¤„ç†å›¾åƒå¤±è´¥ {full_image_path}: {e}")
        return self.create_empty_image_result(str(full_image_path))

def create_empty_image_result(self, image_path: str) -> Dict[str, Any]:
    """åˆ›å»ºç©ºå›¾åƒç»“æœï¼Œç”¨äºé”™è¯¯æƒ…å†µ"""
    return {
        'image': torch.zeros(3, *self.target_size),
        'has_image': False,
        'image_path': image_path,
        'image_size': None,
        'load_status': 'failed'
    }
```

## ğŸ§ª æµ‹è¯•å’ŒéªŒè¯

### å†…ç½®æµ‹è¯•åŠŸèƒ½
```python
def test_image_processor():
    """å®Œæ•´çš„å›¾åƒå¤„ç†å™¨æµ‹è¯•"""
    print("ğŸ–¼ï¸ æµ‹è¯•å›¾åƒå¤„ç†æ¨¡å—")
    
    processor = ImageProcessor(target_size=(224, 224))
    
    # 1. æµ‹è¯•å›¾åƒä¿¡æ¯è·å–
    test_image_dir = Path("data/train/img")
    if test_image_dir.exists():
        image_files = list(test_image_dir.glob("*.jpg"))
        if image_files:
            test_image = image_files[0]
            print(f"æµ‹è¯•å›¾åƒ: {test_image}")
            
            # è·å–å›¾åƒä¿¡æ¯
            img_info = processor.get_image_info(test_image)
            if img_info:
                print(f"å›¾åƒä¿¡æ¯: {img_info}")
            
            # å¤„ç†å›¾åƒ
            tensor = processor.process_single_image(test_image, transform_type='val')
            if tensor is not None:
                print(f"å¤„ç†ç»“æœtensorå½¢çŠ¶: {tensor.shape}")
                
            # æå–ç‰¹å¾
            image = processor.load_image(test_image)
            if image is not None:
                features = processor.extract_image_features(image)
                print(f"å›¾åƒç‰¹å¾æ•°é‡: {len(features)}")
                
    # 2. æµ‹è¯•æ‰¹é‡å¤„ç†
    print("\nğŸ”„ æµ‹è¯•æ‰¹é‡å¤„ç†...")
    try:
        results = processor.process_mr2_dataset(splits=['train'], save_features=False)
        if results:
            stats = results['train']
            print(f"æ‰¹é‡å¤„ç†ç»“æœ: æˆåŠŸ{stats['processed_images']}, å¤±è´¥{stats['failed_images']}")
    except Exception as e:
        print(f"æ‰¹é‡å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
    
    print("âœ… å›¾åƒå¤„ç†æ¨¡å—æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    test_image_processor()
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. ç›®æ ‡å°ºå¯¸é€‰æ‹©
```python
# æ ¹æ®æ¨¡å‹é€‰æ‹©åˆé€‚çš„å°ºå¯¸
model_target_sizes = {
    'resnet': (224, 224),      # ResNetæ ‡å‡†å°ºå¯¸
    'efficientnet': (224, 224), # EfficientNet-B0
    'vit': (224, 224),         # Vision Transformer
    'clip': (224, 224),        # CLIPæ ‡å‡†å°ºå¯¸
    'swin': (224, 224),        # Swin Transformer
    'custom_small': (128, 128), # è‡ªå®šä¹‰å°å°ºå¯¸
    'custom_large': (384, 384)  # è‡ªå®šä¹‰å¤§å°ºå¯¸
}

target_size = model_target_sizes.get('resnet', (224, 224))
processor = ImageProcessor(target_size=target_size)
```

### 2. æ•°æ®å¢å¼ºç­–ç•¥é€‰æ‹©
```python
# æ ¹æ®æ•°æ®é›†å¤§å°é€‰æ‹©å¢å¼ºç­–ç•¥
def choose_augmentation_strategy(dataset_size):
    if dataset_size < 1000:
        return 'heavy'    # å°æ•°æ®é›†ç”¨é‡åº¦å¢å¼º
    elif dataset_size < 5000:
        return 'medium'   # ä¸­ç­‰æ•°æ®é›†ç”¨ä¸­åº¦å¢å¼º
    else:
        return 'light'    # å¤§æ•°æ®é›†ç”¨è½»åº¦å¢å¼º

# åº”ç”¨ç­–ç•¥
augment_type = choose_augmentation_strategy(len(dataset))
```

### 3. è´¨é‡æ§åˆ¶é…ç½®
```python
# ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶
strict_processor = ImageProcessor(target_size=(224, 224))
strict_processor.quality_threshold = 0.8  # æé«˜è´¨é‡é˜ˆå€¼

# å®½æ¾çš„è´¨é‡æ§åˆ¶ï¼ˆé€‚ç”¨äºæ•°æ®ç¨€ç¼ºæƒ…å†µï¼‰
lenient_processor = ImageProcessor(target_size=(224, 224))
lenient_processor.quality_threshold = 0.2  # é™ä½è´¨é‡é˜ˆå€¼
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### ä¾èµ–åº“è¦æ±‚
- **PIL (Pillow)**: 9.0+ï¼Œå›¾åƒåŸºç¡€å¤„ç†
- **OpenCV**: 4.5+ï¼Œè¾¹ç¼˜æ£€æµ‹å’Œé«˜çº§å¤„ç†
- **PyTorch**: 1.10+ï¼Œå¼ é‡å¤„ç†å’Œå˜æ¢
- **torchvision**: 0.11+ï¼Œé¢„å®šä¹‰å˜æ¢
- **numpy**: 1.20+ï¼Œæ•°å€¼è®¡ç®—

### å†…å­˜ç®¡ç†
- å¤§å›¾åƒå¤„ç†æ—¶æ³¨æ„å†…å­˜å ç”¨
- æ‰¹é‡å¤„ç†æ—¶æ§åˆ¶batch_size
- åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å›¾åƒå¯¹è±¡
- ä½¿ç”¨é€‚å½“çš„å›¾åƒå‹ç¼©è´¨é‡

### æ€§èƒ½è€ƒè™‘
- JPEGæ ¼å¼é€šå¸¸æ¯”PNGå¤„ç†æ›´å¿«
- è¾ƒå°çš„target_sizeèƒ½æå‡å¤„ç†é€Ÿåº¦
- å¤šè¿›ç¨‹å¤„ç†å¤§æ•°æ®é›†æ—¶æ³¨æ„ç³»ç»Ÿèµ„æº

---

**[â¬…ï¸ æ–‡æœ¬å¤„ç†](text_processing.md) | [æ¼”ç¤ºè„šæœ¬ â¡ï¸](demo.md)**