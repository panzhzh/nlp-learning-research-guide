# æ–‡æœ¬å¤„ç†å™¨ Text Processing

> ğŸ“ **ä¸“ä¸šçš„ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬é¢„å¤„ç†å™¨ï¼Œæ”¯æŒæ™ºèƒ½åˆ†è¯å’Œå¤šç»´ç‰¹å¾æå–**

## ğŸ“‹ åŠŸèƒ½æ¦‚è§ˆ

`TextProcessor`æ˜¯ä¸“é—¨ä¸ºMR2æ•°æ®é›†è®¾è®¡çš„æ–‡æœ¬é¢„å¤„ç†å™¨ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··åˆæ–‡æœ¬çš„åˆ†è¯ã€æ¸…æ´—ã€æ ‡å‡†åŒ–å’Œç‰¹å¾æå–ã€‚

## ğŸš€ æ ¸å¿ƒç±»

### TextProcessor
ä¸»è¦æ–‡æœ¬å¤„ç†ç±»ï¼Œæ”¯æŒå¤šè¯­è¨€æ™ºèƒ½å¤„ç†ï¼š

```python
from preprocessing import TextProcessor

# åˆå§‹åŒ–å¤„ç†å™¨
processor = TextProcessor(language='mixed')
```

#### åˆå§‹åŒ–å‚æ•°
```python
def __init__(self, language: str = 'mixed'):
    """
    åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨
    
    Args:
        language: è¯­è¨€ç±»å‹
                 'chinese' - çº¯ä¸­æ–‡å¤„ç†
                 'english' - çº¯è‹±æ–‡å¤„ç†  
                 'mixed' - ä¸­è‹±æ–‡æ··åˆå¤„ç†ï¼ˆæ¨èï¼‰
    """
```

## ğŸ”§ æ ¸å¿ƒæ–¹æ³•

### è¯­è¨€æ£€æµ‹
```python
def detect_language(self, text: str) -> str:
    """
    æ™ºèƒ½æ£€æµ‹æ–‡æœ¬è¯­è¨€ç±»å‹
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        
    Returns:
        'chinese' | 'english' | 'mixed' | 'unknown'
    """
```

**æ£€æµ‹é€»è¾‘**ï¼š
```python
# ç»Ÿè®¡å­—ç¬¦ç±»å‹æ¯”ä¾‹
chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
english_chars = len(re.findall(r'[a-zA-Z]', text))

chinese_ratio = chinese_chars / total_chars
english_ratio = english_chars / total_chars

if chinese_ratio > 0.7:
    return 'chinese'
elif english_ratio > 0.7:
    return 'english'
else:
    return 'mixed'
```

### æ–‡æœ¬æ¸…æ´—
```python
def clean_text(self, text: str) -> str:
    """
    æ·±åº¦æ–‡æœ¬æ¸…æ´—
    
    æ¸…æ´—å†…å®¹:
    - URLç§»é™¤ (httpé“¾æ¥å’Œwwwé“¾æ¥)
    - @æåŠç§»é™¤
    - #è¯é¢˜æ ‡ç­¾å¤„ç†ï¼ˆå¯é€‰ï¼‰
    - emojiè½¬æ–‡æœ¬æè¿°
    - HTMLæ ‡ç­¾ç§»é™¤
    - æ ‡ç‚¹ç¬¦å·æ ‡å‡†åŒ–
    - ç©ºç™½å­—ç¬¦è§„èŒƒåŒ–
    """
```

**æ¸…æ´—ç¤ºä¾‹**ï¼š
```python
original = "è¿™æ˜¯æµ‹è¯• @user #topic https://example.com ğŸ˜Š!!!"
cleaned = processor.clean_text(original)
print(cleaned)
# è¾“å‡º: "è¿™æ˜¯æµ‹è¯• ğŸ˜Š"
```

### æ™ºèƒ½åˆ†è¯

#### ç»Ÿä¸€åˆ†è¯æ¥å£
```python
def tokenize(self, text: str, language: Optional[str] = None) -> List[str]:
    """
    ç»Ÿä¸€åˆ†è¯æ¥å£ï¼Œè‡ªåŠ¨é€‰æ‹©åˆ†è¯ç­–ç•¥
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        language: æŒ‡å®šè¯­è¨€ï¼ˆNoneæ—¶è‡ªåŠ¨æ£€æµ‹ï¼‰
        
    Returns:
        åˆ†è¯ç»“æœåˆ—è¡¨
    """
```

#### ä¸­æ–‡åˆ†è¯
```python
def tokenize_chinese(self, text: str) -> List[str]:
    """
    ä¸­æ–‡ä¸“ç”¨åˆ†è¯
    
    ç‰¹ç‚¹:
    - ä½¿ç”¨jiebaåˆ†è¯å¼•æ“
    - è‡ªåŠ¨è¿‡æ»¤åœç”¨è¯
    - å»é™¤æ— æ„ä¹‰å­—ç¬¦
    - æ”¯æŒè‡ªå®šä¹‰è¯å…¸
    """
```

**ä¸­æ–‡åœç”¨è¯**ï¼ˆå†…ç½®ï¼‰ï¼š
```python
chinese_stopwords = {
    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'æœ‰', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ',
    'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'è¿™', 'é‚£', 'è¿™ä¸ª', 'é‚£ä¸ª', 'ä¸Š', 'ä¸‹',
    'ä¸­', 'å¤§', 'å°', 'å¤š', 'å°‘', 'å¥½', 'å', 'å¯¹', 'é”™', 'æ²¡', 'ä¸'
    # ... æ›´å¤šåœç”¨è¯
}
```

#### è‹±æ–‡åˆ†è¯
```python
def tokenize_english(self, text: str) -> List[str]:
    """
    è‹±æ–‡ä¸“ç”¨åˆ†è¯
    
    ç‰¹ç‚¹:
    - ä½¿ç”¨NLTK word_tokenize
    - è‡ªåŠ¨å°å†™åŒ–
    - åœç”¨è¯è¿‡æ»¤
    - æ ‡ç‚¹ç¬¦å·è¿‡æ»¤
    - è¯é•¿è¿‡æ»¤ï¼ˆ>2å­—ç¬¦ï¼‰
    """
```

#### æ··åˆåˆ†è¯
```python
def tokenize_mixed(self, text: str) -> List[str]:
    """
    ä¸­è‹±æ–‡æ··åˆåˆ†è¯
    
    ç­–ç•¥:
    1. æŒ‰ä¸­è‹±æ–‡åˆ†å‰²æ–‡æœ¬
    2. ä¸­æ–‡éƒ¨åˆ†ä½¿ç”¨jiebaåˆ†è¯
    3. è‹±æ–‡éƒ¨åˆ†ä½¿ç”¨NLTKåˆ†è¯
    4. åˆå¹¶ç»“æœå¹¶å»é‡
    """
```

**æ··åˆåˆ†è¯ç¤ºä¾‹**ï¼š
```python
text = "è¿™æ˜¯æµ‹è¯•æ–‡æœ¬ This is a test"
tokens = processor.tokenize_mixed(text)
print(tokens)
# è¾“å‡º: ['è¿™æ˜¯', 'æµ‹è¯•', 'æ–‡æœ¬', 'this', 'test']
```

## ğŸ“Š ç‰¹å¾æå–

### extract_features æ–¹æ³•
```python
def extract_features(self, text: str) -> Dict[str, Union[int, float, List[str]]]:
    """
    æå–å…¨é¢çš„æ–‡æœ¬ç‰¹å¾
    
    è¿”å›ç‰¹å¾ç±»åˆ«:
    - åŸºç¡€ç‰¹å¾: é•¿åº¦ã€è¯æ•°ã€å­—ç¬¦æ•°
    - è¯­è¨€ç‰¹å¾: è¯­è¨€ç±»å‹ã€å¤§å†™æ¯”ä¾‹
    - æ ‡ç‚¹ç‰¹å¾: å„ç§æ ‡ç‚¹ç¬¦å·ç»Ÿè®¡
    - å†…å®¹ç‰¹å¾: URLã€æåŠã€emojiç­‰è®¡æ•°
    - åˆ†è¯ç‰¹å¾: tokenså’Œtokenæ•°é‡
    """
```

### å®Œæ•´ç‰¹å¾è¯´æ˜
```python
features = {
    # === åŸºç¡€æ–‡æœ¬ç‰¹å¾ ===
    'text_length': len(text),           # åŸå§‹æ–‡æœ¬é•¿åº¦
    'word_count': len(text.split()),    # ç®€å•ç©ºæ ¼åˆ†è¯è¯æ•°
    'char_count': len(text),            # å­—ç¬¦æ€»æ•°
    'language': 'mixed',                # æ£€æµ‹çš„è¯­è¨€ç±»å‹
    
    # === åˆ†è¯ç‰¹å¾ ===
    'tokens': ['token1', 'token2'],     # æ™ºèƒ½åˆ†è¯ç»“æœ
    'token_count': 10,                  # æ™ºèƒ½åˆ†è¯tokenæ•°
    
    # === æ ‡ç‚¹ç¬¦å·ç‰¹å¾ ===
    'exclamation_count': 2,             # æ„Ÿå¹å·æ•°é‡
    'question_count': 1,                # é—®å·æ•°é‡
    'period_count': 3,                  # å¥å·æ•°é‡
    'comma_count': 5,                   # é€—å·æ•°é‡
    
    # === è‹±æ–‡ç‰¹å¾ ===
    'uppercase_ratio': 0.15,            # å¤§å†™å­—æ¯æ¯”ä¾‹
    'digit_count': 4,                   # æ•°å­—å­—ç¬¦æ•°
    
    # === å†…å®¹ç‰¹å¾ ===
    'url_count': 1,                     # URLæ•°é‡
    'mention_count': 2,                 # @æåŠæ•°é‡  
    'hashtag_count': 1,                 # #è¯é¢˜æ ‡ç­¾æ•°é‡
    'emoji_count': 3                    # emojiæ•°é‡
}
```

## ğŸ”„ æ‰¹é‡å¤„ç†

### preprocess_batch æ–¹æ³•
```python
def preprocess_batch(self, texts: List[str]) -> List[Dict[str, any]]:
    """
    æ‰¹é‡é¢„å¤„ç†æ–‡æœ¬åˆ—è¡¨
    
    Args:
        texts: æ–‡æœ¬åˆ—è¡¨
        
    Returns:
        å¤„ç†ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
        {
            'original_text': str,     # åŸå§‹æ–‡æœ¬
            'cleaned_text': str,      # æ¸…æ´—åæ–‡æœ¬
            'tokens': List[str],      # åˆ†è¯ç»“æœ
            'features': Dict          # ç‰¹å¾å­—å…¸
        }
    """
```

**æ‰¹é‡å¤„ç†ç¤ºä¾‹**ï¼š
```python
texts = [
    "è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬",
    "This is the second test text!",
    "æ··åˆè¯­è¨€æ–‡æœ¬ mixed language text"
]

results = processor.preprocess_batch(texts)
for i, result in enumerate(results):
    print(f"æ–‡æœ¬{i+1}:")
    print(f"  åŸæ–‡: {result['original_text']}")
    print(f"  æ¸…æ´—: {result['cleaned_text']}")
    print(f"  åˆ†è¯: {result['tokens']}")
    print(f"  è¯­è¨€: {result['features']['language']}")
    print(f"  é•¿åº¦: {result['features']['text_length']}")
```

## âš™ï¸ é…ç½®é›†æˆ

### è‡ªåŠ¨é…ç½®åŠ è½½
```python
# ä»é…ç½®ç®¡ç†å™¨è‡ªåŠ¨åŠ è½½å‚æ•°
if USE_CONFIG:
    try:
        config = get_data_config()
        self.processing_config = config.get('processing', {}).get('text', {})
    except:
        self.processing_config = {}
else:
    self.processing_config = {}

# è®¾ç½®å¤„ç†å‚æ•°
self.max_length = self.processing_config.get('max_length', 512)
self.remove_urls = self.processing_config.get('remove_urls', True)
self.remove_mentions = self.processing_config.get('remove_mentions', True)
self.remove_hashtags = self.processing_config.get('remove_hashtags', False)
```

### å¯é…ç½®å‚æ•°
| é…ç½®å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|----------|--------|------|
| `max_length` | 512 | æœ€å¤§æ–‡æœ¬é•¿åº¦ |
| `remove_urls` | True | æ˜¯å¦ç§»é™¤URL |
| `remove_mentions` | True | æ˜¯å¦ç§»é™¤@æåŠ |
| `remove_hashtags` | False | æ˜¯å¦ç§»é™¤#æ ‡ç­¾ |
| `normalize_whitespace` | True | æ˜¯å¦æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦ |
| `tokenization` | "mixed" | åˆ†è¯ç±»å‹ |

## ğŸ”§ ä¾èµ–åº“å¤„ç†

### æ™ºèƒ½ä¾èµ–æ£€æµ‹
```python
# ä¸­æ–‡åˆ†è¯åº“æ£€æµ‹
try:
    import jieba
    import jieba.posseg as pseg
    HAS_JIEBA = True
except ImportError:
    print("âš ï¸ jiebaæœªå®‰è£…ï¼Œä¸­æ–‡åˆ†è¯åŠŸèƒ½ä¸å¯ç”¨")
    HAS_JIEBA = False

# è‹±æ–‡å¤„ç†åº“æ£€æµ‹
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.stem import PorterStemmer, WordNetLemmatizer
    HAS_NLTK = True
except ImportError:
    print("âš ï¸ nltkæœªå®‰è£…ï¼Œè‹±æ–‡é«˜çº§å¤„ç†åŠŸèƒ½ä¸å¯ç”¨")
    HAS_NLTK = False
```

### é™çº§å¤„ç†ç­–ç•¥
```python
# ä¸­æ–‡åˆ†è¯é™çº§
if not HAS_JIEBA:
    def tokenize_chinese(self, text: str) -> List[str]:
        # ç®€å•çš„å­—ç¬¦çº§åˆ†å‰²
        return list(text.replace(' ', ''))

# è‹±æ–‡åˆ†è¯é™çº§
if not HAS_NLTK:
    def tokenize_english(self, text: str) -> List[str]:
        # ç®€å•çš„ç©ºæ ¼åˆ†å‰²
        return text.lower().split()
```

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨
```python
from preprocessing import TextProcessor

# åˆ›å»ºå¤„ç†å™¨
processor = TextProcessor(language='mixed')

# å¤„ç†å•ä¸ªæ–‡æœ¬
text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ This is a test! @user #topic http://example.com"

# 1. è¯­è¨€æ£€æµ‹
language = processor.detect_language(text)
print(f"è¯­è¨€ç±»å‹: {language}")  # è¾“å‡º: mixed

# 2. æ–‡æœ¬æ¸…æ´—
cleaned = processor.clean_text(text)
print(f"æ¸…æ´—å: {cleaned}")
# è¾“å‡º: "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ This is a test"

# 3. åˆ†è¯
tokens = processor.tokenize(text)
print(f"åˆ†è¯ç»“æœ: {tokens}")
# è¾“å‡º: ['è¿™æ˜¯', 'ä¸€ä¸ª', 'æµ‹è¯•', 'æ–‡æœ¬', 'this', 'test']

# 4. ç‰¹å¾æå–
features = processor.extract_features(text)
print(f"ç‰¹å¾æ‘˜è¦:")
print(f"  æ–‡æœ¬é•¿åº¦: {features['text_length']}")
print(f"  è¯æ•°: {features['token_count']}")
print(f"  URLæ•°: {features['url_count']}")
print(f"  æåŠæ•°: {features['mention_count']}")
```

### è‡ªå®šä¹‰é…ç½®
```python
# åˆ›å»ºè‡ªå®šä¹‰é…ç½®çš„å¤„ç†å™¨
processor = TextProcessor(language='chinese')

# ä¿®æ”¹å¤„ç†å‚æ•°
processor.remove_hashtags = True   # å¯ç”¨è¯é¢˜æ ‡ç­¾ç§»é™¤
processor.remove_urls = False      # ä¿ç•™URL

# å¤„ç†æ–‡æœ¬
text = "å…³æ³¨ #AIæŠ€æœ¯ å‘å±• https://ai-news.com"
cleaned = processor.clean_text(text)
print(cleaned)  # è¾“å‡º: "å…³æ³¨ å‘å±• https://ai-news.com"
```

### å¤šè¯­è¨€å¤„ç†å¯¹æ¯”
```python
# æµ‹è¯•ä¸åŒè¯­è¨€çš„å¤„ç†æ•ˆæœ
test_texts = {
    'chinese': "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥ã€‚",
    'english': "Today is a beautiful day for a walk.",
    'mixed': "ä»Šå¤©weatherå¾ˆå¥½ perfect for walking"
}

for lang, text in test_texts.items():
    print(f"\n=== {lang.upper()} æ–‡æœ¬å¤„ç† ===")
    print(f"åŸæ–‡: {text}")
    
    # è¯­è¨€æ£€æµ‹
    detected = processor.detect_language(text)
    print(f"æ£€æµ‹è¯­è¨€: {detected}")
    
    # åˆ†è¯
    tokens = processor.tokenize(text)
    print(f"åˆ†è¯ç»“æœ: {tokens}")
    
    # ç‰¹å¾
    features = processor.extract_features(text)
    print(f"è¯æ•°: {features['token_count']}")
```

## ğŸ” é«˜çº§åŠŸèƒ½

### åœç”¨è¯è‡ªå®šä¹‰
```python
# æ‰©å±•ä¸­æ–‡åœç”¨è¯
processor.chinese_stopwords.update({
    'ä¸€äº›', 'å„ç§', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ'
})

# æ‰©å±•è‹±æ–‡åœç”¨è¯
processor.english_stopwords.update({
    'would', 'could', 'should', 'might'
})
```

### è¯å¹²æå–å’Œè¯å½¢è¿˜åŸï¼ˆè‹±æ–‡ï¼‰
```python
if HAS_NLTK:
    # è¯å¹²æå–
    stemmed = processor.stemmer.stem('running')  # è¾“å‡º: 'run'
    
    # è¯å½¢è¿˜åŸ
    lemmatized = processor.lemmatizer.lemmatize('better', 'a')  # è¾“å‡º: 'good'
```

### æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
```python
# å†…ç½®çš„æ¸…æ´—æ¨¡å¼
patterns = {
    'url': r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
    'www': r'www\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
    'mention': r'@[a-zA-Z0-9_\u4e00-\u9fff]+',
    'hashtag': r'#[a-zA-Z0-9_\u4e00-\u9fff]+',
    'emoji': r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]'
}
```

## ğŸ“Š æ€§èƒ½åˆ†æ

### å¤„ç†é€Ÿåº¦åŸºå‡†
```python
import time

# æ€§èƒ½æµ‹è¯•
test_texts = ["æµ‹è¯•æ–‡æœ¬ test text"] * 1000

start_time = time.time()
for text in test_texts:
    processor.tokenize(text)
end_time = time.time()

print(f"å¤„ç†é€Ÿåº¦: {len(test_texts)/(end_time-start_time):.1f} æ–‡æœ¬/ç§’")

# æ‰¹é‡å¤„ç†æ€§èƒ½
start_time = time.time()
results = processor.preprocess_batch(test_texts)
end_time = time.time()

print(f"æ‰¹é‡å¤„ç†é€Ÿåº¦: {len(test_texts)/(end_time-start_time):.1f} æ–‡æœ¬/ç§’")
```

### å†…å­˜ä¼˜åŒ–
```python
# å¯¹äºå¤§æ‰¹é‡å¤„ç†ï¼Œå¯ä»¥åˆ†å—å¤„ç†
def process_large_batch(texts, chunk_size=1000):
    results = []
    for i in range(0, len(texts), chunk_size):
        chunk = texts[i:i+chunk_size]
        chunk_results = processor.preprocess_batch(chunk)
        results.extend(chunk_results)
        
        # å¯é€‰ï¼šé‡Šæ”¾å†…å­˜
        del chunk_results
        
    return results
```

## ğŸš¨ é”™è¯¯å¤„ç†

### è¾“å…¥éªŒè¯
```python
def clean_text(self, text: str) -> str:
    # è¾“å…¥ç±»å‹æ£€æŸ¥
    if not text or not isinstance(text, str):
        return ""
    
    try:
        # å¤„ç†é€»è¾‘
        cleaned = self._apply_cleaning_steps(text)
        return cleaned
    except Exception as e:
        logger.warning(f"æ–‡æœ¬æ¸…æ´—å¤±è´¥: {e}")
        return text  # è¿”å›åŸæ–‡æœ¬è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
```

### ç¼–ç é—®é¢˜å¤„ç†
```python
def safe_encode_text(self, text: str) -> str:
    """å®‰å…¨å¤„ç†æ–‡æœ¬ç¼–ç é—®é¢˜"""
    try:
        # ç¡®ä¿UTF-8ç¼–ç 
        if isinstance(text, bytes):
            text = text.decode('utf-8', errors='ignore')
        
        # å¤„ç†ç‰¹æ®Šå­—ç¬¦
        text = text.encode('utf-8', errors='ignore').decode('utf-8')
        return text
    except Exception:
        return ""
```

### ä¾èµ–åº“ç¼ºå¤±å¤„ç†
```python
def tokenize_with_fallback(self, text: str) -> List[str]:
    """å¸¦é™çº§çš„åˆ†è¯æ–¹æ³•"""
    try:
        if self.language == 'chinese' and HAS_JIEBA:
            return self.tokenize_chinese(text)
        elif self.language == 'english' and HAS_NLTK:
            return self.tokenize_english(text)
        else:
            # é™çº§åˆ°ç®€å•åˆ†è¯
            return self._simple_tokenize(text)
    except Exception as e:
        logger.warning(f"åˆ†è¯å¤±è´¥ï¼Œä½¿ç”¨ç®€å•åˆ†è¯: {e}")
        return text.split()
```

## ğŸ”§ è°ƒè¯•å’Œæµ‹è¯•

### å†…ç½®æµ‹è¯•åŠŸèƒ½
```python
def test_processor():
    """æµ‹è¯•æ–‡æœ¬å¤„ç†å™¨åŠŸèƒ½"""
    processor = TextProcessor(language='mixed')
    
    test_texts = [
        "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ This is a test!",
        "åŒ…å«URLçš„æ–‡æœ¬ https://example.com å’Œ@username",
        "æ··åˆè¯­è¨€æ–‡æœ¬ with English words ä¸­æ–‡å­—ç¬¦"
    ]
    
    print("ğŸ“ === æ–‡æœ¬å¤„ç†æµ‹è¯• ===")
    for i, text in enumerate(test_texts, 1):
        print(f"\næµ‹è¯• {i}: {text}")
        
        # è¯­è¨€æ£€æµ‹
        language = processor.detect_language(text)
        print(f"  è¯­è¨€: {language}")
        
        # æ–‡æœ¬æ¸…æ´—
        cleaned = processor.clean_text(text)
        print(f"  æ¸…æ´—å: {cleaned}")
        
        # åˆ†è¯
        tokens = processor.tokenize(text)
        print(f"  åˆ†è¯: {tokens[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
        
        # ç‰¹å¾
        features = processor.extract_features(text)
        print(f"  ç‰¹å¾: é•¿åº¦={features['text_length']}, "
              f"è¯æ•°={features['token_count']}, "
              f"è¯­è¨€={features['language']}")

if __name__ == "__main__":
    test_processor()
```

### è°ƒè¯•è¾“å‡ºç¤ºä¾‹
```
ğŸ“ === æ–‡æœ¬å¤„ç†æµ‹è¯• ===

æµ‹è¯• 1: è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ This is a test!
  è¯­è¨€: mixed
  æ¸…æ´—å: è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ This is a test
  åˆ†è¯: ['è¿™æ˜¯', 'ä¸€ä¸ª', 'æµ‹è¯•', 'æ–‡æœ¬', 'this']...
  ç‰¹å¾: é•¿åº¦=25, è¯æ•°=5, è¯­è¨€=mixed

æµ‹è¯• 2: åŒ…å«URLçš„æ–‡æœ¬ https://example.com å’Œ@username
  è¯­è¨€: mixed  
  æ¸…æ´—å: åŒ…å«URLçš„æ–‡æœ¬ å’Œ
  åˆ†è¯: ['åŒ…å«', 'url', 'æ–‡æœ¬']...
  ç‰¹å¾: é•¿åº¦=30, è¯æ•°=3, è¯­è¨€=mixed
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. è¯­è¨€ç±»å‹é€‰æ‹©
```python
# æ ¹æ®æ•°æ®ç‰¹ç‚¹é€‰æ‹©è¯­è¨€æ¨¡å¼
if dataset_is_chinese_only:
    processor = TextProcessor(language='chinese')
elif dataset_is_english_only:
    processor = TextProcessor(language='english')
else:  # æ¨èç”¨äºMR2æ•°æ®é›†
    processor = TextProcessor(language='mixed')
```

### 2. é…ç½®ä¼˜åŒ–
```python
# é’ˆå¯¹è°£è¨€æ£€æµ‹ä»»åŠ¡çš„ä¼˜åŒ–é…ç½®
processor = TextProcessor(language='mixed')
processor.remove_urls = True        # ç§»é™¤URLå‡å°‘å™ªå£°
processor.remove_mentions = True    # ç§»é™¤@æåŠ
processor.remove_hashtags = False   # ä¿ç•™è¯é¢˜æ ‡ç­¾ï¼ˆå¯èƒ½æœ‰ä¿¡æ¯ä»·å€¼ï¼‰
processor.normalize_whitespace = True  # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
```

### 3. æ‰¹é‡å¤„ç†ç­–ç•¥
```python
# å¤§æ•°æ®é›†å¤„ç†ç­–ç•¥
def efficient_batch_processing(texts, batch_size=1000):
    processor = TextProcessor(language='mixed')
    
    all_results = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        batch_results = processor.preprocess_batch(batch)
        all_results.extend(batch_results)
        
        # è¿›åº¦æŠ¥å‘Š
        if (i // batch_size) % 10 == 0:
            print(f"å·²å¤„ç†: {i + len(batch)}/{len(texts)}")
    
    return all_results
```

### 4. ç‰¹å¾é€‰æ‹©
```python
# é’ˆå¯¹æœºå™¨å­¦ä¹ ä»»åŠ¡çš„ç‰¹å¾é€‰æ‹©
def extract_ml_features(text, processor):
    """æå–é€‚åˆæœºå™¨å­¦ä¹ çš„ç‰¹å¾"""
    features = processor.extract_features(text)
    
    # é€‰æ‹©æ•°å€¼ç‰¹å¾
    ml_features = {
        'text_length': features['text_length'],
        'token_count': features['token_count'],
        'exclamation_count': features['exclamation_count'],
        'question_count': features['question_count'],
        'uppercase_ratio': features['uppercase_ratio'],
        'url_count': features['url_count'],
        'mention_count': features['mention_count'],
        'emoji_count': features['emoji_count']
    }
    
    # è¯­è¨€ç±»å‹ç¼–ç 
    ml_features['is_chinese'] = 1 if features['language'] == 'chinese' else 0
    ml_features['is_english'] = 1 if features['language'] == 'english' else 0
    ml_features['is_mixed'] = 1 if features['language'] == 'mixed' else 0
    
    return ml_features
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### ç¼–ç é—®é¢˜
- ç¡®ä¿è¾“å…¥æ–‡æœ¬ä½¿ç”¨UTF-8ç¼–ç 
- å¤„ç†ç‰¹æ®Šå­—ç¬¦æ—¶æ³¨æ„ç¼–ç è½¬æ¢
- è¾“å‡ºç»“æœä¿æŒç¼–ç ä¸€è‡´æ€§

### ä¾èµ–åº“ç‰ˆæœ¬
- jieba: å»ºè®®ä½¿ç”¨0.42+ç‰ˆæœ¬
- nltk: å»ºè®®ä½¿ç”¨3.6+ç‰ˆæœ¬ï¼Œéœ€è¦ä¸‹è½½ç›¸å…³æ•°æ®åŒ…
- emoji: å»ºè®®ä½¿ç”¨1.6+ç‰ˆæœ¬

### æ€§èƒ½è€ƒè™‘
- å¤§æ–‡æœ¬å¤„ç†æ—¶æ³¨æ„å†…å­˜ä½¿ç”¨
- æ‰¹é‡å¤„ç†æ¯”å•ä¸ªå¤„ç†æ›´é«˜æ•ˆ
- ä¸­è‹±æ–‡æ··åˆå¤„ç†æ¯”å•è¯­è¨€å¤„ç†ç¨æ…¢

---

**[â¬…ï¸ é¢„å¤„ç†æ¨¡å—æ¦‚è§ˆ](README.md) | [å›¾åƒå¤„ç† â¡ï¸](image_processing.md)**