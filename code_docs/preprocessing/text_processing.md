# æ–‡æœ¬å¤„ç†å™¨ Text Processing

> ğŸ“ **ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬çš„åˆ†è¯ã€æ¸…æ´—ã€æ ‡å‡†åŒ–å¤„ç†å·¥å…·**

## ğŸ“‹ åŠŸèƒ½è¯´æ˜

`TextProcessor` ä¸“é—¨å¤„ç†MR2æ•°æ®é›†ä¸­çš„ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬ï¼Œæä¾›åˆ†è¯ã€æ¸…æ´—ã€ç‰¹å¾æå–ç­‰å®Œæ•´çš„æ–‡æœ¬é¢„å¤„ç†åŠŸèƒ½ã€‚

## ğŸ¯ ä¸»è¦åŠŸèƒ½

### è¯­è¨€å¤„ç†åŠŸèƒ½
- **è¯­è¨€æ£€æµ‹**: è‡ªåŠ¨è¯†åˆ«ä¸­æ–‡ã€è‹±æ–‡æˆ–æ··åˆæ–‡æœ¬
- **ä¸­æ–‡åˆ†è¯**: åŸºäºjiebaçš„æ™ºèƒ½ä¸­æ–‡åˆ†è¯
- **è‹±æ–‡å¤„ç†**: åŸºäºNLTKçš„è‹±æ–‡åˆ†è¯å’Œè¯å½¢å¤„ç†
- **æ··åˆå¤„ç†**: ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬çš„è”åˆå¤„ç†

### æ–‡æœ¬æ¸…æ´—åŠŸèƒ½
- **URLç§»é™¤**: æ¸…ç†HTTPé“¾æ¥å’Œç½‘å€
- **æåŠæ¸…ç†**: ç§»é™¤@ç”¨æˆ·åæåŠ
- **è¯é¢˜æ ‡ç­¾**: å¯é€‰çš„#è¯é¢˜æ ‡ç­¾å¤„ç†
- **è¡¨æƒ…ç¬¦å·**: emojiè½¬æ–‡æœ¬æè¿°æˆ–ç§»é™¤
- **HTMLæ¸…ç†**: ç§»é™¤HTMLæ ‡ç­¾å’Œå®ä½“

### ç‰¹å¾æå–åŠŸèƒ½
- **åŸºç¡€ç‰¹å¾**: æ–‡æœ¬é•¿åº¦ã€è¯æ•°ã€å­—ç¬¦æ•°
- **è¯­è¨€ç‰¹å¾**: è¯­è¨€ç±»å‹ã€å¤§å†™æ¯”ä¾‹
- **æ ‡ç‚¹ç»Ÿè®¡**: æ„Ÿå¹å·ã€é—®å·ç­‰æ ‡ç‚¹ç¬¦å·è®¡æ•°
- **å†…å®¹ç‰¹å¾**: URLæ•°é‡ã€æåŠæ•°é‡ã€emojiæ•°é‡

## ğŸš€ æ ¸å¿ƒç±»å’Œæ–¹æ³•

### TextProcessor ç±»

#### åˆå§‹åŒ–æ–¹æ³•
```python
TextProcessor(language='mixed')
```

**å‚æ•°è¯´æ˜:**
- `language`: è¯­è¨€ç±»å‹ ('chinese', 'english', 'mixed')

#### ä¸»è¦æ–¹æ³•

##### è¯­è¨€æ£€æµ‹
- `detect_language(text)`: æ£€æµ‹æ–‡æœ¬è¯­è¨€ç±»å‹
- è¿”å›: 'chinese', 'english', 'mixed', 'unknown'

##### æ–‡æœ¬æ¸…æ´—
- `clean_text(text)`: ç»¼åˆæ–‡æœ¬æ¸…æ´—
- ç§»é™¤URLã€æåŠã€HTMLæ ‡ç­¾ç­‰å™ªå£°
- æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·

##### åˆ†è¯åŠŸèƒ½
- `tokenize(text, language=None)`: ç»Ÿä¸€åˆ†è¯æ¥å£
- `tokenize_chinese(text)`: ä¸­æ–‡ä¸“ç”¨åˆ†è¯
- `tokenize_english(text)`: è‹±æ–‡ä¸“ç”¨åˆ†è¯
- `tokenize_mixed(text)`: æ··åˆè¯­è¨€åˆ†è¯

##### ç‰¹å¾æå–
- `extract_features(text)`: æå–å®Œæ•´æ–‡æœ¬ç‰¹å¾
- `preprocess_batch(texts)`: æ‰¹é‡æ–‡æœ¬é¢„å¤„ç†

## ğŸ“¦ å¤„ç†é…ç½®

### æ–‡æœ¬æ¸…æ´—é…ç½®
- `remove_urls`: æ˜¯å¦ç§»é™¤URL (é»˜è®¤True)
- `remove_mentions`: æ˜¯å¦ç§»é™¤@æåŠ (é»˜è®¤True)
- `remove_hashtags`: æ˜¯å¦ç§»é™¤#æ ‡ç­¾ (é»˜è®¤False)
- `normalize_whitespace`: æ˜¯å¦æ ‡å‡†åŒ–ç©ºç™½ (é»˜è®¤True)

### åˆ†è¯é…ç½®
- **ä¸­æ–‡é…ç½®**: jiebaåˆ†è¯ã€è‡ªå®šä¹‰è¯å…¸
- **è‹±æ–‡é…ç½®**: NLTKåˆ†è¯ã€åœç”¨è¯è¿‡æ»¤ã€è¯å¹²æå–

## ğŸ“Š è¿”å›æ•°æ®æ ¼å¼

### åˆ†è¯ç»“æœ
```python
tokens = processor.tokenize(text)
# è¿”å›: List[str] - åˆ†è¯ç»“æœåˆ—è¡¨
```

### ç‰¹å¾æå–ç»“æœ
```python
features = processor.extract_features(text)
# è¿”å›å­—å…¸:
{
    'text_length': int,           # æ–‡æœ¬é•¿åº¦
    'word_count': int,            # è¯æ•°
    'char_count': int,            # å­—ç¬¦æ•°
    'language': str,              # è¯­è¨€ç±»å‹
    'tokens': List[str],          # åˆ†è¯ç»“æœ
    'token_count': int,           # tokenæ•°é‡
    'exclamation_count': int,     # æ„Ÿå¹å·æ•°é‡
    'question_count': int,        # é—®å·æ•°é‡
    'uppercase_ratio': float,     # å¤§å†™å­—æ¯æ¯”ä¾‹
    'digit_count': int,           # æ•°å­—å­—ç¬¦æ•°
    'url_count': int,             # URLæ•°é‡
    'mention_count': int,         # æåŠæ•°é‡
    'hashtag_count': int,         # è¯é¢˜æ ‡ç­¾æ•°é‡
    'emoji_count': int            # emojiæ•°é‡
}
```

### æ‰¹é‡å¤„ç†ç»“æœ
```python
results = processor.preprocess_batch(texts)
# è¿”å›: List[Dict] - æ¯ä¸ªæ–‡æœ¬çš„å¤„ç†ç»“æœ
[
    {
        'original_text': str,     # åŸå§‹æ–‡æœ¬
        'cleaned_text': str,      # æ¸…æ´—åæ–‡æœ¬
        'tokens': List[str],      # åˆ†è¯ç»“æœ
        'features': Dict          # ç‰¹å¾å­—å…¸
    },
    ...
]
```

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€æ–‡æœ¬å¤„ç†
```python
from preprocessing import TextProcessor

# åˆ›å»ºå¤„ç†å™¨
processor = TextProcessor(language='mixed')

# æ–‡æœ¬æ¸…æ´—
text = "è¿™æ˜¯æµ‹è¯•æ–‡æœ¬ This is test! @user #topic http://example.com"
cleaned = processor.clean_text(text)
print(f"æ¸…æ´—å: {cleaned}")

# è¯­è¨€æ£€æµ‹
language = processor.detect_language(text)
print(f"è¯­è¨€ç±»å‹: {language}")

# åˆ†è¯å¤„ç†
tokens = processor.tokenize(text)
print(f"åˆ†è¯ç»“æœ: {tokens}")
```

### ç‰¹å¾æå–
```python
# æå–æ–‡æœ¬ç‰¹å¾
features = processor.extract_features(text)
print(f"æ–‡æœ¬é•¿åº¦: {features['text_length']}")
print(f"è¯æ•°: {features['word_count']}")
print(f"è¯­è¨€: {features['language']}")
print(f"URLæ•°é‡: {features['url_count']}")
```

### æ‰¹é‡å¤„ç†
```python
# æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æœ¬
texts = [
    "è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬",
    "This is the second test text!",
    "æ··åˆè¯­è¨€æ–‡æœ¬ mixed language text"
]

results = processor.preprocess_batch(texts)
for i, result in enumerate(results):
    print(f"æ–‡æœ¬{i+1}: {len(result['tokens'])} ä¸ªtokens")
```

### è‡ªå®šä¹‰é…ç½®
```python
# è‡ªå®šä¹‰æ¸…æ´—é…ç½®
processor = TextProcessor(language='chinese')
processor.remove_hashtags = True  # ç§»é™¤è¯é¢˜æ ‡ç­¾
processor.remove_urls = False     # ä¿ç•™URL

cleaned = processor.clean_text(text)
```

## ğŸ”§ é…ç½®å’Œä¼˜åŒ–

### ä¸­æ–‡å¤„ç†ä¼˜åŒ–
- **è‡ªå®šä¹‰è¯å…¸**: æ·»åŠ é¢†åŸŸç›¸å…³è¯æ±‡
- **åœç”¨è¯æ‰©å±•**: æ ¹æ®ä»»åŠ¡è°ƒæ•´åœç”¨è¯åˆ—è¡¨
- **åˆ†è¯ç²¾åº¦**: è°ƒæ•´jiebaåˆ†è¯å‚æ•°

### è‹±æ–‡å¤„ç†ä¼˜åŒ–
- **è¯å½¢è¿˜åŸ**: å¯ç”¨lemmatizationæé«˜å‡†ç¡®æ€§
- **è¯å¹²æå–**: ä½¿ç”¨stemmingå‡å°‘è¯æ±‡å˜å½¢
- **åœç”¨è¯è¿‡æ»¤**: æ ¹æ®ä»»åŠ¡éœ€æ±‚è°ƒæ•´åœç”¨è¯

### æ€§èƒ½ä¼˜åŒ–
- **æ‰¹é‡å¤„ç†**: ä½¿ç”¨batchæ–¹æ³•æé«˜å¤„ç†æ•ˆç‡
- **ç¼“å­˜æœºåˆ¶**: ç¼“å­˜å¸¸ç”¨è¯å…¸å’Œæ¨¡å‹
- **å¹¶è¡Œå¤„ç†**: å¤§æ•°æ®é‡æ—¶ä½¿ç”¨å¤šè¿›ç¨‹

## âš ï¸ é‡è¦è¯´æ˜

### ä¾èµ–åº“è¦æ±‚
- **jieba**: ä¸­æ–‡åˆ†è¯åº“ï¼Œéœ€è¦é¢„å…ˆä¸‹è½½è¯å…¸
- **nltk**: è‹±æ–‡å¤„ç†åº“ï¼Œéœ€è¦ä¸‹è½½ç›¸å…³æ•°æ®
- **emoji**: è¡¨æƒ…ç¬¦å·å¤„ç†åº“

### æ€§èƒ½è€ƒè™‘
- å¤§æ–‡æœ¬å¤„ç†æ—¶æ³¨æ„å†…å­˜ä½¿ç”¨
- æ‰¹é‡å¤„ç†æ¯”å•ä¸ªå¤„ç†æ›´é«˜æ•ˆ
- ä¸­è‹±æ–‡æ··åˆå¤„ç†æ¯”å•è¯­è¨€å¤„ç†ç¨æ…¢

### ç¼–ç é—®é¢˜
- ç¡®ä¿è¾“å…¥æ–‡æœ¬ä½¿ç”¨UTF-8ç¼–ç 
- å¤„ç†ç‰¹æ®Šå­—ç¬¦æ—¶æ³¨æ„ç¼–ç è½¬æ¢
- è¾“å‡ºç»“æœä¿æŒç¼–ç ä¸€è‡´æ€§

---

**[â¬…ï¸ é¢„å¤„ç†æ¨¡å—æ¦‚è§ˆ](README.md) | [å›¾åƒå¤„ç† â¡ï¸](image_processing.md)**
