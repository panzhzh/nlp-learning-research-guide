#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# data_utils/data_loaders.py

"""
å¼ºåˆ¶ä½¿ç”¨çœŸå®æ•°æ®é›†çš„æ•°æ®åŠ è½½å™¨
ä¸å†æ”¯æŒæ¼”ç¤ºæ•°æ®ï¼Œæ‰¾ä¸åˆ°æ•°æ®é›†å°±æŠ¥é”™
"""

import torch
from torch.utils.data import DataLoader
import numpy as np
from typing import Dict, List, Optional, Any
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å¯¼å…¥å¿…è¦æ¨¡å—
try:
    from .mr2_dataset import MR2Dataset
    USE_CUSTOM_MODULES = True
    print("âœ… æˆåŠŸå¯¼å…¥è‡ªå®šä¹‰æ¨¡å— (ç›¸å¯¹å¯¼å…¥)")
except ImportError:
    try:
        from mr2_dataset import MR2Dataset
        USE_CUSTOM_MODULES = True
        print("âœ… æˆåŠŸå¯¼å…¥è‡ªå®šä¹‰æ¨¡å— (ç›´æ¥å¯¼å…¥)")
    except ImportError:
        try:
            import sys
            data_utils_path = current_file.parent
            sys.path.insert(0, str(data_utils_path))
            from mr2_dataset import MR2Dataset
            USE_CUSTOM_MODULES = True
            print("âœ… æˆåŠŸå¯¼å…¥è‡ªå®šä¹‰æ¨¡å— (è·¯å¾„å¯¼å…¥)")
        except ImportError as e:
            print(f"âŒ æ— æ³•å¯¼å…¥MR2Dataset: {e}")
            print("âŒ ç³»ç»Ÿè¦æ±‚å¿…é¡»ä½¿ç”¨çœŸå®æ•°æ®é›†ï¼Œä¸æ”¯æŒå ä½ç¬¦æ•°æ®")
            raise ImportError("âŒ å¿…é¡»å¯¼å…¥MR2Datasetæ¨¡å—æ‰èƒ½ç»§ç»­")

# å¯¼å…¥é…ç½®ç®¡ç†å™¨
try:
    from utils.config_manager import get_config_manager, get_data_dir, check_data_requirements
    USE_CONFIG = True
    print("âœ… æˆåŠŸå¯¼å…¥é…ç½®ç®¡ç†å™¨")
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥é…ç½®ç®¡ç†å™¨: {e}")
    raise ImportError("âŒ å¿…é¡»å¯¼å…¥é…ç½®ç®¡ç†å™¨æ‰èƒ½ç»§ç»­")


class StrictDataLoaderConfig:
    """ä¸¥æ ¼çš„æ•°æ®åŠ è½½å™¨é…ç½® - åªæ”¯æŒçœŸå®æ•°æ®"""
    
    def __init__(self):
        """åˆå§‹åŒ–é…ç½®"""
        # æ£€æŸ¥æ•°æ®è¦æ±‚
        try:
            check_data_requirements()
            print("âœ… æ•°æ®è¦æ±‚æ£€æŸ¥é€šè¿‡")
        except Exception as e:
            print(f"âŒ æ•°æ®è¦æ±‚æ£€æŸ¥å¤±è´¥: {e}")
            raise
        
        self.config = self.load_config()
    
    def load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        try:
            config_manager = get_config_manager()
            training_config = config_manager.get_training_config()
            
            # ä»é…ç½®ä¸­æå–å…³é”®å‚æ•°
            general_config = training_config.get('general', {}).get('data', {})
            return {
                'train_batch_size': general_config.get('train_batch_size', 32),
                'eval_batch_size': general_config.get('eval_batch_size', 64),
                'num_workers': general_config.get('data_workers', 4),
                'pin_memory': general_config.get('pin_memory', True)
            }
        except Exception as e:
            print(f"âš ï¸  åŠ è½½é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
            return {
                'train_batch_size': 32,
                'eval_batch_size': 64,
                'num_workers': 4,
                'pin_memory': True
            }


class StrictCollateFunction:
    """ä¸¥æ ¼çš„æ‰¹å¤„ç†å‡½æ•° - è¦æ±‚æ•°æ®å®Œæ•´"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        pass
    
    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ä¸¥æ ¼çš„æ‰¹å¤„ç†å‡½æ•°
        
        Args:
            batch: æ‰¹æ¬¡æ•°æ®åˆ—è¡¨
            
        Returns:
            æ‰¹å¤„ç†åçš„æ•°æ®å­—å…¸
        """
        if not batch:
            raise ValueError("âŒ æ‰¹æ¬¡æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæ‰¹å¤„ç†")
        
        batch_data = {}
        
        # å¤„ç†åŸºæœ¬å­—æ®µ
        for key in ['item_id', 'label', 'text', 'caption']:
            if key in batch[0]:
                batch_data[key] = [item.get(key) for item in batch]
        
        # éªŒè¯å¿…è¦å­—æ®µ
        if 'label' not in batch_data and 'caption' not in batch_data and 'text' not in batch_data:
            raise ValueError("âŒ æ‰¹æ¬¡æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ (label, text/caption)")
        
        # å¤„ç†æ ‡ç­¾ï¼ˆè½¬æ¢ä¸ºtensorï¼‰
        if 'label' in batch_data:
            try:
                batch_data['labels'] = torch.tensor(batch_data['label'], dtype=torch.long)
            except Exception as e:
                raise ValueError(f"âŒ æ ‡ç­¾è½¬æ¢å¤±è´¥: {e}")
        
        # å¤„ç†å›¾åƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'image' in batch[0]:
            try:
                images = []
                for item in batch:
                    if 'image' in item and item['image'] is not None:
                        images.append(item['image'])
                    else:
                        # å¯¹äºç¼ºå¤±å›¾åƒï¼Œåˆ›å»ºé›¶tensor
                        images.append(torch.zeros(3, 224, 224))
                
                if images:
                    batch_data['images'] = torch.stack(images)
            except Exception as e:
                print(f"âš ï¸  å›¾åƒæ‰¹å¤„ç†å¤±è´¥: {e}")
                batch_data['images'] = torch.zeros(len(batch), 3, 224, 224)
        
        return batch_data


def create_strict_dataloader(split: str = 'train',
                           batch_size: Optional[int] = None,
                           shuffle: Optional[bool] = None,
                           num_workers: int = 4) -> DataLoader:
    """
    åˆ›å»ºä¸¥æ ¼çš„æ•°æ®åŠ è½½å™¨ - å¿…é¡»ä½¿ç”¨çœŸå®æ•°æ®é›†
    
    Args:
        split: æ•°æ®åˆ’åˆ† ('train', 'val', 'test')
        batch_size: æ‰¹æ¬¡å¤§å°
        shuffle: æ˜¯å¦æ‰“ä¹±
        num_workers: å·¥ä½œè¿›ç¨‹æ•°
        
    Returns:
        DataLoaderå¯¹è±¡
        
    Raises:
        FileNotFoundError: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶
        ValueError: æ•°æ®é›†ä¸ºç©ºæˆ–ä¸ç¬¦åˆè¦æ±‚
    """
    try:
        # åŠ è½½é…ç½®
        config = StrictDataLoaderConfig()
        
        # è·å–æ•°æ®ç›®å½•
        data_dir = get_data_dir()
        print(f"ğŸ“‚ ä½¿ç”¨æ•°æ®ç›®å½•: {data_dir}")
        
        # è®¾ç½®é»˜è®¤å‚æ•°
        if batch_size is None:
            batch_size = config.config['train_batch_size'] if split == 'train' else config.config['eval_batch_size']
        
        if shuffle is None:
            shuffle = (split == 'train')
        
        print(f"ğŸ“‚ åˆ›å»º {split} æ•°æ®åŠ è½½å™¨...")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   æ˜¯å¦æ‰“ä¹±: {shuffle}")
        
        # åˆ›å»ºçœŸå®æ•°æ®é›†
        dataset = MR2Dataset(
            data_dir=str(data_dir),
            split=split,
            transform_type='train' if split == 'train' else 'val',
            load_images=True
        )
        
        # ä¸¥æ ¼éªŒè¯æ•°æ®é›†
        if len(dataset) == 0:
            raise ValueError(f"âŒ {split} æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºæ•°æ®åŠ è½½å™¨")
        
        # æ£€æŸ¥æœ€å°æ ·æœ¬æ•°è¦æ±‚
        data_config = get_config_manager().get_data_config()
        min_samples = data_config.get('dataset', {}).get('requirements', {}).get('min_samples_per_split', 10)
        
        if len(dataset) < min_samples:
            raise ValueError(f"âŒ {split} æ•°æ®é›†æ ·æœ¬æ•°ä¸è¶³: {len(dataset)} < {min_samples}")
        
        print(f"âœ… {split} æ•°æ®é›†éªŒè¯é€šè¿‡: {len(dataset)} æ ·æœ¬")
        
        # åˆ›å»ºcollateå‡½æ•°
        collate_fn = StrictCollateFunction()
        
        # åˆ›å»ºDataLoader
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=min(num_workers, 4),
            collate_fn=collate_fn,
            pin_memory=config.config['pin_memory'] and torch.cuda.is_available(),
            drop_last=False
        )
        
        print(f"âœ… {split} æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        
        return dataloader
        
    except FileNotFoundError as e:
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {e}")
        print("è¯·ç¡®ä¿MR2æ•°æ®é›†å·²ä¸‹è½½å¹¶è§£å‹åˆ°æ­£ç¡®ä½ç½®")
        raise
    except ValueError as e:
        print(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: {e}")
        raise
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
        raise


def create_all_dataloaders(batch_sizes: Optional[Dict[str, int]] = None) -> Dict[str, DataLoader]:
    """
    åˆ›å»ºæ‰€æœ‰æ•°æ®åŠ è½½å™¨ - ä¸¥æ ¼æ¨¡å¼
    
    Args:
        batch_sizes: å„æ•°æ®é›†çš„æ‰¹æ¬¡å¤§å°
        
    Returns:
        æ•°æ®åŠ è½½å™¨å­—å…¸
        
    Raises:
        Exception: ä»»ä½•æ•°æ®åŠ è½½å¤±è´¥éƒ½ä¼šæŠ›å‡ºå¼‚å¸¸
    """
    if batch_sizes is None:
        batch_sizes = {'train': 32, 'val': 64, 'test': 64}
    
    dataloaders = {}
    errors = []
    
    for split in ['train', 'val', 'test']:
        print(f"\nğŸ“‚ åˆ›å»º {split} æ•°æ®åŠ è½½å™¨...")
        
        try:
            batch_size = batch_sizes.get(split, 32)
            dataloader = create_strict_dataloader(
                split=split,
                batch_size=batch_size,
                num_workers=2  # å‡å°‘å·¥ä½œè¿›ç¨‹é¿å…é—®é¢˜
            )
            
            dataloaders[split] = dataloader
            print(f"âœ… {split} æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
            
        except Exception as e:
            error_msg = f"âŒ {split} æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}"
            print(error_msg)
            errors.append(error_msg)
    
    # å¦‚æœæœ‰ä»»ä½•é”™è¯¯ï¼ŒæŠ›å‡ºå¼‚å¸¸
    if errors:
        error_summary = "\n".join(errors)
        raise RuntimeError(
            f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥:\n{error_summary}\n\n"
            f"è¯·æ£€æŸ¥:\n"
            f"1. MR2æ•°æ®é›†æ˜¯å¦å·²ä¸‹è½½å¹¶è§£å‹\n"
            f"2. æ•°æ®æ–‡ä»¶æ˜¯å¦åœ¨æ­£ç¡®è·¯å¾„: {get_data_dir()}\n"
            f"3. æ•°æ®æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®\n"
            f"ä¸‹è½½é“¾æ¥: https://pan.baidu.com/s/1sfUwsaeV2nfl54OkrfrKVw?pwd=jxhc"
        )
    
    # éªŒè¯æ‰€æœ‰æ•°æ®åŠ è½½å™¨éƒ½å·²åˆ›å»º
    required_splits = ['train', 'val', 'test']
    missing_splits = [split for split in required_splits if split not in dataloaders]
    
    if missing_splits:
        raise RuntimeError(f"âŒ ç¼ºå°‘å¿…è¦çš„æ•°æ®åˆ†å‰²: {missing_splits}")
    
    print(f"\nâœ… æ‰€æœ‰æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
    for split, loader in dataloaders.items():
        print(f"   {split}: {len(loader.dataset)} æ ·æœ¬, æ‰¹æ¬¡å¤§å° {loader.batch_size}")
    
    return dataloaders


def test_dataloader(dataloader: DataLoader, max_batches: int = 3):
    """
    æµ‹è¯•æ•°æ®åŠ è½½å™¨ - ä¸¥æ ¼éªŒè¯
    
    Args:
        dataloader: è¦æµ‹è¯•çš„æ•°æ®åŠ è½½å™¨
        max_batches: æœ€å¤§æµ‹è¯•æ‰¹æ¬¡æ•°
    """
    print(f"\nğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½å™¨ (æœ€å¤š {max_batches} ä¸ªæ‰¹æ¬¡)")
    
    try:
        batch_count = 0
        for batch_idx, batch in enumerate(dataloader):
            print(f"  æ‰¹æ¬¡ {batch_idx}:")
            
            if not isinstance(batch, dict):
                raise ValueError(f"âŒ æ‰¹æ¬¡æ•°æ®ç±»å‹é”™è¯¯: {type(batch)}")
            
            print(f"    æ•°æ®é”®: {list(batch.keys())}")
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if 'labels' not in batch and 'label' not in batch:
                raise ValueError("âŒ æ‰¹æ¬¡ç¼ºå°‘æ ‡ç­¾å­—æ®µ")
            
            if 'labels' in batch:
                if not isinstance(batch['labels'], torch.Tensor):
                    raise ValueError("âŒ æ ‡ç­¾ä¸æ˜¯tensorç±»å‹")
                print(f"    æ ‡ç­¾: {batch['labels']}")
                batch_size = len(batch['labels'])
            elif 'label' in batch:
                print(f"    æ ‡ç­¾: {batch['label']}")
                batch_size = len(batch['label'])
            
            print(f"    æ‰¹æ¬¡å¤§å°: {batch_size}")
            
            # éªŒè¯æ–‡æœ¬å­—æ®µ
            if 'text' in batch:
                if len(batch['text']) != batch_size:
                    raise ValueError(f"âŒ æ–‡æœ¬æ•°é‡ä¸æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…: {len(batch['text'])} != {batch_size}")
                print(f"    æ–‡æœ¬æ ·ä¾‹: {batch['text'][0][:50]}...")
            elif 'caption' in batch:
                if len(batch['caption']) != batch_size:
                    raise ValueError(f"âŒ æ–‡æœ¬æ•°é‡ä¸æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…: {len(batch['caption'])} != {batch_size}")
                print(f"    æ–‡æœ¬æ ·ä¾‹: {batch['caption'][0][:50]}...")
            
            # éªŒè¯å›¾åƒå­—æ®µ
            if 'images' in batch:
                if not isinstance(batch['images'], torch.Tensor):
                    raise ValueError("âŒ å›¾åƒä¸æ˜¯tensorç±»å‹")
                print(f"    å›¾åƒå½¢çŠ¶: {batch['images'].shape}")
                expected_shape = (batch_size, 3, 224, 224)
                if batch['images'].shape != expected_shape:
                    print(f"    âš ï¸  å›¾åƒå½¢çŠ¶ä¸é¢„æœŸä¸ç¬¦: {batch['images'].shape} != {expected_shape}")
            
            batch_count += 1
            if batch_count >= max_batches:
                break
        
        print("âœ… æ•°æ®åŠ è½½å™¨æµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨æµ‹è¯•å¤±è´¥: {e}")
        raise


def demo_usage():
    """æ¼”ç¤ºä¸¥æ ¼æ•°æ®åŠ è½½å™¨çš„ä½¿ç”¨æ–¹æ³•"""
    print("ğŸ”„ æ¼”ç¤ºä¸¥æ ¼æ•°æ®åŠ è½½å™¨ä½¿ç”¨æ–¹æ³•")
    print("âš ï¸  æ³¨æ„: æ­¤ç‰ˆæœ¬è¦æ±‚å¿…é¡»æœ‰çœŸå®æ•°æ®é›†")
    
    try:
        # æ£€æŸ¥æ•°æ®è¦æ±‚
        check_data_requirements()
        
        # åˆ›å»ºå•ä¸ªæ•°æ®åŠ è½½å™¨
        print(f"\nğŸ“š === åˆ›å»ºå•ä¸ªæ•°æ®åŠ è½½å™¨ ===")
        train_loader = create_strict_dataloader(
            split='train',
            batch_size=8,
            shuffle=True
        )
        
        if train_loader:
            test_dataloader(train_loader, max_batches=2)
        
        # åˆ›å»ºæ‰€æœ‰æ•°æ®åŠ è½½å™¨
        print(f"\nğŸ“š === åˆ›å»ºæ‰€æœ‰æ•°æ®åŠ è½½å™¨ ===")
        all_loaders = create_all_dataloaders(
            batch_sizes={'train': 4, 'val': 8, 'test': 8}
        )
        
        print(f"\nğŸ“Š æ•°æ®åŠ è½½å™¨æ‘˜è¦:")
        for split, loader in all_loaders.items():
            print(f"  {split}: {len(loader.dataset)} æ ·æœ¬, æ‰¹æ¬¡å¤§å° {loader.batch_size}")
        
        # æµ‹è¯•ä¸€ä¸ªæ•°æ®åŠ è½½å™¨
        print(f"\nğŸ§ª æµ‹è¯•éªŒè¯é›†æ•°æ®åŠ è½½å™¨:")
        test_dataloader(all_loaders['val'], max_batches=1)
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print("1. ç¡®ä¿MR2æ•°æ®é›†å·²ä¸‹è½½")
        print("2. æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("3. éªŒè¯æ•°æ®æ–‡ä»¶æ ¼å¼æ˜¯å¦å®Œæ•´")
        raise


# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸæœ‰å‡½æ•°å
def create_simple_dataloader(*args, **kwargs):
    """å‘åå…¼å®¹çš„å‡½æ•°å"""
    return create_strict_dataloader(*args, **kwargs)

def create_mr2_dataloaders(*args, **kwargs):
    """å‘åå…¼å®¹çš„å‡½æ•°å"""
    return create_all_dataloaders(*args, **kwargs)


# ä¸»æ‰§è¡Œä»£ç 
if __name__ == "__main__":
    print("ğŸ”„ æµ‹è¯•ä¸¥æ ¼æ•°æ®åŠ è½½å™¨é…ç½®")
    
    try:
        # è¿è¡Œæ¼”ç¤º
        demo_usage()
        print("\nâœ… ä¸¥æ ¼æ•°æ®åŠ è½½å™¨æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿MR2æ•°æ®é›†å·²æ­£ç¡®å®‰è£…")
        sys.exit(1)