#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# data_utils/mr2_dataset.py

"""
ä¸¥æ ¼çš„MR2æ•°æ®é›†ç±»
åªæ”¯æŒçœŸå®æ•°æ®é›†ï¼Œä¸å†æä¾›æ¼”ç¤ºæ•°æ®fallback
"""

import json
import torch
from torch.utils.data import Dataset
from torchvision import transforms
from PIL import Image
import numpy as np
from typing import Dict, List, Optional, Tuple, Union, Any
from pathlib import Path
import sys
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å¯¼å…¥é…ç½®ç®¡ç†
try:
    from utils.config_manager import get_data_config, get_data_dir, get_label_mapping, check_data_requirements
    USE_CONFIG = True
    print("âœ… æˆåŠŸå¯¼å…¥é…ç½®ç®¡ç†å™¨")
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥é…ç½®ç®¡ç†å™¨: {e}")
    raise ImportError("âŒ å¿…é¡»å¯¼å…¥é…ç½®ç®¡ç†å™¨æ‰èƒ½ç»§ç»­")

import logging
logger = logging.getLogger(__name__)


class MR2Dataset(Dataset):
    """
    ä¸¥æ ¼çš„MR2å¤šæ¨¡æ€è°£è¨€æ£€æµ‹æ•°æ®é›†
    
    åŠŸèƒ½:
    - åªåŠ è½½çœŸå®æ•°æ®é›†
    - æ‰¾ä¸åˆ°æ•°æ®å°±æŠ¥é”™
    - ä¸¥æ ¼çš„æ•°æ®éªŒè¯
    """
    
    def __init__(self, 
                 data_dir: Union[str, Path],
                 split: str = 'train',
                 transform_type: str = 'train',
                 target_size: Tuple[int, int] = (224, 224),
                 load_images: bool = True):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            split: æ•°æ®åˆ’åˆ† ('train', 'val', 'test')
            transform_type: å›¾åƒå˜æ¢ç±»å‹ ('train', 'val')
            target_size: ç›®æ ‡å›¾åƒå°ºå¯¸
            load_images: æ˜¯å¦åŠ è½½å›¾åƒ
            
        Raises:
            FileNotFoundError: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨
            ValueError: æ•°æ®æ ¼å¼é”™è¯¯æˆ–ä¸ºç©º
        """
        self.data_dir = Path(data_dir)
        self.split = split
        self.transform_type = transform_type
        self.target_size = target_size
        self.load_images = load_images
        
        # éªŒè¯æ•°æ®è¦æ±‚
        try:
            check_data_requirements()
        except Exception as e:
            raise RuntimeError(f"âŒ æ•°æ®è¦æ±‚æ£€æŸ¥å¤±è´¥: {e}")
        
        # åŠ è½½é…ç½®
        self.setup_config()
        
        # è®¾ç½®å›¾åƒå˜æ¢
        self.setup_transforms()
        
        # åŠ è½½æ•°æ®é›†
        self.load_dataset()
        
        # éªŒè¯æ•°æ®é›†
        self.validate_dataset()
        
        print(f"ğŸ“š MR2æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ•°æ®åˆ’åˆ†: {self.split}")
        print(f"   æ ·æœ¬æ•°é‡: {len(self.items)}")
        print(f"   åŠ è½½å›¾åƒ: {self.load_images}")
        print(f"   æ ‡ç­¾åˆ†å¸ƒ: {self.get_label_distribution()}")
    
    def setup_config(self):
        """è®¾ç½®é…ç½®"""
        try:
            self.label_mapping = get_label_mapping()
            data_config = get_data_config()
            self.dataset_config = data_config.get('dataset', {})
        except Exception as e:
            raise RuntimeError(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
    
    def setup_transforms(self):
        """è®¾ç½®å›¾åƒå˜æ¢"""
        # è®­ç»ƒæ—¶çš„å˜æ¢
        if self.transform_type == 'train':
            self.image_transforms = transforms.Compose([
                transforms.Resize(self.target_size),
                transforms.RandomHorizontalFlip(p=0.3),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
        else:
            # éªŒè¯/æµ‹è¯•æ—¶çš„å˜æ¢
            self.image_transforms = transforms.Compose([
                transforms.Resize(self.target_size),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
    
    def load_dataset(self):
        """åŠ è½½æ•°æ®é›†æ–‡ä»¶"""
        # æ„å»ºæ•°æ®æ–‡ä»¶è·¯å¾„
        dataset_file = self.data_dir / f'dataset_items_{self.split}.json'
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not dataset_file.exists():
            raise FileNotFoundError(
                f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_file}\n"
                f"è¯·ç¡®ä¿MR2æ•°æ®é›†å·²ä¸‹è½½å¹¶è§£å‹åˆ°: {self.data_dir}\n"
                f"ä¸‹è½½é“¾æ¥: https://pan.baidu.com/s/1sfUwsaeV2nfl54OkrfrKVw?pwd=jxhc"
            )
        
        # åŠ è½½JSONæ–‡ä»¶
        try:
            with open(dataset_file, 'r', encoding='utf-8') as f:
                self.raw_data = json.load(f)
            print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶: {dataset_file}")
        except json.JSONDecodeError as e:
            raise ValueError(f"âŒ JSONæ–‡ä»¶æ ¼å¼é”™è¯¯: {dataset_file}, é”™è¯¯: {e}")
        except Exception as e:
            raise RuntimeError(f"âŒ åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥: {dataset_file}, é”™è¯¯: {e}")
        
        # æ„å»ºæ•°æ®é¡¹åˆ—è¡¨
        self.items = []
        self.item_ids = []
        
        # éªŒè¯æ•°æ®æ ¼å¼å¹¶æ„å»ºæ•°æ®é¡¹
        for item_id, item_data in self.raw_data.items():
            # éªŒè¯å¿…è¦å­—æ®µ
            if not isinstance(item_data, dict):
                logger.warning(f"è·³è¿‡æ— æ•ˆæ•°æ®é¡¹ {item_id}: ä¸æ˜¯å­—å…¸æ ¼å¼")
                continue
                
            if 'caption' not in item_data:
                logger.warning(f"è·³è¿‡æ•°æ®é¡¹ {item_id}: ç¼ºå°‘captionå­—æ®µ")
                continue
                
            if 'label' not in item_data:
                logger.warning(f"è·³è¿‡æ•°æ®é¡¹ {item_id}: ç¼ºå°‘labelå­—æ®µ")
                continue
            
            # éªŒè¯æ ‡ç­¾å€¼
            label = item_data['label']
            if not isinstance(label, int) or label not in self.label_mapping:
                logger.warning(f"è·³è¿‡æ•°æ®é¡¹ {item_id}: æ— æ•ˆæ ‡ç­¾ {label}")
                continue
            
            # éªŒè¯æ–‡æœ¬å†…å®¹
            caption = item_data['caption']
            if not isinstance(caption, str) or len(caption.strip()) == 0:
                logger.warning(f"è·³è¿‡æ•°æ®é¡¹ {item_id}: æ— æ•ˆæ–‡æœ¬å†…å®¹")
                continue
            
            # æ·»åŠ æœ‰æ•ˆæ•°æ®é¡¹
            self.items.append(item_data)
            self.item_ids.append(item_id)
        
        print(f"ğŸ“‚ åŠ è½½ {self.split} æ•°æ®: {len(self.items)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
    
    def validate_dataset(self):
        """éªŒè¯æ•°æ®é›†"""
        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
        if len(self.items) == 0:
            raise ValueError(f"âŒ {self.split} æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•ç»§ç»­")
        
        # æ£€æŸ¥æœ€å°æ ·æœ¬æ•°è¦æ±‚
        min_samples = self.dataset_config.get('requirements', {}).get('min_samples_per_split', 10)
        if len(self.items) < min_samples:
            raise ValueError(f"âŒ {self.split} æ•°æ®é›†æ ·æœ¬æ•°ä¸è¶³: {len(self.items)} < {min_samples}")
        
        # éªŒè¯æ ‡ç­¾åˆ†å¸ƒ
        label_counts = {}
        for item in self.items:
            label = item['label']
            label_counts[label] = label_counts.get(label, 0) + 1
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰æ ‡ç­¾ç±»åˆ«
        expected_labels = set(self.label_mapping.keys())
        found_labels = set(label_counts.keys())
        
        if not found_labels.issubset(expected_labels):
            invalid_labels = found_labels - expected_labels
            raise ValueError(f"âŒ å‘ç°æ— æ•ˆæ ‡ç­¾: {invalid_labels}")
        
        # è­¦å‘Šç¼ºå¤±çš„æ ‡ç­¾ç±»åˆ«
        missing_labels = expected_labels - found_labels
        if missing_labels:
            missing_names = [self.label_mapping[label] for label in missing_labels]
            logger.warning(f"âš ï¸  {self.split} æ•°æ®é›†ç¼ºå°‘æ ‡ç­¾ç±»åˆ«: {missing_names}")
        
        print(f"âœ… {self.split} æ•°æ®é›†éªŒè¯é€šè¿‡")
    
    def __len__(self) -> int:
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.items)
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """
        è·å–å•ä¸ªæ•°æ®æ ·æœ¬
        
        Args:
            idx: æ ·æœ¬ç´¢å¼•
            
        Returns:
            åŒ…å«æ–‡æœ¬ã€å›¾åƒã€æ ‡ç­¾ç­‰ä¿¡æ¯çš„å­—å…¸
            
        Raises:
            IndexError: ç´¢å¼•è¶…å‡ºèŒƒå›´
        """
        if idx >= len(self.items):
            raise IndexError(f"ç´¢å¼•è¶…å‡ºèŒƒå›´: {idx} >= {len(self.items)}")
        
        item = self.items[idx]
        item_id = self.item_ids[idx]
        
        # æ„å»ºåŸºæœ¬æ•°æ®é¡¹
        data_item = {
            'item_id': item_id,
            'text': item.get('caption', ''),
            'caption': item.get('caption', ''),  # å…¼å®¹æ€§
            'label': item.get('label', -1),
            'language': item.get('language', 'unknown'),
            'text_length': len(item.get('caption', '')),
            'token_count': len(item.get('caption', '').split())
        }
        
        # å¤„ç†å›¾åƒ
        if self.load_images and 'image_path' in item:
            image_result = self.load_image_safe(item['image_path'])
            data_item.update(image_result)
        else:
            # å¦‚æœä¸åŠ è½½å›¾åƒæˆ–æ²¡æœ‰å›¾åƒè·¯å¾„ï¼Œåˆ›å»ºç©ºtensor
            data_item.update({
                'image': torch.zeros(3, *self.target_size),
                'has_image': False,
                'image_path': item.get('image_path', None)
            })
        
        return data_item
    
    def load_image_safe(self, image_path: str) -> Dict[str, Any]:
        """
        å®‰å…¨åœ°åŠ è½½å›¾åƒ
        
        Args:
            image_path: å›¾åƒç›¸å¯¹è·¯å¾„
            
        Returns:
            å›¾åƒæ•°æ®å­—å…¸
        """
        # æ„å»ºå®Œæ•´è·¯å¾„
        full_image_path = self.data_dir / image_path
        
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not full_image_path.exists():
                logger.warning(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {full_image_path}")
                return self.create_empty_image_result(str(full_image_path))
            
            # å°è¯•åŠ è½½å›¾åƒ
            with Image.open(full_image_path) as image:
                # è½¬æ¢ä¸ºRGBæ¨¡å¼
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                
                # åº”ç”¨å˜æ¢
                image_tensor = self.image_transforms(image)
                
                return {
                    'image': image_tensor,
                    'has_image': True,
                    'image_path': str(full_image_path),
                    'image_size': image.size
                }
                
        except Exception as e:
            logger.error(f"å¤„ç†å›¾åƒå¤±è´¥ {full_image_path}: {e}")
            return self.create_empty_image_result(str(full_image_path))
    
    def create_empty_image_result(self, image_path: str) -> Dict[str, Any]:
        """åˆ›å»ºç©ºå›¾åƒç»“æœ"""
        return {
            'image': torch.zeros(3, *self.target_size),
            'has_image': False,
            'image_path': image_path
        }
    
    def get_label_distribution(self) -> Dict[str, int]:
        """è·å–æ ‡ç­¾åˆ†å¸ƒ"""
        if not self.items:
            return {}
        
        label_counts = {}
        for item in self.items:
            label = item.get('label', -1)
            label_name = self.label_mapping.get(label, f'Unknown({label})')
            label_counts[label_name] = label_counts.get(label_name, 0) + 1
        return label_counts
    
    def get_sample_by_id(self, item_id: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ®IDè·å–æ ·æœ¬"""
        try:
            idx = self.item_ids.index(item_id)
            return self[idx]
        except ValueError:
            return None
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        if not self.items:
            return {'total_samples': 0}
        
        stats = {
            'total_samples': len(self.items),
            'label_distribution': self.get_label_distribution(),
            'has_image_count': 0,
            'text_length_stats': {}
        }
        
        # ç»Ÿè®¡å›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯
        text_lengths = []
        for item in self.items:
            # å›¾åƒç»Ÿè®¡
            if 'image_path' in item:
                image_path = self.data_dir / item['image_path']
                if image_path.exists():
                    stats['has_image_count'] += 1
            
            # æ–‡æœ¬é•¿åº¦ç»Ÿè®¡
            text = item.get('caption', '')
            text_lengths.append(len(text))
        
        # æ–‡æœ¬é•¿åº¦ç»Ÿè®¡
        if text_lengths:
            stats['text_length_stats'] = {
                'min': min(text_lengths),
                'max': max(text_lengths),
                'mean': np.mean(text_lengths),
                'std': np.std(text_lengths)
            }
        
        return stats
    
    def print_sample_info(self, idx: int = 0):
        """æ‰“å°æ ·æœ¬ä¿¡æ¯ç”¨äºè°ƒè¯•"""
        if idx >= len(self.items):
            print(f"âŒ ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´")
            return
        
        sample = self[idx]
        print(f"\nğŸ” æ ·æœ¬ {idx} ä¿¡æ¯:")
        print(f"   ID: {sample['item_id']}")
        print(f"   æ–‡æœ¬: {sample['text'][:50]}...")
        print(f"   æ ‡ç­¾: {sample['label']} ({self.label_mapping.get(sample['label'], 'Unknown')})")
        print(f"   æ–‡æœ¬é•¿åº¦: {sample['text_length']}")
        print(f"   æœ‰å›¾åƒ: {sample['has_image']}")
        if sample['has_image']:
            print(f"   å›¾åƒè·¯å¾„: {sample['image_path']}")
            print(f"   å›¾åƒå¼ é‡å½¢çŠ¶: {sample['image'].shape}")


# å…¼å®¹æ€§åˆ«å
SimpleMR2Dataset = MR2Dataset


# æµ‹è¯•å’Œæ¼”ç¤ºä»£ç 
def test_dataset():
    """æµ‹è¯•æ•°æ®é›†åŠŸèƒ½"""
    print("ğŸ“š æµ‹è¯•ä¸¥æ ¼MR2æ•°æ®é›†")
    
    try:
        # è·å–æ•°æ®ç›®å½•
        data_dir = get_data_dir()
        
        # å°è¯•åˆ›å»ºæ•°æ®é›†
        print(f"\nğŸ“‚ å°è¯•åŠ è½½æ•°æ®é›†...")
        dataset = MR2Dataset(
            data_dir=data_dir,
            split='train',
            transform_type='val',  # ä½¿ç”¨éªŒè¯æ¨¡å¼ï¼Œå‡å°‘éšæœºæ€§
            load_images=True
        )
        
        print(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œå¤§å°: {len(dataset)}")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = dataset.get_statistics()
        print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
        print(f"   æ ‡ç­¾åˆ†å¸ƒ: {stats['label_distribution']}")
        if 'text_length_stats' in stats and stats['text_length_stats']:
            print(f"   æ–‡æœ¬é•¿åº¦: å¹³å‡ {stats['text_length_stats']['mean']:.1f}, "
                  f"èŒƒå›´ {stats['text_length_stats']['min']}-{stats['text_length_stats']['max']}")
        
        # æµ‹è¯•æ ·æœ¬è®¿é—®
        if len(dataset) > 0:
            dataset.print_sample_info(0)
            
            # æµ‹è¯•å¤šä¸ªæ ·æœ¬
            print(f"\nğŸ§ª æµ‹è¯•å¤šä¸ªæ ·æœ¬:")
            for i in range(min(3, len(dataset))):
                sample = dataset[i]
                print(f"   æ ·æœ¬ {i}: æ ‡ç­¾={sample['label']}, æ–‡æœ¬é•¿åº¦={sample['text_length']}, æœ‰å›¾åƒ={sample['has_image']}")
        
        return dataset
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print("1. ç¡®ä¿MR2æ•°æ®é›†å·²ä¸‹è½½")
        print("2. æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("3. éªŒè¯æ•°æ®æ–‡ä»¶æ ¼å¼æ˜¯å¦å®Œæ•´")
        raise


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    try:
        dataset = test_dataset()
        
        if dataset and len(dataset) > 0:
            print(f"\nâœ… ä¸¥æ ¼MR2æ•°æ®é›†æµ‹è¯•å®Œæˆ")
            print(f"æ•°æ®é›†å¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼ŒåŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")
        
    except Exception as e:
        print(f"\nâŒ æ•°æ®é›†æµ‹è¯•å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿MR2æ•°æ®é›†å·²æ­£ç¡®å®‰è£…")
        sys.exit(1)
    
    print(f"\nğŸ“ ä½¿ç”¨è¯´æ˜:")
    print(f"1. å¿…é¡»ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨äºæ­£ç¡®è·¯å¾„")
    print(f"2. ä¸å†æ”¯æŒæ¼”ç¤ºæ•°æ®ï¼Œå¿…é¡»ä½¿ç”¨çœŸå®æ•°æ®é›†") 
    print(f"3. æ•°æ®é›†ä¼šè¿›è¡Œä¸¥æ ¼éªŒè¯ï¼Œç¡®ä¿æ•°æ®è´¨é‡")