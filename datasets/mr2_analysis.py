#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# code/datasets/mr2_analysis.py

"""
MR2å¤šæ¨¡æ€è°£è¨€æ£€æµ‹æ•°æ®é›†æ·±åº¦åˆ†æ
å…¨é¢åˆ†ææ•°æ®é›†ç»“æ„ã€åˆ†å¸ƒç‰¹å¾ï¼Œç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
ä¿®å¤ç‰ˆæœ¬ï¼šå¢å¼ºé”™è¯¯å¤„ç†å’Œæ•°æ®ä¸ºç©ºæ—¶çš„å¤„ç†
"""

import json
import os
import re
import sys
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Any
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®è‹±æ–‡å­—ä½“é¿å…å›¾è¡¨ä¹±ç ï¼Œå…¶ä»–è¾“å‡ºä¿æŒä¸­æ–‡
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

# æ·»åŠ é…ç½®ç®¡ç†å™¨è·¯å¾„
current_file = Path(__file__).resolve()
code_root = current_file.parent.parent
sys.path.append(str(code_root))

try:
    from utils.config_manager import get_config_manager, get_output_path, get_analysis_config, get_label_mapping, get_data_dir
    USE_CONFIG_MANAGER = True
except ImportError:
    print("âš ï¸  é…ç½®ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    USE_CONFIG_MANAGER = False


class MR2DatasetAnalyzer:
    """MR2æ•°æ®é›†åˆ†æå™¨ - å¢å¼ºç‰ˆ"""
    
    def __init__(self, data_dir: str = 'data', output_dir: str = 'outputs'):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„ (ç›¸å¯¹äºcodeç›®å½•)
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ (ç›¸å¯¹äºcodeç›®å½•)
        """
        if USE_CONFIG_MANAGER:
            # ä½¿ç”¨é…ç½®ç®¡ç†å™¨
            self.config_manager = get_config_manager()
            self.config_manager.create_output_directories()
            
            self.data_dir = get_data_dir()
            self.charts_dir = get_output_path('datasets', 'charts')
            self.reports_dir = get_output_path('datasets', 'reports')
            self.analysis_dir = get_output_path('datasets', 'analysis')
            
            analysis_config = get_analysis_config()
            viz_config = analysis_config.get('visualization', {})
            self.colors = viz_config.get('colors', self._default_colors())
            self.label_mapping = get_label_mapping()
            
            print(f"ğŸ”§ ä½¿ç”¨é…ç½®ç®¡ç†å™¨")
            print(f"ğŸ”§ æ•°æ®ç›®å½•: {self.data_dir}")
            print(f"ğŸ”§ è¾“å‡ºç›®å½•: {self.charts_dir.parent}")
            
        else:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            self.data_dir = Path(data_dir)
            self.output_dir = Path(output_dir)
            self.charts_dir = self.output_dir / 'charts'
            self.reports_dir = self.output_dir / 'reports'
            self.analysis_dir = self.output_dir / 'analysis'
            
            # åˆ›å»ºè¾“å‡ºç›®å½• - å…³é”®ä¿®å¤
            for dir_path in [self.charts_dir, self.reports_dir, self.analysis_dir]:
                dir_path.mkdir(parents=True, exist_ok=True)
            
            self.colors = self._default_colors()
            self.label_mapping = {0: 'Non-rumor', 1: 'Rumor', 2: 'Unverified'}
            
            print(f"ğŸ”§ ä½¿ç”¨é»˜è®¤é…ç½®")
            print(f"ğŸ”§ æ•°æ®ç›®å½•: {self.data_dir}")
            print(f"ğŸ”§ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # å­˜å‚¨åˆ†æç»“æœ
        self.analysis_results = {}
        self.datasets = {}
    
    def _default_colors(self):
        """é»˜è®¤é¢œè‰²é…ç½®"""
        return {
            'primary': '#FF6B6B',
            'secondary': '#4ECDC4', 
            'tertiary': '#45B7D1',
            'accent': '#96CEB4',
            'warning': '#FFEAA7',
            'info': '#DDA0DD',
            'success': '#98FB98'
        }
    
    def get_color(self, color_key: str, default_color: str = '#666666') -> str:
        """
        å®‰å…¨è·å–é¢œè‰²ï¼Œé¿å…KeyError
        
        Args:
            color_key: é¢œè‰²é”®å
            default_color: é»˜è®¤é¢œè‰²
            
        Returns:
            é¢œè‰²å€¼
        """
        return self.colors.get(color_key, default_color)
    
    def check_data_availability(self) -> Dict[str, bool]:
        """æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å¯ç”¨"""
        print("ğŸ” æ£€æŸ¥æ•°æ®æ–‡ä»¶å¯ç”¨æ€§...")
        
        availability = {}
        splits = ['train', 'val', 'test']
        
        for split in splits:
            file_path = self.data_dir / f'dataset_items_{split}.json'
            availability[split] = file_path.exists()
            if availability[split]:
                print(f"âœ… æ‰¾åˆ° {split} æ•°æ®æ–‡ä»¶")
            else:
                print(f"âŒ æœªæ‰¾åˆ° {split} æ•°æ®æ–‡ä»¶: {file_path}")
        
        return availability
    
    def create_demo_data(self):
        """åˆ›å»ºæ¼”ç¤ºæ•°æ®ï¼ˆå½“çœŸå®æ•°æ®ä¸å¯ç”¨æ—¶ï¼‰"""
        print("ğŸ”§ åˆ›å»ºæ¼”ç¤ºæ•°æ®...")
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ¼”ç¤ºæ•°æ®
        demo_texts = [
            "è¿™æ˜¯ä¸€ä¸ªè°£è¨€æ£€æµ‹çš„æ¼”ç¤ºæ–‡æœ¬",
            "This is a demo text for rumor detection",
            "æ··åˆè¯­è¨€çš„æ¼”ç¤ºæ–‡æœ¬ mixed language demo",
            "Breaking news about technology advancement",
            "å…³äºæ–°æŠ€æœ¯å‘å±•çš„é‡è¦æ¶ˆæ¯",
            "Fake news spreads faster than real news",
            "è™šå‡ä¿¡æ¯ä¼ æ’­é€Ÿåº¦æ¯”çœŸå®ä¿¡æ¯æ›´å¿«",
            "AI technology is revolutionizing the world",
            "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨æ”¹å˜ä¸–ç•Œ",
            "Climate change affects global economy"
        ]
        
        for split in ['train', 'val', 'test']:
            demo_data = {}
            num_samples = {'train': 8, 'val': 3, 'test': 4}[split]
            
            for i in range(num_samples):
                demo_data[str(i)] = {
                    'caption': demo_texts[i % len(demo_texts)],
                    'label': i % 3,  # 0, 1, 2 å¾ªç¯
                    'language': 'mixed',
                    'image_path': f'{split}/img/{i}.jpg'
                }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            demo_file = self.data_dir / f'dataset_items_{split}.json'
            with open(demo_file, 'w', encoding='utf-8') as f:
                json.dump(demo_data, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… åˆ›å»º {split} æ¼”ç¤ºæ•°æ®: {len(demo_data)} æ ·æœ¬")
        
        return True
        
    def load_data(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®é›†"""
        print("ğŸ”„ å¼€å§‹åŠ è½½MR2æ•°æ®é›†...")
        
        # æ£€æŸ¥æ•°æ®å¯ç”¨æ€§
        availability = self.check_data_availability()
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•æ•°æ®ï¼Œåˆ›å»ºæ¼”ç¤ºæ•°æ®
        if not any(availability.values()):
            print("â“ æ²¡æœ‰æ‰¾åˆ°çœŸå®æ•°æ®ï¼Œæ˜¯å¦åˆ›å»ºæ¼”ç¤ºæ•°æ®ï¼Ÿ")
            try:
                self.create_demo_data()
                # é‡æ–°æ£€æŸ¥å¯ç”¨æ€§
                availability = self.check_data_availability()
            except Exception as e:
                print(f"âŒ åˆ›å»ºæ¼”ç¤ºæ•°æ®å¤±è´¥: {e}")
        
        self.datasets = {}
        splits = ['train', 'val', 'test']
        
        for split in splits:
            if availability.get(split, False):
                file_path = self.data_dir / f'dataset_items_{split}.json'
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        self.datasets[split] = json.load(f)
                    print(f"âœ… åŠ è½½ {split} æ•°æ®: {len(self.datasets[split])} æ¡")
                except Exception as e:
                    print(f"âŒ åŠ è½½ {split} æ•°æ®å¤±è´¥: {e}")
                    
        return self.datasets
    
    def basic_statistics(self):
        """åŸºç¡€ç»Ÿè®¡åˆ†æ"""
        print("\nğŸ“Š === åŸºç¡€ç»Ÿè®¡åˆ†æ ===")
        
        if not self.datasets:
            print("âš ï¸  æ²¡æœ‰å¯ç”¨æ•°æ®è¿›è¡Œç»Ÿè®¡åˆ†æ")
            self.analysis_results['basic_stats'] = {}
            return {}
        
        stats = {}
        total_samples = 0
        
        for split, data in self.datasets.items():
            split_stats = {
                'total_samples': len(data),
                'label_distribution': Counter(),
                'has_image': 0,
                'has_direct_annotation': 0,
                'has_inverse_annotation': 0
            }
            
            for item_id, item in data.items():
                # æ ‡ç­¾åˆ†å¸ƒ
                label = item.get('label', -1)
                split_stats['label_distribution'][label] += 1
                
                # å›¾åƒæ–‡ä»¶æ£€æŸ¥
                if 'image_path' in item:
                    image_path = self.data_dir / item['image_path']
                    if image_path.exists():
                        split_stats['has_image'] += 1
                
                # æ£€ç´¢æ ‡æ³¨æ£€æŸ¥
                if 'direct_path' in item:
                    direct_path = self.data_dir / item['direct_path'] / 'direct_annotation.json'
                    if direct_path.exists():
                        split_stats['has_direct_annotation'] += 1
                        
                if 'inv_path' in item:
                    inv_path = self.data_dir / item['inv_path'] / 'inverse_annotation.json'
                    if inv_path.exists():
                        split_stats['has_inverse_annotation'] += 1
            
            stats[split] = split_stats
            total_samples += split_stats['total_samples']
            
            print(f"\n{split.upper()} æ•°æ®é›†:")
            print(f"  æ ·æœ¬æ€»æ•°: {split_stats['total_samples']}")
            print(f"  æ ‡ç­¾åˆ†å¸ƒ: {dict(split_stats['label_distribution'])}")
            print(f"  æœ‰å›¾åƒ: {split_stats['has_image']}")
            print(f"  æœ‰ç›´æ¥æ£€ç´¢: {split_stats['has_direct_annotation']}")
            print(f"  æœ‰åå‘æ£€ç´¢: {split_stats['has_inverse_annotation']}")
        
        print(f"\næ€»æ ·æœ¬æ•°: {total_samples}")
        self.analysis_results['basic_stats'] = stats
        return stats
    
    def text_analysis(self):
        """æ–‡æœ¬å†…å®¹åˆ†æ"""
        print("\nğŸ“ === æ–‡æœ¬å†…å®¹åˆ†æ ===")
        
        if not self.datasets:
            print("âš ï¸  æ²¡æœ‰å¯ç”¨æ•°æ®è¿›è¡Œæ–‡æœ¬åˆ†æ")
            self.analysis_results['text_stats'] = {}
            return {}
        
        text_stats = {
            'total_texts': 0,
            'length_distribution': [],
            'language_distribution': Counter(),
            'word_count_distribution': [],
            'character_distribution': Counter(),
            'common_words': Counter(),
            'samples_by_length': {'short': [], 'medium': [], 'long': []}
        }
        
        all_texts = []
        
        for split, data in self.datasets.items():
            for item_id, item in data.items():
                caption = item.get('caption', '').strip()
                if caption:
                    all_texts.append(caption)
                    text_stats['total_texts'] += 1
                    
                    # æ–‡æœ¬é•¿åº¦
                    text_length = len(caption)
                    text_stats['length_distribution'].append(text_length)
                    
                    # è¯æ•°ç»Ÿè®¡
                    words = caption.split()
                    word_count = len(words)
                    text_stats['word_count_distribution'].append(word_count)
                    
                    # å¸¸ç”¨è¯ç»Ÿè®¡
                    for word in words:
                        cleaned_word = re.sub(r'[^\w]', '', word.lower())
                        if len(cleaned_word) > 2:  # è¿‡æ»¤çŸ­è¯
                            text_stats['common_words'][cleaned_word] += 1
                    
                    # å­—ç¬¦åˆ†å¸ƒ
                    for char in caption.lower():
                        if char.isalpha():
                            text_stats['character_distribution'][char] += 1
                    
                    # è¯­è¨€æ£€æµ‹ (ç®€å•è§„åˆ™)
                    if re.search(r'[\u4e00-\u9fff]', caption):
                        if re.search(r'[a-zA-Z]', caption):
                            text_stats['language_distribution']['mixed'] += 1
                        else:
                            text_stats['language_distribution']['chinese'] += 1
                    else:
                        text_stats['language_distribution']['english'] += 1
                    
                    # æŒ‰é•¿åº¦åˆ†ç±»æ ·æœ¬
                    if text_length < 30:
                        if len(text_stats['samples_by_length']['short']) < 5:
                            text_stats['samples_by_length']['short'].append(caption)
                    elif text_length < 100:
                        if len(text_stats['samples_by_length']['medium']) < 5:
                            text_stats['samples_by_length']['medium'].append(caption)
                    else:
                        if len(text_stats['samples_by_length']['long']) < 3:
                            text_stats['samples_by_length']['long'].append(caption)
        
        # ç»Ÿè®¡æ‘˜è¦
        if text_stats['length_distribution']:
            print(f"æ–‡æœ¬æ€»æ•°: {text_stats['total_texts']}")
            print(f"å¹³å‡é•¿åº¦: {np.mean(text_stats['length_distribution']):.1f} å­—ç¬¦")
            print(f"å¹³å‡è¯æ•°: {np.mean(text_stats['word_count_distribution']):.1f} è¯")
            print(f"è¯­è¨€åˆ†å¸ƒ: {dict(text_stats['language_distribution'])}")
            print(f"æœ€å¸¸è§è¯æ±‡: {text_stats['common_words'].most_common(10)}")
            
            # å±•ç¤ºä¸åŒé•¿åº¦çš„æ ·æœ¬
            print(f"\nçŸ­æ–‡æœ¬æ ·ä¾‹ (<30å­—ç¬¦):")
            for i, text in enumerate(text_stats['samples_by_length']['short'][:5]):
                print(f"  {i+1}. {text}")
            
            print(f"\nä¸­ç­‰æ–‡æœ¬æ ·ä¾‹ (30-100å­—ç¬¦):")
            for i, text in enumerate(text_stats['samples_by_length']['medium'][:5]):
                print(f"  {i+1}. {text}")
                
            print(f"\né•¿æ–‡æœ¬æ ·ä¾‹ (>100å­—ç¬¦):")
            for i, text in enumerate(text_stats['samples_by_length']['long'][:3]):
                print(f"  {i+1}. {text}")
        
        self.analysis_results['text_stats'] = text_stats
        return text_stats
    
    def image_analysis(self):
        """å›¾åƒæ•°æ®åˆ†æ"""
        print("\nğŸ–¼ï¸  === å›¾åƒæ•°æ®åˆ†æ ===")
        
        if not self.datasets:
            print("âš ï¸  æ²¡æœ‰å¯ç”¨æ•°æ®è¿›è¡Œå›¾åƒåˆ†æ")
            self.analysis_results['image_stats'] = {}
            return {}
        
        image_stats = {
            'total_images': 0,
            'valid_images': 0,
            'image_sizes': [],
            'image_formats': Counter(),
            'size_distribution': {'width': [], 'height': []},
            'file_sizes': []
        }
        
        for split, data in self.datasets.items():
            for item_id, item in data.items():
                if 'image_path' in item:
                    image_stats['total_images'] += 1
                    image_path = self.data_dir / item['image_path']
                    
                    if image_path.exists():
                        try:
                            # å°è¯•æ‰“å¼€å›¾åƒ
                            with Image.open(image_path) as img:
                                image_stats['valid_images'] += 1
                                
                                # å›¾åƒå°ºå¯¸
                                width, height = img.size
                                image_stats['size_distribution']['width'].append(width)
                                image_stats['size_distribution']['height'].append(height)
                                image_stats['image_sizes'].append((width, height))
                                
                                # å›¾åƒæ ¼å¼
                                image_stats['image_formats'][img.format] += 1
                                
                                # æ–‡ä»¶å¤§å°
                                file_size = image_path.stat().st_size
                                image_stats['file_sizes'].append(file_size)
                                
                        except Exception as e:
                            print(f"âš ï¸  æ— æ³•è¯»å–å›¾åƒ {image_path}: {e}")
        
        if image_stats['valid_images'] > 0:
            print(f"å›¾åƒæ€»æ•°: {image_stats['total_images']}")
            print(f"æœ‰æ•ˆå›¾åƒ: {image_stats['valid_images']}")
            print(f"å›¾åƒæ ¼å¼: {dict(image_stats['image_formats'])}")
            print(f"å¹³å‡å°ºå¯¸: {np.mean(image_stats['size_distribution']['width']):.0f} x {np.mean(image_stats['size_distribution']['height']):.0f}")
            print(f"å¹³å‡æ–‡ä»¶å¤§å°: {np.mean(image_stats['file_sizes'])/1024:.1f} KB")
        else:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒæ–‡ä»¶")
        
        self.analysis_results['image_stats'] = image_stats
        return image_stats
    
    def annotation_analysis(self):
        """æ£€ç´¢æ ‡æ³¨æ•°æ®åˆ†æ"""
        print("\nğŸ” === æ£€ç´¢æ ‡æ³¨åˆ†æ ===")
        
        if not self.datasets:
            print("âš ï¸  æ²¡æœ‰å¯ç”¨æ•°æ®è¿›è¡Œæ ‡æ³¨åˆ†æ")
            self.analysis_results['annotation_stats'] = {
                'direct_annotations': 0,
                'inverse_annotations': 0,
                'direct_stats': {'total_retrieved_images': [], 'domains': Counter()},
                'inverse_stats': {'entities_count': [], 'common_entities': Counter()}
            }
            return self.analysis_results['annotation_stats']
        
        annotation_stats = {
            'direct_annotations': 0,
            'inverse_annotations': 0,
            'direct_stats': {
                'images_with_captions': [],
                'images_no_captions': [],
                'domains': Counter(),
                'total_retrieved_images': []
            },
            'inverse_stats': {
                'entities_count': [],
                'entity_scores': [],
                'best_guess_labels': [],
                'fully_matched': [],
                'partially_matched': [],
                'common_entities': Counter()
            }
        }
        
        for split, data in self.datasets.items():
            for item_id, item in data.items():
                # ç›´æ¥æ£€ç´¢åˆ†æ
                if 'direct_path' in item:
                    direct_file = self.data_dir / item['direct_path'] / 'direct_annotation.json'
                    if direct_file.exists():
                        try:
                            with open(direct_file, 'r', encoding='utf-8') as f:
                                direct_data = json.load(f)
                                annotation_stats['direct_annotations'] += 1
                                
                                # ç»Ÿè®¡æ£€ç´¢ç»“æœ
                                if 'images_with_captions' in direct_data:
                                    count = len(direct_data['images_with_captions'])
                                    annotation_stats['direct_stats']['images_with_captions'].append(count)
                                    
                                    # åŸŸåç»Ÿè®¡
                                    for img_info in direct_data['images_with_captions']:
                                        domain = img_info.get('domain', 'unknown')
                                        annotation_stats['direct_stats']['domains'][domain] += 1
                                
                                if 'images_with_no_captions' in direct_data:
                                    count = len(direct_data['images_with_no_captions'])
                                    annotation_stats['direct_stats']['images_no_captions'].append(count)
                                
                                # æ€»æ£€ç´¢å›¾åƒæ•°
                                total = len(direct_data.get('images_with_captions', [])) + len(direct_data.get('images_with_no_captions', []))
                                annotation_stats['direct_stats']['total_retrieved_images'].append(total)
                                
                        except Exception as e:
                            print(f"âš ï¸  è¯»å–ç›´æ¥æ£€ç´¢æ–‡ä»¶å¤±è´¥ {direct_file}: {e}")
                
                # åå‘æ£€ç´¢åˆ†æ
                if 'inv_path' in item:
                    inverse_file = self.data_dir / item['inv_path'] / 'inverse_annotation.json'
                    if inverse_file.exists():
                        try:
                            with open(inverse_file, 'r', encoding='utf-8') as f:
                                inverse_data = json.load(f)
                                annotation_stats['inverse_annotations'] += 1
                                
                                # å®ä½“åˆ†æ
                                entities = inverse_data.get('entities', [])
                                annotation_stats['inverse_stats']['entities_count'].append(len(entities))
                                
                                for entity in entities:
                                    annotation_stats['inverse_stats']['common_entities'][entity] += 1
                                
                                # å®ä½“åˆ†æ•°
                                scores = inverse_data.get('entities_scores', [])
                                annotation_stats['inverse_stats']['entity_scores'].extend(scores)
                                
                                # æœ€ä½³çŒœæµ‹æ ‡ç­¾
                                best_guess = inverse_data.get('best_guess_lbl', [])
                                annotation_stats['inverse_stats']['best_guess_labels'].extend(best_guess)
                                
                                # åŒ¹é…ç»“æœç»Ÿè®¡
                                fully_matched = len(inverse_data.get('all_fully_matched_captions', []))
                                partially_matched = len(inverse_data.get('all_partially_matched_captions', []))
                                annotation_stats['inverse_stats']['fully_matched'].append(fully_matched)
                                annotation_stats['inverse_stats']['partially_matched'].append(partially_matched)
                                
                        except Exception as e:
                            print(f"âš ï¸  è¯»å–åå‘æ£€ç´¢æ–‡ä»¶å¤±è´¥ {inverse_file}: {e}")
        
        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        print(f"ç›´æ¥æ£€ç´¢æ ‡æ³¨æ•°: {annotation_stats['direct_annotations']}")
        print(f"åå‘æ£€ç´¢æ ‡æ³¨æ•°: {annotation_stats['inverse_annotations']}")
        
        if annotation_stats['direct_stats']['total_retrieved_images']:
            print(f"å¹³å‡æ£€ç´¢å›¾åƒæ•°: {np.mean(annotation_stats['direct_stats']['total_retrieved_images']):.1f}")
            print(f"çƒ­é—¨åŸŸå: {annotation_stats['direct_stats']['domains'].most_common(5)}")
        
        if annotation_stats['inverse_stats']['entities_count']:
            print(f"å¹³å‡å®ä½“æ•°: {np.mean(annotation_stats['inverse_stats']['entities_count']):.1f}")
            print(f"å¸¸è§å®ä½“: {annotation_stats['inverse_stats']['common_entities'].most_common(10)}")
        
        self.analysis_results['annotation_stats'] = annotation_stats
        return annotation_stats
    
    def create_visualizations(self):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        print("\nğŸ“Š === ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ===")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.charts_dir.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if not self.analysis_results.get('basic_stats'):
            print("âš ï¸  æ²¡æœ‰åˆ†æç»“æœï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
            return
        
        # è®¾ç½®å›¾è¡¨å‚æ•°
        plt.style.use('default')
        
        try:
            # 1. æ•°æ®é›†åŸºç¡€åˆ†å¸ƒå›¾
            self._plot_basic_distribution()
            
            # 2. æ–‡æœ¬é•¿åº¦åˆ†å¸ƒå›¾
            self._plot_text_distribution()
            
            # 3. å›¾åƒå°ºå¯¸åˆ†å¸ƒå›¾
            self._plot_image_distribution()
            
            # 4. æ£€ç´¢ç»“æœåˆ†æå›¾
            self._plot_annotation_analysis()
            
            # 5. ç»¼åˆåˆ†æä»ªè¡¨æ¿
            self._create_dashboard()
            
            print("âœ… æ‰€æœ‰å›¾è¡¨å·²ç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {e}")
            print("è¿™å¯èƒ½æ˜¯ç”±äºæ•°æ®ä¸è¶³æˆ–æ ¼å¼é—®é¢˜å¯¼è‡´çš„")
    
    def _plot_basic_distribution(self):
        """ç»˜åˆ¶åŸºç¡€æ•°æ®åˆ†å¸ƒ - å¢å¼ºé”™è¯¯å¤„ç†"""
        try:
            if not self.analysis_results.get('basic_stats'):
                print("âš ï¸  è·³è¿‡åŸºç¡€åˆ†å¸ƒå›¾ï¼šæ²¡æœ‰ç»Ÿè®¡æ•°æ®")
                return
                
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('MR2 Dataset Basic Distribution Analysis', fontsize=16, fontweight='bold')
            
            # 1. æ•°æ®é›†å¤§å°åˆ†å¸ƒ
            stats = self.analysis_results['basic_stats']
            if stats:
                splits = list(stats.keys())
                sizes = [stats[split]['total_samples'] for split in splits]
                
                colors = [self.colors['primary'], self.colors['secondary'], self.colors['tertiary']]
                axes[0, 0].bar(splits, sizes, color=colors[:len(splits)])
                axes[0, 0].set_title('Dataset Size Distribution')
                axes[0, 0].set_ylabel('Number of Samples')
                for i, v in enumerate(sizes):
                    if v > 0:  # åªåœ¨æœ‰æ•°æ®æ—¶æ·»åŠ æ ‡ç­¾
                        axes[0, 0].text(i, v + max(sizes)*0.01, str(v), ha='center', va='bottom')
            else:
                axes[0, 0].text(0.5, 0.5, 'No Data Available', ha='center', va='center', transform=axes[0, 0].transAxes)
                axes[0, 0].set_title('Dataset Size Distribution')
            
            # 2. æ ‡ç­¾åˆ†å¸ƒ (åˆå¹¶æ‰€æœ‰split)
            all_labels = Counter()
            for split_stats in stats.values():
                all_labels.update(split_stats['label_distribution'])
            
            if all_labels and sum(all_labels.values()) > 0:
                labels = [self.label_mapping.get(k, f'Unknown({k})') for k in all_labels.keys()]
                counts = list(all_labels.values())
                
                axes[0, 1].pie(counts, labels=labels, autopct='%1.1f%%', colors=colors[:len(labels)])
                axes[0, 1].set_title('Label Distribution')
            else:
                axes[0, 1].text(0.5, 0.5, 'No Label Data', ha='center', va='center', transform=axes[0, 1].transAxes)
                axes[0, 1].set_title('Label Distribution')
            
            # 3. æ•°æ®å®Œæ•´æ€§åˆ†æ - ä¿®å¤å…³é”®é”™è¯¯
            if stats:
                completeness_data = []
                for split, split_stats in stats.items():
                    total = split_stats['total_samples']
                    if total > 0:  # é¿å…é™¤é›¶é”™è¯¯
                        completeness_data.append({
                            'Split': split,
                            'Has Image': split_stats['has_image'] / total * 100,
                            'Has Direct': split_stats['has_direct_annotation'] / total * 100,
                            'Has Inverse': split_stats['has_inverse_annotation'] / total * 100
                        })
                
                if completeness_data:
                    df_completeness = pd.DataFrame(completeness_data)
                    x = np.arange(len(df_completeness))
                    width = 0.25
                    
                    # å®‰å…¨è®¿é—®DataFrameåˆ—
                    try:
                        axes[1, 0].bar(x - width, df_completeness['Has Image'], width, label='Has Image', color=self.colors['primary'])
                        axes[1, 0].bar(x, df_completeness['Has Direct'], width, label='Has Direct Search', color=self.colors['secondary'])
                        axes[1, 0].bar(x + width, df_completeness['Has Inverse'], width, label='Has Inverse Search', color=self.colors['tertiary'])
                        
                        axes[1, 0].set_ylabel('Completeness (%)')
                        axes[1, 0].set_title('Data Completeness Analysis')
                        axes[1, 0].set_xticks(x)
                        axes[1, 0].set_xticklabels(df_completeness['Split'])
                        axes[1, 0].legend()
                    except KeyError as e:
                        print(f"âš ï¸  DataFrameåˆ—è®¿é—®é”™è¯¯: {e}")
                        axes[1, 0].text(0.5, 0.5, 'Data Processing Error', ha='center', va='center', transform=axes[1, 0].transAxes)
                        axes[1, 0].set_title('Data Completeness Analysis')
                else:
                    axes[1, 0].text(0.5, 0.5, 'No Completeness Data', ha='center', va='center', transform=axes[1, 0].transAxes)
                    axes[1, 0].set_title('Data Completeness Analysis')
            
            # 4. æŒ‰splitçš„æ ‡ç­¾åˆ†å¸ƒ - å¢å¼ºé”™è¯¯å¤„ç†
            if stats and all_labels:
                try:
                    split_label_data = []
                    for split, split_stats in stats.items():
                        for label, count in split_stats['label_distribution'].items():
                            split_label_data.append({
                                'Split': split,
                                'Label': self.label_mapping.get(label, f'Unknown({label})'),
                                'Count': count
                            })
                    
                    if split_label_data:
                        df_split_labels = pd.DataFrame(split_label_data)
                        pivot_df = df_split_labels.pivot(index='Split', columns='Label', values='Count').fillna(0)
                        
                        pivot_df.plot(kind='bar', ax=axes[1, 1], color=colors[:len(pivot_df.columns)])
                        axes[1, 1].set_title('Label Distribution by Split')
                        axes[1, 1].set_ylabel('Number of Samples')
                        axes[1, 1].legend(title='Labels')
                        axes[1, 1].tick_params(axis='x', rotation=0)
                    else:
                        axes[1, 1].text(0.5, 0.5, 'No Split Label Data', ha='center', va='center', transform=axes[1, 1].transAxes)
                        axes[1, 1].set_title('Label Distribution by Split')
                except Exception as e:
                    print(f"âš ï¸  å¤„ç†æ ‡ç­¾åˆ†å¸ƒæ—¶å‡ºé”™: {e}")
                    axes[1, 1].text(0.5, 0.5, 'Label Processing Error', ha='center', va='center', transform=axes[1, 1].transAxes)
                    axes[1, 1].set_title('Label Distribution by Split')
            else:
                axes[1, 1].text(0.5, 0.5, 'No Label Data', ha='center', va='center', transform=axes[1, 1].transAxes)
                axes[1, 1].set_title('Label Distribution by Split')
            
            plt.tight_layout()
            
            # å®‰å…¨ä¿å­˜å›¾è¡¨
            try:
                output_file = self.charts_dir / 'basic_distribution.png'
                plt.savefig(output_file, dpi=300, bbox_inches='tight')
                print(f"âœ… åŸºç¡€åˆ†å¸ƒå›¾å·²ä¿å­˜: {output_file}")
                plt.show()
            except Exception as e:
                print(f"âŒ ä¿å­˜åŸºç¡€åˆ†å¸ƒå›¾å¤±è´¥: {e}")
                plt.show()
                
        except Exception as e:
            print(f"âŒ ç”ŸæˆåŸºç¡€åˆ†å¸ƒå›¾å¤±è´¥: {e}")
    
    def _plot_text_distribution(self):
        """ç»˜åˆ¶æ–‡æœ¬åˆ†å¸ƒåˆ†æ - å¢å¼ºé”™è¯¯å¤„ç†"""
        try:
            text_stats = self.analysis_results.get('text_stats')
            if not text_stats or not text_stats.get('length_distribution'):
                print("âš ï¸  è·³è¿‡æ–‡æœ¬åˆ†å¸ƒå›¾ï¼šæ²¡æœ‰æ–‡æœ¬æ•°æ®")
                return
                
            fig, axes = plt.subplots(2, 3, figsize=(18, 10))
            fig.suptitle('Text Content Analysis', fontsize=16, fontweight='bold')
            
            # 1. æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
            if text_stats['length_distribution']:
                axes[0, 0].hist(text_stats['length_distribution'], bins=30, color=self.colors['primary'], alpha=0.7)
                axes[0, 0].set_title('Text Length Distribution')
                axes[0, 0].set_xlabel('Number of Characters')
                axes[0, 0].set_ylabel('Frequency')
            else:
                axes[0, 0].text(0.5, 0.5, 'No Text Length Data', ha='center', va='center', transform=axes[0, 0].transAxes)
                axes[0, 0].set_title('Text Length Distribution')
            
            # 2. è¯æ•°åˆ†å¸ƒ
            if text_stats['word_count_distribution']:
                axes[0, 1].hist(text_stats['word_count_distribution'], bins=20, color=self.colors['secondary'], alpha=0.7)
                axes[0, 1].set_title('Word Count Distribution')
                axes[0, 1].set_xlabel('Number of Words')
                axes[0, 1].set_ylabel('Frequency')
            else:
                axes[0, 1].text(0.5, 0.5, 'No Word Count Data', ha='center', va='center', transform=axes[0, 1].transAxes)
                axes[0, 1].set_title('Word Count Distribution')
            
            # 3. è¯­è¨€åˆ†å¸ƒ
            lang_data = text_stats.get('language_distribution')
            if lang_data and sum(lang_data.values()) > 0:
                colors = [self.colors['primary'], self.colors['secondary'], self.colors['tertiary']]
                axes[0, 2].pie(lang_data.values(), labels=lang_data.keys(), autopct='%1.1f%%', colors=colors[:len(lang_data)])
                axes[0, 2].set_title('Language Distribution')
            else:
                axes[0, 2].text(0.5, 0.5, 'No Language Data', ha='center', va='center', transform=axes[0, 2].transAxes)
                axes[0, 2].set_title('Language Distribution')
            
            # 4. å¸¸ç”¨è¯äº‘å½¢å¼çš„æŸ±çŠ¶å›¾
            common_words = text_stats.get('common_words', Counter()).most_common(15)
            if common_words:
                words, counts = zip(*common_words)
                
                axes[1, 0].barh(range(len(words)), counts, color=self.get_color('accent', '#96CEB4'))
                axes[1, 0].set_yticks(range(len(words)))
                axes[1, 0].set_yticklabels(words)
                axes[1, 0].set_title('Most Common Words (Top 15)')
                axes[1, 0].set_xlabel('Frequency')
            else:
                axes[1, 0].text(0.5, 0.5, 'No Word Data', ha='center', va='center', transform=axes[1, 0].transAxes)
                axes[1, 0].set_title('Most Common Words (Top 15)')
            
            # 5. å­—ç¬¦é¢‘ç‡åˆ†å¸ƒ
            char_freq = text_stats.get('character_distribution', Counter()).most_common(20)
            if char_freq:
                chars, freqs = zip(*char_freq)
                
                axes[1, 1].bar(range(len(chars)), freqs, color=self.get_color('warning', '#FFEAA7'))
                axes[1, 1].set_xticks(range(len(chars)))
                axes[1, 1].set_xticklabels(chars)
                axes[1, 1].set_title('Character Frequency Distribution (Top 20)')
                axes[1, 1].set_ylabel('Frequency')
            else:
                axes[1, 1].text(0.5, 0.5, 'No Character Data', ha='center', va='center', transform=axes[1, 1].transAxes)
                axes[1, 1].set_title('Character Frequency Distribution (Top 20)')
            
            # 6. æ–‡æœ¬é•¿åº¦ç»Ÿè®¡æ‘˜è¦
            if text_stats.get('length_distribution'):
                length_stats = {
                    'Average Length': np.mean(text_stats['length_distribution']),
                    'Median': np.median(text_stats['length_distribution']),
                    'Max Length': np.max(text_stats['length_distribution']),
                    'Min Length': np.min(text_stats['length_distribution']),
                    'Std Dev': np.std(text_stats['length_distribution'])
                }
                
                axes[1, 2].axis('off')
                stats_text = '\n'.join([f'{k}: {v:.1f}' for k, v in length_stats.items()])
                axes[1, 2].text(0.1, 0.5, f'Text Length Statistics:\n\n{stats_text}', 
                               transform=axes[1, 2].transAxes, fontsize=12,
                               verticalalignment='center', bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
            else:
                axes[1, 2].text(0.5, 0.5, 'No Statistics Available', ha='center', va='center', transform=axes[1, 2].transAxes)
            
            plt.tight_layout()
            
            # å®‰å…¨ä¿å­˜å›¾è¡¨
            try:
                output_file = self.charts_dir / 'text_distribution.png'
                plt.savefig(output_file, dpi=300, bbox_inches='tight')
                print(f"âœ… æ–‡æœ¬åˆ†å¸ƒå›¾å·²ä¿å­˜: {output_file}")
                plt.show()
            except Exception as e:
                print(f"âŒ ä¿å­˜æ–‡æœ¬åˆ†å¸ƒå›¾å¤±è´¥: {e}")
                plt.show()
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ–‡æœ¬åˆ†å¸ƒå›¾å¤±è´¥: {e}")
    
    def _plot_image_distribution(self):
        """ç»˜åˆ¶å›¾åƒåˆ†å¸ƒåˆ†æ - å¢å¼ºé”™è¯¯å¤„ç†"""
        try:
            image_stats = self.analysis_results.get('image_stats')
            if not image_stats or image_stats.get('valid_images', 0) == 0:
                print("âš ï¸  è·³è¿‡å›¾åƒåˆ†å¸ƒå›¾ï¼šæ²¡æœ‰æœ‰æ•ˆå›¾åƒæ•°æ®")
                return
                
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('Image Data Analysis', fontsize=16, fontweight='bold')
            
            # 1. å›¾åƒå°ºå¯¸æ•£ç‚¹å›¾
            widths = image_stats.get('size_distribution', {}).get('width', [])
            heights = image_stats.get('size_distribution', {}).get('height', [])
            
            if widths and heights:
                axes[0, 0].scatter(widths, heights, alpha=0.6, color=self.colors['primary'])
                axes[0, 0].set_xlabel('Width (pixels)')
                axes[0, 0].set_ylabel('Height (pixels)')
                axes[0, 0].set_title('Image Size Distribution')
            else:
                axes[0, 0].text(0.5, 0.5, 'No Size Data', ha='center', va='center', transform=axes[0, 0].transAxes)
                axes[0, 0].set_title('Image Size Distribution')
            
            # 2. å®½åº¦åˆ†å¸ƒç›´æ–¹å›¾
            if widths:
                axes[0, 1].hist(widths, bins=20, color=self.colors['secondary'], alpha=0.7)
                axes[0, 1].set_xlabel('Width (pixels)')
                axes[0, 1].set_ylabel('Frequency')
                axes[0, 1].set_title('Image Width Distribution')
            else:
                axes[0, 1].text(0.5, 0.5, 'No Width Data', ha='center', va='center', transform=axes[0, 1].transAxes)
                axes[0, 1].set_title('Image Width Distribution')
            
            # 3. é«˜åº¦åˆ†å¸ƒç›´æ–¹å›¾
            if heights:
                axes[1, 0].hist(heights, bins=20, color=self.colors['tertiary'], alpha=0.7)
                axes[1, 0].set_xlabel('Height (pixels)')
                axes[1, 0].set_ylabel('Frequency')
                axes[1, 0].set_title('Image Height Distribution')
            else:
                axes[1, 0].text(0.5, 0.5, 'No Height Data', ha='center', va='center', transform=axes[1, 0].transAxes)
                axes[1, 0].set_title('Image Height Distribution')
            
            # 4. å›¾åƒæ ¼å¼åˆ†å¸ƒ
            format_data = image_stats.get('image_formats')
            if format_data and sum(format_data.values()) > 0:
                colors = [
                    self.get_color('primary', '#FF6B6B'), 
                    self.get_color('secondary', '#4ECDC4'), 
                    self.get_color('tertiary', '#45B7D1'), 
                    self.get_color('accent', '#96CEB4')
                ]
                axes[1, 1].pie(format_data.values(), labels=format_data.keys(), autopct='%1.1f%%', colors=colors[:len(format_data)])
                axes[1, 1].set_title('Image Format Distribution')
            else:
                axes[1, 1].text(0.5, 0.5, 'No Format Data', ha='center', va='center', transform=axes[1, 1].transAxes)
                axes[1, 1].set_title('Image Format Distribution')
            
            plt.tight_layout()
            
            # å®‰å…¨ä¿å­˜å›¾è¡¨
            try:
                output_file = self.charts_dir / 'image_distribution.png'
                plt.savefig(output_file, dpi=300, bbox_inches='tight')
                print(f"âœ… å›¾åƒåˆ†å¸ƒå›¾å·²ä¿å­˜: {output_file}")
                plt.show()
            except Exception as e:
                print(f"âŒ ä¿å­˜å›¾åƒåˆ†å¸ƒå›¾å¤±è´¥: {e}")
                plt.show()
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›¾åƒåˆ†å¸ƒå›¾å¤±è´¥: {e}")
    
    def _plot_annotation_analysis(self):
        """ç»˜åˆ¶æ£€ç´¢æ ‡æ³¨åˆ†æ - ä¿®å¤ç‰ˆæœ¬ï¼Œæ˜¾ç¤ºæœ‰æ„ä¹‰çš„æ•°æ®"""
        try:
            annotation_stats = self.analysis_results.get('annotation_stats')
            if not annotation_stats:
                print("âš ï¸  è·³è¿‡æ ‡æ³¨åˆ†æå›¾ï¼šæ²¡æœ‰æ ‡æ³¨æ•°æ®")
                return
                
            fig, axes = plt.subplots(2, 3, figsize=(18, 10))
            fig.suptitle('Retrieval Annotation Analysis', fontsize=16, fontweight='bold')
            
            # 1. æ£€ç´¢æ•°é‡å¯¹æ¯” (ä¿æŒåŸæœ‰çš„æ­£ç¡®å†…å®¹)
            annotation_counts = [
                annotation_stats.get('direct_annotations', 0),
                annotation_stats.get('inverse_annotations', 0)
            ]
            
            if max(annotation_counts) > 0:
                axes[0, 0].bar(['Direct Search', 'Inverse Search'], annotation_counts, 
                              color=[self.colors['primary'], self.colors['secondary']])
                axes[0, 0].set_title('Annotation Count')
                axes[0, 0].set_ylabel('Number of Annotations')
                for i, v in enumerate(annotation_counts):
                    if v > 0:
                        axes[0, 0].text(i, v + max(annotation_counts)*0.01, str(v), ha='center', va='bottom')
            else:
                axes[0, 0].text(0.5, 0.5, 'No Annotation Data', ha='center', va='center', transform=axes[0, 0].transAxes)
                axes[0, 0].set_title('Annotation Count')
            
            # 2. ç›´æ¥æ£€ç´¢æ–‡ä»¶å¯ç”¨æ€§
            if annotation_stats.get('direct_available_files', 0) > 0 or annotation_stats.get('inverse_available_files', 0) > 0:
                file_counts = [
                    annotation_stats.get('direct_available_files', 0),
                    annotation_stats.get('inverse_available_files', 0)
                ]
                axes[0, 1].bar(['Direct Files', 'Inverse Files'], file_counts,
                              color=[self.colors['tertiary'], self.colors['accent']])
                axes[0, 1].set_title('Available Annotation Files')
                axes[0, 1].set_ylabel('Number of Files')
                for i, v in enumerate(file_counts):
                    if v > 0:
                        axes[0, 1].text(i, v + 0.1, str(v), ha='center', va='bottom')
            else:
                axes[0, 1].text(0.5, 0.5, 'No Files Found', ha='center', va='center', transform=axes[0, 1].transAxes)
                axes[0, 1].set_title('Available Annotation Files')
            
            # 3. åŸŸååˆ†å¸ƒ (å¦‚æœæœ‰ç›´æ¥æ£€ç´¢æ•°æ®)
            domains = annotation_stats.get('direct_stats', {}).get('domains', Counter())
            if domains and sum(domains.values()) > 0:
                top_domains = domains.most_common(5)
                domain_names, domain_counts = zip(*top_domains)
                
                axes[0, 2].barh(range(len(domain_names)), domain_counts, 
                               color=self.get_color('warning', '#FFEAA7'))
                axes[0, 2].set_yticks(range(len(domain_names)))
                axes[0, 2].set_yticklabels(domain_names)
                axes[0, 2].set_title('Top Domains (Direct Search)')
                axes[0, 2].set_xlabel('Count')
            else:
                axes[0, 2].text(0.5, 0.5, 'No Domain Data', ha='center', va='center', transform=axes[0, 2].transAxes)
                axes[0, 2].set_title('Top Domains (Direct Search)')
            
            # 4. å®ä½“ç»Ÿè®¡ (å¦‚æœæœ‰åå‘æ£€ç´¢æ•°æ®)
            entities_count = annotation_stats.get('inverse_stats', {}).get('entities_count', [])
            if entities_count:
                axes[1, 0].hist(entities_count, bins=10, color=self.get_color('info', '#DDA0DD'), alpha=0.7)
                axes[1, 0].set_title('Entities Count Distribution')
                axes[1, 0].set_xlabel('Number of Entities')
                axes[1, 0].set_ylabel('Frequency')
            else:
                axes[1, 0].text(0.5, 0.5, 'No Entity Data', ha='center', va='center', transform=axes[1, 0].transAxes)
                axes[1, 0].set_title('Entities Count Distribution')
            
            # 5. å¸¸è§å®ä½“è¯äº‘ (å¦‚æœæœ‰å®ä½“æ•°æ®)
            common_entities = annotation_stats.get('inverse_stats', {}).get('common_entities', Counter())
            if common_entities and sum(common_entities.values()) > 0:
                top_entities = common_entities.most_common(10)
                entity_names, entity_counts = zip(*top_entities)
                
                axes[1, 1].barh(range(len(entity_names)), entity_counts,
                               color=self.get_color('success', '#98FB98'))
                axes[1, 1].set_yticks(range(len(entity_names)))
                axes[1, 1].set_yticklabels(entity_names)
                axes[1, 1].set_title('Most Common Entities')
                axes[1, 1].set_xlabel('Frequency')
            else:
                axes[1, 1].text(0.5, 0.5, 'No Entity Data', ha='center', va='center', transform=axes[1, 1].transAxes)
                axes[1, 1].set_title('Most Common Entities')
            
            # 6. åŒ¹é…ç±»å‹åˆ†å¸ƒ
            fully_matched = annotation_stats.get('inverse_stats', {}).get('fully_matched', [])
            partially_matched = annotation_stats.get('inverse_stats', {}).get('partially_matched', [])
            
            if fully_matched or partially_matched:
                match_types = ['Fully Matched', 'Partially Matched']
                match_counts = [
                    sum(fully_matched) if fully_matched else 0,
                    sum(partially_matched) if partially_matched else 0
                ]
                
                if sum(match_counts) > 0:
                    axes[1, 2].pie(match_counts, labels=match_types, autopct='%1.1f%%',
                                  colors=[self.colors['primary'], self.colors['secondary']])
                    axes[1, 2].set_title('Match Types Distribution')
                else:
                    axes[1, 2].text(0.5, 0.5, 'No Match Data', ha='center', va='center', transform=axes[1, 2].transAxes)
                    axes[1, 2].set_title('Match Types Distribution')
            else:
                axes[1, 2].text(0.5, 0.5, 'No Match Data', ha='center', va='center', transform=axes[1, 2].transAxes)
                axes[1, 2].set_title('Match Types Distribution')
            
            plt.tight_layout()
            
            # å®‰å…¨ä¿å­˜å›¾è¡¨
            try:
                output_file = self.charts_dir / 'annotation_analysis.png'
                plt.savefig(output_file, dpi=300, bbox_inches='tight')
                print(f"âœ… æ ‡æ³¨åˆ†æå›¾å·²ä¿å­˜: {output_file}")
                plt.show()
            except Exception as e:
                print(f"âŒ ä¿å­˜æ ‡æ³¨åˆ†æå›¾å¤±è´¥: {e}")
                plt.show()
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ ‡æ³¨åˆ†æå›¾å¤±è´¥: {e}")
    
    def _create_dashboard(self):
        """åˆ›å»ºç»¼åˆåˆ†æä»ªè¡¨æ¿ - ä¿®å¤ç‰ˆæœ¬ï¼Œæ˜¾ç¤ºå®Œæ•´çš„ä»ªè¡¨æ¿"""
        try:
            fig, axes = plt.subplots(2, 3, figsize=(18, 10))
            fig.suptitle('MR2 Dataset Analysis Dashboard', fontsize=16, fontweight='bold')
            
            # è·å–åŸºç¡€ç»Ÿè®¡æ•°æ®
            stats = self.analysis_results.get('basic_stats', {})
            text_stats = self.analysis_results.get('text_stats', {})
            image_stats = self.analysis_results.get('image_stats', {})
            
            # 1. æ•°æ®é›†æ¦‚è§ˆ (ä¿æŒåŸæœ‰çš„æ­£ç¡®å†…å®¹)
            if stats:
                splits = list(stats.keys())
                sizes = [stats[split]['total_samples'] for split in splits]
                
                if sizes and max(sizes) > 0:
                    colors = [self.colors['primary'], self.colors['secondary'], self.colors['tertiary']]
                    axes[0, 0].bar(splits, sizes, color=colors[:len(splits)])
                    axes[0, 0].set_title('Dataset Overview')
                    axes[0, 0].set_ylabel('Samples')
                    # æ·»åŠ æ•°å€¼æ ‡ç­¾
                    for i, v in enumerate(sizes):
                        axes[0, 0].text(i, v + max(sizes)*0.01, str(v), ha='center', va='bottom')
                else:
                    axes[0, 0].text(0.5, 0.5, 'No Data', ha='center', va='center', transform=axes[0, 0].transAxes)
                    axes[0, 0].set_title('Dataset Overview')
            else:
                axes[0, 0].text(0.5, 0.5, 'No Data', ha='center', va='center', transform=axes[0, 0].transAxes)
                axes[0, 0].set_title('Dataset Overview')
            
            # 2. æ ‡ç­¾åˆ†å¸ƒ (ä¿æŒåŸæœ‰çš„æ­£ç¡®å†…å®¹)
            all_labels = Counter()
            for split_stats in stats.values():
                all_labels.update(split_stats.get('label_distribution', {}))
            
            if all_labels and sum(all_labels.values()) > 0:
                labels = [self.label_mapping.get(k, f'Label {k}') for k in all_labels.keys()]
                counts = list(all_labels.values())
                colors = [self.colors['primary'], self.colors['secondary'], self.colors['tertiary']]
                
                axes[0, 1].pie(counts, labels=labels, autopct='%1.1f%%', colors=colors[:len(labels)])
                axes[0, 1].set_title('Label Distribution')
            else:
                axes[0, 1].text(0.5, 0.5, 'No Labels', ha='center', va='center', transform=axes[0, 1].transAxes)
                axes[0, 1].set_title('Label Distribution')
            
            # 3. æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ (ä¿æŒåŸæœ‰çš„æ­£ç¡®å†…å®¹)
            if text_stats.get('length_distribution'):
                axes[0, 2].hist(text_stats['length_distribution'], bins=20, color=self.get_color('accent', '#96CEB4'), alpha=0.7)
                axes[0, 2].set_title('Text Length Distribution')
                axes[0, 2].set_xlabel('Characters')
                axes[0, 2].set_ylabel('Frequency')
            else:
                axes[0, 2].text(0.5, 0.5, 'No Text Data', ha='center', va='center', transform=axes[0, 2].transAxes)
                axes[0, 2].set_title('Text Length Distribution')
            
            # 4. è¯­è¨€åˆ†æ - ä¿®å¤ä¸ºçœŸå®çš„è¯­è¨€åˆ†å¸ƒ
            lang_data = text_stats.get('language_distribution', {})
            if lang_data and sum(lang_data.values()) > 0:
                languages = list(lang_data.keys())
                lang_counts = list(lang_data.values())
                colors = [self.colors['primary'], self.colors['secondary'], self.colors['tertiary']]
                
                axes[1, 0].pie(lang_counts, labels=languages, autopct='%1.1f%%', colors=colors[:len(languages)])
                axes[1, 0].set_title('Language Analysis')
            else:
                axes[1, 0].text(0.5, 0.5, 'No Language Data', ha='center', va='center', transform=axes[1, 0].transAxes)
                axes[1, 0].set_title('Language Analysis')
            
            # 5. è´¨é‡æŒ‡æ ‡ - æ˜¾ç¤ºæ•°æ®å®Œæ•´æ€§
            if stats:
                quality_metrics = []
                quality_values = []
                
                # è®¡ç®—å„ç§å®Œæ•´æ€§æŒ‡æ ‡
                total_samples = sum(s.get('total_samples', 0) for s in stats.values())
                total_images = sum(s.get('has_image', 0) for s in stats.values())
                total_direct = sum(s.get('has_direct_annotation', 0) for s in stats.values())
                total_inverse = sum(s.get('has_inverse_annotation', 0) for s in stats.values())
                
                if total_samples > 0:
                    quality_metrics = ['Image\nCompleteness', 'Direct\nRetrieval', 'Inverse\nRetrieval', 'Text\nAvailability']
                    quality_values = [
                        (total_images / total_samples) * 100,
                        (total_direct / total_samples) * 100,
                        (total_inverse / total_samples) * 100,
                        (text_stats.get('total_texts', 0) / total_samples) * 100 if total_samples > 0 else 0
                    ]
                    
                    bars = axes[1, 1].bar(quality_metrics, quality_values, 
                                         color=[self.colors['primary'], self.colors['secondary'], 
                                               self.colors['tertiary'], self.colors['accent']])
                    axes[1, 1].set_title('Quality Metrics')
                    axes[1, 1].set_ylabel('Completeness (%)')
                    axes[1, 1].set_ylim(0, 100)
                    
                    # æ·»åŠ ç™¾åˆ†æ¯”æ ‡ç­¾
                    for bar, value in zip(bars, quality_values):
                        height = bar.get_height()
                        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 1,
                                        f'{value:.1f}%', ha='center', va='bottom', fontsize=10)
                else:
                    axes[1, 1].text(0.5, 0.5, 'No Quality Data', ha='center', va='center', transform=axes[1, 1].transAxes)
                    axes[1, 1].set_title('Quality Metrics')
            else:
                axes[1, 1].text(0.5, 0.5, 'No Quality Data', ha='center', va='center', transform=axes[1, 1].transAxes)
                axes[1, 1].set_title('Quality Metrics')
            
            # 6. æ±‡æ€»ç»Ÿè®¡ - æ˜¾ç¤ºå…³é”®æ•°å­—
            axes[1, 2].axis('off')
            
            # æ”¶é›†æ±‡æ€»ä¿¡æ¯
            summary_info = []
            if stats:
                total_samples = sum(s.get('total_samples', 0) for s in stats.values())
                summary_info.append(f"Total Samples: {total_samples:,}")
                
                # æ ‡ç­¾åˆ†å¸ƒæ‘˜è¦
                if all_labels:
                    max_label = max(all_labels, key=all_labels.get)
                    max_label_name = self.label_mapping.get(max_label, f'Label {max_label}')
                    summary_info.append(f"Most Common: {max_label_name}")
            
            if text_stats:
                total_texts = text_stats.get('total_texts', 0)
                summary_info.append(f"Text Items: {total_texts:,}")
                
                if text_stats.get('length_distribution'):
                    avg_length = np.mean(text_stats['length_distribution'])
                    summary_info.append(f"Avg Length: {avg_length:.0f} chars")
            
            if image_stats:
                valid_images = image_stats.get('valid_images', 0)
                summary_info.append(f"Valid Images: {valid_images:,}")
            
            # æ˜¾ç¤ºæ±‡æ€»ä¿¡æ¯
            if summary_info:
                summary_text = '\n'.join(summary_info)
                axes[1, 2].text(0.1, 0.7, f'Summary Statistics:\n\n{summary_text}', 
                               transform=axes[1, 2].transAxes, fontsize=14,
                               verticalalignment='top', 
                               bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.7))
            else:
                axes[1, 2].text(0.5, 0.5, 'No Summary Data', ha='center', va='center', transform=axes[1, 2].transAxes)
            
            axes[1, 2].set_title('Summary Statistics')
            
            plt.tight_layout()
            
            # å®‰å…¨ä¿å­˜å›¾è¡¨
            try:
                output_file = self.charts_dir / 'comprehensive_dashboard.png'
                plt.savefig(output_file, dpi=300, bbox_inches='tight')
                print(f"âœ… ç»¼åˆä»ªè¡¨æ¿å·²ä¿å­˜: {output_file}")
                plt.show()
            except Exception as e:
                print(f"âŒ ä¿å­˜ç»¼åˆä»ªè¡¨æ¿å¤±è´¥: {e}")
                plt.show()
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆç»¼åˆä»ªè¡¨æ¿å¤±è´¥: {e}")
    
    def generate_report(self):
        """ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š - å¢å¼ºé”™è¯¯å¤„ç†"""
        print("\nğŸ“„ === ç”Ÿæˆåˆ†ææŠ¥å‘Š ===")
        
        try:
            report_content = []
            report_content.append("# MR2å¤šæ¨¡æ€è°£è¨€æ£€æµ‹æ•°æ®é›†åˆ†ææŠ¥å‘Š\n")
            report_content.append(f"**ç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            report_content.append("---\n")
            
            # åŸºç¡€ç»Ÿè®¡
            stats = self.analysis_results.get('basic_stats', {})
            if stats:
                total_samples = sum(s.get('total_samples', 0) for s in stats.values())
                report_content.append("## æ‰§è¡Œæ‘˜è¦\n")
                report_content.append(f"MR2æ•°æ®é›†åŒ…å« **{total_samples:,}** ä¸ªå¤šæ¨¡æ€æ ·æœ¬ã€‚\n")
                
                # æ•°æ®é›†ç»Ÿè®¡
                report_content.append("## æ•°æ®é›†ç»Ÿè®¡\n")
                report_content.append("### æ•°æ®é‡åˆ†å¸ƒ\n")
                for split, split_stats in stats.items():
                    report_content.append(f"- **{split.upper()}**: {split_stats.get('total_samples', 0):,} æ ·æœ¬")
                
                # æ ‡ç­¾åˆ†å¸ƒ
                all_labels = Counter()
                for split_stats in stats.values():
                    all_labels.update(split_stats.get('label_distribution', {}))
                
                if all_labels:
                    report_content.append("\n### æ ‡ç­¾åˆ†å¸ƒ\n")
                    for label, count in all_labels.items():
                        label_name = self.label_mapping.get(label, f'Unknown({label})')
                        percentage = count / total_samples * 100 if total_samples > 0 else 0
                        report_content.append(f"- **{label_name}**: {count:,} ({percentage:.1f}%)")
            else:
                report_content.append("## æ‰§è¡Œæ‘˜è¦\n")
                report_content.append("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®é›†æ–‡ä»¶ã€‚\n")
            
            # æ–‡æœ¬åˆ†æç»“æœ
            text_stats = self.analysis_results.get('text_stats', {})
            if text_stats and text_stats.get('total_texts', 0) > 0:
                report_content.append("\n## æ–‡æœ¬å†…å®¹åˆ†æ\n")
                report_content.append(f"- **æ–‡æœ¬æ€»æ•°**: {text_stats['total_texts']:,}")
                
                if text_stats.get('length_distribution'):
                    avg_length = np.mean(text_stats['length_distribution'])
                    report_content.append(f"- **å¹³å‡é•¿åº¦**: {avg_length:.1f} å­—ç¬¦")
                
                if text_stats.get('word_count_distribution'):
                    avg_words = np.mean(text_stats['word_count_distribution'])
                    report_content.append(f"- **å¹³å‡è¯æ•°**: {avg_words:.1f} è¯")
                
                # è¯­è¨€åˆ†å¸ƒ
                lang_dist = text_stats.get('language_distribution', {})
                if lang_dist:
                    report_content.append("\n### è¯­è¨€åˆ†å¸ƒ")
                    total_texts = text_stats['total_texts']
                    for lang, count in lang_dist.items():
                        percentage = count / total_texts * 100 if total_texts > 0 else 0
                        report_content.append(f"- **{lang}**: {count:,} ({percentage:.1f}%)")
            
            # æ•°æ®è´¨é‡è¯„ä¼°
            if stats:
                report_content.append("\n## æ•°æ®è´¨é‡è¯„ä¼°\n")
                for split, split_stats in stats.items():
                    total = split_stats.get('total_samples', 0)
                    if total > 0:
                        image_rate = split_stats.get('has_image', 0) / total * 100
                        direct_rate = split_stats.get('has_direct_annotation', 0) / total * 100
                        inverse_rate = split_stats.get('has_inverse_annotation', 0) / total * 100
                        
                        report_content.append(f"### {split.upper()} æ•°æ®é›†")
                        report_content.append(f"- **å›¾åƒå®Œæ•´æ€§**: {image_rate:.1f}%")
                        report_content.append(f"- **ç›´æ¥æ£€ç´¢å®Œæ•´æ€§**: {direct_rate:.1f}%")
                        report_content.append(f"- **åå‘æ£€ç´¢å®Œæ•´æ€§**: {inverse_rate:.1f}%")
            
            # å»ºè®®å’Œç»“è®º
            report_content.append("\n## å»ºè®®å’Œç»“è®º\n")
            report_content.append("### æ•°æ®é›†ç‰¹ç‚¹")
            report_content.append("1. **å¤šæ¨¡æ€æ•°æ®**: ç»“åˆæ–‡æœ¬ã€å›¾åƒå’Œæ£€ç´¢ä¿¡æ¯")
            report_content.append("2. **å¤šè¯­è¨€æ”¯æŒ**: ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬")
            report_content.append("3. **ä¸°å¯Œæ ‡æ³¨**: åŒ…å«æ£€ç´¢éªŒè¯ä¿¡æ¯")
            
            # ä¿å­˜æŠ¥å‘Š
            report_text = '\n'.join(report_content)
            
            # ç¡®ä¿æŠ¥å‘Šç›®å½•å­˜åœ¨
            self.reports_dir.mkdir(parents=True, exist_ok=True)
            report_file = self.reports_dir / 'mr2_dataset_analysis_report.md'
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report_text)
            
            print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            return report_text
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆåˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            return ""
    
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®é›†åˆ†ææµç¨‹"""
        print("ğŸš€ === å¼€å§‹MR2æ•°æ®é›†å®Œæ•´åˆ†æ ===")
        
        try:
            # 1. åŠ è½½æ•°æ®
            self.load_data()
            
            # 2. åŸºç¡€ç»Ÿè®¡åˆ†æ
            self.basic_statistics()
            
            # 3. æ–‡æœ¬åˆ†æ
            self.text_analysis()
            
            # 4. å›¾åƒåˆ†æ
            self.image_analysis()
            
            # 5. æ£€ç´¢æ ‡æ³¨åˆ†æ
            self.annotation_analysis()
            
            # 6. åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
            self.create_visualizations()
            
            # 7. ç”Ÿæˆåˆ†ææŠ¥å‘Š
            self.generate_report()
            
            print(f"\nğŸ‰ === åˆ†æå®Œæˆ! ===")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.charts_dir.parent}")
            print(f"ğŸ“Š å›¾è¡¨ç›®å½•: {self.charts_dir}")
            print(f"ğŸ“„ æŠ¥å‘Šç›®å½•: {self.reports_dir}")
            
            return self.analysis_results
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            print("è¿™å¯èƒ½æ˜¯ç”±äºæ•°æ®æ–‡ä»¶ç¼ºå¤±æˆ–æ ¼å¼é—®é¢˜å¯¼è‡´çš„")
            return {}

# ä¸»æ‰§è¡Œä»£ç 
if __name__ == "__main__":
    print("ğŸ” MR2æ•°æ®é›†æ·±åº¦åˆ†æå·¥å…·")
    
    try:
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹ (è‡ªåŠ¨æ£€æµ‹é…ç½®)
        analyzer = MR2DatasetAnalyzer()
        
        # è¿è¡Œå®Œæ•´åˆ†æ
        results = analyzer.run_complete_analysis()
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        if results and results.get('basic_stats'):
            print("\n" + "="*50)
            print("åˆ†æç»“æœé¢„è§ˆ:")
            print("="*50)
            
            for split, stats in results['basic_stats'].items():
                print(f"\n{split.upper()} æ•°æ®é›†: {stats['total_samples']} æ ·æœ¬")
                print(f"  æ ‡ç­¾åˆ†å¸ƒ: {dict(stats['label_distribution'])}")
                print(f"  æ•°æ®å®Œæ•´æ€§: å›¾åƒ({stats['has_image']}) ç›´æ¥æ£€ç´¢({stats['has_direct_annotation']}) åå‘æ£€ç´¢({stats['has_inverse_annotation']})")
        else:
            print("\nâš ï¸  åˆ†ææœªå®Œæˆæˆ–æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
            print("è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨äºæ­£ç¡®çš„ä½ç½®")
    
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥:")
        print("1. æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        print("2. ä¾èµ–åŒ…æ˜¯å¦å®‰è£…å®Œæ•´")
        print("3. è¾“å‡ºç›®å½•æ˜¯å¦æœ‰å†™å…¥æƒé™")
    
    print("\nâœ… ç¨‹åºæ‰§è¡Œå®Œæˆ")