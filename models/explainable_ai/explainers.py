#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# models/explainable_ai/explainers.py

"""
å¯è§£é‡Šæ€§AIæ¨¡å—
åŒ…å«LIMEå’ŒSHAPç­‰æ¨¡å‹è§£é‡Šæ–¹æ³•çš„å®ç°
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import lime
import lime.lime_tabular
import shap
from typing import Dict, List, Tuple, Any, Optional, Union
from pathlib import Path
import sys
import warnings
warnings.filterwarnings('ignore')

# å¿«é€Ÿè·¯å¾„è®¾ç½® (æ ¹æ®å®é™…é¡¹ç›®ç»“æ„è°ƒæ•´)
current_file = Path(__file__).resolve()
# å‡è®¾ explainable_ai æ–‡ä»¶å¤¹åœ¨ models æ–‡ä»¶å¤¹ä¸‹ï¼Œmodels åœ¨é¡¹ç›®æ ¹ç›®å½•
project_root = current_file.parent.parent.parent 
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

print(f"Project root (from explainers.py): {project_root}")

# å¯¼å…¥é¡¹ç›®æ¨¡å— (æ ¹æ®éœ€è¦è°ƒæ•´)
try:
    # å‡è®¾ä½ éœ€è¦ä»ä¼ ç»Ÿæ¨¡å‹ä¸­åŠ è½½ä¸€ä¸ªå·²è®­ç»ƒçš„æ¨¡å‹æˆ–æ•°æ®
    from models.traditional.ml_classifiers import MLClassifierTrainer 
    # å‡è®¾ä½ éœ€è¦æ•°æ®åŠ è½½å™¨
    from data_utils.data_loaders import create_all_dataloaders 
    # å‡è®¾ä½ éœ€è¦é…ç½®æ–‡ä»¶ç®¡ç†å™¨
    from utils.config_manager import get_config_manager, get_output_path
    USE_PROJECT_MODULES = True
    print("âœ… æˆåŠŸå¯¼å…¥é¡¹ç›®æ¨¡å— (explainers.py)")
except ImportError as e:
    print(f"âš ï¸ å¯¼å…¥é¡¹ç›®æ¨¡å—å¤±è´¥ (explainers.py): {e}")
    USE_PROJECT_MODULES = False

import logging
logger = logging.getLogger(__name__)


class ModelExplainer:
    """
    æ¨¡å‹è§£é‡Šå™¨åŸºç±»
    """
    def __init__(self, model: Any, feature_names: List[str]):
        """
        åˆå§‹åŒ–è§£é‡Šå™¨

        Args:
            model: å·²è®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        """
        self.model = model
        self.feature_names = feature_names

    def explain_instance(self, instance: np.ndarray, **kwargs) -> Any:
        """
        è§£é‡Šå•ä¸ªæ ·æœ¬çš„é¢„æµ‹

        Args:
            instance: å•ä¸ªæ ·æœ¬çš„ç‰¹å¾å€¼ (1D numpy array)
            **kwargs: è§£é‡Šæ–¹æ³•ç‰¹å®šçš„å‚æ•°

        Returns:
            è§£é‡Šç»“æœ
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°æ­¤æ–¹æ³•")

    def explain_model(self, data: np.ndarray, **kwargs) -> Any:
        """
        è§£é‡Šæ•´ä¸ªæ¨¡å‹çš„è¡Œä¸º

        Args:
            data: ç”¨äºè§£é‡Šçš„æ•°æ®é›† (2D numpy array)
            **kwargs: è§£é‡Šæ–¹æ³•ç‰¹å®šçš„å‚æ•°

        Returns:
            æ¨¡å‹çº§åˆ«çš„è§£é‡Šç»“æœ
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°æ­¤æ–¹æ³•")


class LimeExplainer(ModelExplainer):
    """
    ä½¿ç”¨LIMEè¿›è¡Œæ¨¡å‹è§£é‡Š
    """
    def __init__(self, model: Any, training_data: np.ndarray, feature_names: List[str], 
                 class_names: List[str], mode: str = "classification"):
        """
        åˆå§‹åŒ–LIMEè§£é‡Šå™¨

        Args:
            model: å·²è®­ç»ƒçš„åˆ†ç±»æˆ–å›å½’æ¨¡å‹
            training_data: ç”¨äºLIMEèƒŒæ™¯åˆ†å¸ƒçš„è®­ç»ƒæ•°æ® (numpy array)
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
            class_names: ç±»åˆ«åç§°åˆ—è¡¨ (ä»…åˆ†ç±»ä»»åŠ¡)
            mode: 'classification' æˆ– 'regression'
        """
        super().__init__(model, feature_names)
        self.training_data = training_data
        self.class_names = class_names
        self.mode = mode
        self.explainer = lime.lime_tabular.LimeTabularExplainer(
            training_data=self.training_data,
            feature_names=self.feature_names,
            class_names=self.class_names,
            mode=self.mode,
            discretize_continuous=True
        )
        print("âœ… LIMEè§£é‡Šå™¨åˆå§‹åŒ–å®Œæˆ")

    def explain_instance(self, instance: np.ndarray, num_features: int = 5, **kwargs) -> lime.explanation.Explanation:
        """
        è§£é‡Šå•ä¸ªæ ·æœ¬çš„é¢„æµ‹

        Args:
            instance: å•ä¸ªæ ·æœ¬ (1D numpy array)
            num_features: è¦æ˜¾ç¤ºçš„ç‰¹å¾æ•°é‡
            **kwargs: ä¼ é€’ç»™LIME explainer.explain_instanceçš„é¢å¤–å‚æ•°

        Returns:
            LIMEçš„Explanationå¯¹è±¡
        """
        if self.mode == "classification":
            # åˆ†ç±»ä»»åŠ¡éœ€è¦ predict_proba æ–¹æ³•
            if not hasattr(self.model, 'predict_proba'):
                raise AttributeError("åˆ†ç±»æ¨¡å‹å¿…é¡»æœ‰ predict_proba æ–¹æ³•æ‰èƒ½ä½¿ç”¨LIMEã€‚")
            explanation = self.explainer.explain_instance(
                data_row=instance,
                predict_fn=self.model.predict_proba,
                num_features=num_features,
                **kwargs
            )
        else: # regression
            if not hasattr(self.model, 'predict'):
                raise AttributeError("å›å½’æ¨¡å‹å¿…é¡»æœ‰ predict æ–¹æ³•æ‰èƒ½ä½¿ç”¨LIMEã€‚")
            explanation = self.explainer.explain_instance(
                data_row=instance,
                predict_fn=self.model.predict,
                num_features=num_features,
                **kwargs
            )
        return explanation

    def explain_model(self, data: np.ndarray, **kwargs) -> Any:
        """
        LIMEé€šå¸¸ç”¨äºè§£é‡Šå±€éƒ¨é¢„æµ‹ï¼Œå…¨å±€è§£é‡Šå¯ä»¥é€šè¿‡èšåˆå±€éƒ¨è§£é‡Šå®ç°ï¼Œ
        ä½†LIMEæœ¬èº«ä¸ç›´æ¥æä¾›ä¸€ä¸ªæ ‡å‡†çš„â€œå…¨å±€æ¨¡å‹è§£é‡Šâ€å¯¹è±¡ã€‚
        è¿™é‡Œå¯ä»¥è¿”å›å¤šä¸ªæ ·æœ¬çš„è§£é‡Šã€‚
        """
        print("âš ï¸  LIMEä¸»è¦ç”¨äºå±€éƒ¨è§£é‡Šã€‚æ­¤æ–¹æ³•å°†è¿”å›å¤šä¸ªå®ä¾‹çš„è§£é‡Šã€‚")
        explanations = []
        for i in range(min(5, data.shape[0])): # è§£é‡Šå‰5ä¸ªæ ·æœ¬ä½œä¸ºç¤ºä¾‹
            explanations.append(self.explain_instance(data[i], **kwargs))
        return explanations


class ShapExplainer(ModelExplainer):
    """
    ä½¿ç”¨SHAPè¿›è¡Œæ¨¡å‹è§£é‡Š
    """
    def __init__(self, model: Any, data: Optional[pd.DataFrame] = None, feature_names: Optional[List[str]] = None):
        """
        åˆå§‹åŒ–SHAPè§£é‡Šå™¨

        Args:
            model: å·²è®­ç»ƒçš„æ¨¡å‹
            data: ç”¨äºSHAPèƒŒæ™¯åˆ†å¸ƒçš„æ•°æ® (Pandas DataFrame æˆ– numpy array)ã€‚
                  å¯¹äºæŸäº›SHAPè§£é‡Šå™¨ç±»å‹ï¼ˆå¦‚TreeExplainerçš„æŸäº›æƒ…å†µï¼‰å¯èƒ½ä¸éœ€è¦ã€‚
            feature_names: ç‰¹å¾åç§°ï¼Œå¦‚æœdataæ˜¯numpy arrayåˆ™éœ€è¦æä¾›ã€‚
        """
        if feature_names is None and data is not None and isinstance(data, np.ndarray):
            raise ValueError("å½“dataæ˜¯numpy arrayæ—¶ï¼Œå¿…é¡»æä¾›feature_namesã€‚")
        
        _feature_names = feature_names
        if data is not None and isinstance(data, pd.DataFrame):
            _feature_names = data.columns.tolist()
        
        super().__init__(model, _feature_names)
        self.data = data
        
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åˆé€‚çš„SHAPè§£é‡Šå™¨
        # TreeExplainer: é€‚ç”¨äºæ ‘æ¨¡å‹ (RandomForest, XGBoost, LightGBM, CatBoost)
        # KernelExplainer: æ¨¡å‹æ— å…³ï¼Œä½†é€Ÿåº¦è¾ƒæ…¢
        # DeepExplainer: é€‚ç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹ (TensorFlow, Keras, PyTorch)
        # LinearExplainer: é€‚ç”¨äºçº¿æ€§æ¨¡å‹
        
        # å°è¯•ä¸ºæ ‘æ¨¡å‹ä½¿ç”¨TreeExplainer
        if hasattr(model, 'predict') and (isinstance(model, (RandomForestClassifier, LogisticRegression)) or "xgboost" in str(type(model)).lower() or "lightgbm" in str(type(model)).lower()):
             # å¯¹äºéæ·±åº¦å­¦ä¹ çš„sklearnæ¨¡å‹ï¼Œé€šå¸¸SHAPä¼šå°è¯•å°è£…
            print("INFO: å°è¯•ä½¿ç”¨ shap.Explainer è‡ªåŠ¨é€‰æ‹©è§£é‡Šå™¨...")
            self.explainer = shap.Explainer(self.model, self.data)
        elif "torch" in str(type(model)).lower() and hasattr(model, 'forward'):
            # å‡è®¾æ˜¯PyTorchæ¨¡å‹ï¼Œéœ€è¦ DeepExplainer æˆ– GradientExplainer
            # æ³¨æ„: PyTorchæ¨¡å‹çš„SHAPè§£é‡Šé€šå¸¸æ›´å¤æ‚ï¼Œå¯èƒ½éœ€è¦ç‰¹å®šçš„åŒ…è£…æˆ–è¾“å…¥æ ¼å¼
            print(f"INFO: æ£€æµ‹åˆ°PyTorchæ¨¡å‹ã€‚ä½ å¯èƒ½éœ€è¦ä½¿ç”¨ shap.DeepExplainer æˆ– shap.GradientExplainerï¼Œå¹¶ç¡®ä¿è¾“å…¥æ ¼å¼æ­£ç¡®ã€‚")
            # ç¤ºä¾‹: self.explainer = shap.DeepExplainer(self.model, background_data_tensor)
            # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ KernelExplainer ä½œä¸ºé€šç”¨å›é€€ï¼Œä½†å®ƒå¯èƒ½å¾ˆæ…¢
            if self.data is None:
                raise ValueError("å¯¹äº KernelExplainerï¼Œå¿…é¡»æä¾›èƒŒæ™¯æ•°æ®ã€‚")
            print("WARN: å›é€€åˆ° shap.KernelExplainerï¼Œå¯¹äºå¤æ‚æ¨¡å‹å¯èƒ½ä¼šå¾ˆæ…¢ã€‚")
            self.explainer = shap.KernelExplainer(self.model.predict_proba if hasattr(self.model, 'predict_proba') else self.model.predict, self.data)
        else:
            # å¯¹äºå…¶ä»–æœªçŸ¥ç±»å‹çš„æ¨¡å‹ï¼ŒKernelExplaineræ˜¯ä¸€ä¸ªæ›´é€šç”¨çš„é€‰æ‹©ï¼Œä½†å¯èƒ½è¾ƒæ…¢
            if self.data is None:
                raise ValueError("å¯¹äº KernelExplainerï¼Œå¿…é¡»æä¾›èƒŒæ™¯æ•°æ®ã€‚")
            print("INFO: æœªæ£€æµ‹åˆ°ç‰¹å®šæ¨¡å‹ç±»å‹ï¼Œå°è¯•ä½¿ç”¨ shap.KernelExplainerã€‚")
            # KernelExplainer éœ€è¦ä¸€ä¸ªè¿”å›æ¦‚ç‡ï¼ˆåˆ†ç±»ï¼‰æˆ–é¢„æµ‹å€¼ï¼ˆå›å½’ï¼‰çš„å‡½æ•°
            predict_fn = self.model.predict_proba if hasattr(self.model, 'predict_proba') else self.model.predict
            self.explainer = shap.KernelExplainer(predict_fn, self.data)
            
        print("âœ… SHAPè§£é‡Šå™¨åˆå§‹åŒ–å®Œæˆ")

    def explain_instance(self, instance: Union[np.ndarray, pd.DataFrame], **kwargs) -> shap.Explanation:
        """
        è§£é‡Šå•ä¸ªæ ·æœ¬çš„é¢„æµ‹

        Args:
            instance: å•ä¸ªæ ·æœ¬ (1D numpy array æˆ– Pandas Series/DataFrame)
            **kwargs: ä¼ é€’ç»™SHAP explainerçš„é¢å¤–å‚æ•°

        Returns:
            SHAPçš„Explanationå¯¹è±¡æˆ–SHAPå€¼æ•°ç»„
        """
        # TreeExplainerå¯ä»¥ç›´æ¥å¤„ç†numpy arrayï¼ŒKernelExplainerä¹Ÿæ˜¯
        # å¦‚æœinstanceæ˜¯1D numpy arrayï¼Œä¸”explaineræœŸæœ›DataFrameï¼Œéœ€è¦è½¬æ¢
        if isinstance(instance, np.ndarray) and instance.ndim == 1 and self.feature_names:
            if isinstance(self.data, pd.DataFrame) or (hasattr(self.explainer, 'expected_data_format') and self.explainer.expected_data_format == 'dataframe'):
                 instance_df = pd.DataFrame([instance], columns=self.feature_names)
                 shap_values_instance = self.explainer(instance_df, **kwargs)
                 return shap_values_instance
        
        # å¯¹äºå…¶ä»–æƒ…å†µæˆ–å¦‚æœexplainerèƒ½ç›´æ¥å¤„ç†instanceç±»å‹
        shap_values_instance = self.explainer(instance, **kwargs)
        return shap_values_instance

    def explain_model(self, data: Union[np.ndarray, pd.DataFrame], **kwargs) -> shap.Explanation:
        """
        è®¡ç®—æ•´ä¸ªæ•°æ®é›†çš„SHAPå€¼

        Args:
            data: è¦è§£é‡Šçš„æ•°æ®é›† (numpy array æˆ– Pandas DataFrame)
             **kwargs: ä¼ é€’ç»™SHAP explainerçš„é¢å¤–å‚æ•°

        Returns:
            SHAPçš„Explanationå¯¹è±¡æˆ–SHAPå€¼æ•°ç»„
        """
        shap_values = self.explainer(data, **kwargs)
        return shap_values

def load_sample_data(n_samples=1000, n_features=10, n_classes=2) -> Tuple[np.ndarray, np.ndarray, List[str], List[str]]:
    """åŠ è½½æˆ–ç”Ÿæˆä¸€ä¸ªç®€å•çš„æ ·æœ¬æ•°æ®é›†ç”¨äºæ¼”ç¤º"""
    from sklearn.datasets import make_classification
    X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_features//2, 
                               n_classes=n_classes, random_state=42)
    feature_names = [f"feature_{i}" for i in range(n_features)]
    class_names = [f"class_{i}" for i in range(n_classes)]
    return X, y, feature_names, class_names

def demo_explainable_ai():
    """
    æ¼”ç¤ºå¯è§£é‡Šæ€§AIæ¨¡å—çš„ä½¿ç”¨
    """
    print("ğŸš€ å¯è§£é‡Šæ€§AIæ¨¡å—æ¼”ç¤º")
    print("="*50)

    # 1. åŠ è½½æ•°æ®å’Œè®­ç»ƒä¸€ä¸ªç®€å•æ¨¡å‹
    print("\n[é˜¶æ®µ1: åŠ è½½æ•°æ®å’Œè®­ç»ƒæ¨¡å‹]")
    X, y, feature_names, class_names = load_sample_data(n_samples=200) # ä½¿ç”¨è¾ƒå°æ•°æ®é›†ä»¥åŠ é€ŸSHAP
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # è®­ç»ƒä¸€ä¸ªéšæœºæ£®æ—æ¨¡å‹
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)
    accuracy = accuracy_score(y_test, model.predict(X_test))
    print(f"éšæœºæ£®æ—æ¨¡å‹è®­ç»ƒå®Œæˆã€‚æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f}")

    # 2. LIMEè§£é‡Š
    print("\n[é˜¶æ®µ2: ä½¿ç”¨LIMEè¿›è¡Œè§£é‡Š]")
    try:
        lime_explainer = LimeExplainer(
            model=model,
            training_data=X_train,
            feature_names=feature_names,
            class_names=class_names,
            mode="classification"
        )
        
        # è§£é‡Šæµ‹è¯•é›†ä¸­çš„ä¸€ä¸ªæ ·æœ¬
        instance_to_explain_lime = X_test[0]
        lime_explanation = lime_explainer.explain_instance(instance_to_explain_lime, num_features=5)
        
        print(f"\nLIMEè§£é‡Šæ ·æœ¬0 (çœŸå®æ ‡ç­¾: {class_names[y_test[0]]}):")
        # LIMEçš„è§£é‡Šå¯ä»¥ç›´æ¥æ‰“å°æˆ–ä¿å­˜ä¸ºHTML
        # lime_explanation.show_in_notebook(show_table=True, show_all=False) # åœ¨Jupyter Notebookä¸­æ˜¾ç¤º
        # lime_explanation.save_to_file('lime_report.html') # ä¿å­˜ä¸ºHTML
        print("LIMEè§£é‡Šç‰¹å¾æƒé‡ (å¯¹äºé¢„æµ‹æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«):")
        for feature, weight in lime_explanation.as_list():
            print(f"  ç‰¹å¾: {feature}, æƒé‡: {weight:.4f}")

    except Exception as e:
        print(f"âŒ LIMEè§£é‡Šå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    # 3. SHAPè§£é‡Š
    print("\n[é˜¶æ®µ3: ä½¿ç”¨SHAPè¿›è¡Œè§£é‡Š]")
    try:
        # å¯¹äºSHAPï¼Œé€šå¸¸ä¼ é€’DataFrameæ›´å®¹æ˜“å¤„ç†ç‰¹å¾åç§°
        X_train_df = pd.DataFrame(X_train, columns=feature_names)
        X_test_df = pd.DataFrame(X_test, columns=feature_names)

        shap_explainer = ShapExplainer(
            model=model,
            data=X_train_df # èƒŒæ™¯æ•°æ®
        )
        
        # è§£é‡Šæµ‹è¯•é›†ä¸­çš„ä¸€ä¸ªæ ·æœ¬
        instance_to_explain_shap = X_test_df.iloc[[0]] # SHAPé€šå¸¸æœŸæœ›DataFrame
        shap_explanation_instance = shap_explainer.explain_instance(instance_to_explain_shap)
        
        print(f"\nSHAPè§£é‡Šæ ·æœ¬0 (çœŸå®æ ‡ç­¾: {class_names[y_test[0]]}):")
        print(f"  åŸºç¡€å€¼ (Expected Value): {shap_explanation_instance.base_values[0]}") # å¯¹äºå¤šè¾“å‡ºæ¨¡å‹ï¼Œå¯èƒ½æ˜¯æ•°ç»„
        print(f"  SHAPå€¼ (å¯¹äºé¢„æµ‹æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«):")
        # shap_explanation_instance.values æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå¯¹äºäºŒåˆ†ç±»ï¼Œé€šå¸¸æ˜¯ [shap_values_for_class_0, shap_values_for_class_1]
        # æˆ‘ä»¬è¿™é‡Œå‡è®¾å…³æ³¨class 1çš„SHAPå€¼ (å¦‚æœæ¨¡å‹è¾“å‡ºæ¦‚ç‡çš„è¯)
        # å¯¹äºéšæœºæ£®æ—ï¼Œshap.Explainer(model, data) è¿”å›çš„æ˜¯é’ˆå¯¹æ¯ä¸ªç±»åˆ«çš„SHAPå€¼
        # shap_values_instance.valuesçš„å½¢çŠ¶å¯èƒ½æ˜¯ (num_instances, num_features, num_classes) æˆ– (num_instances, num_features)
        
        shap_values_for_instance = shap_explanation_instance.values
        if shap_values_for_instance.ndim == 3: # (instances, features, classes)
            # é€šå¸¸å…³æ³¨æ­£ç±»ï¼ˆå¦‚ç±»åˆ«1ï¼‰çš„SHAPå€¼
            # å¦‚æœæ¨¡å‹æ˜¯äºŒåˆ†ç±»ï¼Œå¹¶ä¸”è¾“å‡ºä¸¤ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œshap_values_for_instance.values[0, :, 1] æ˜¯æ ·æœ¬å¯¹ç±»åˆ«1çš„SHAPå€¼
            # å¦‚æœæ¨¡å‹ç›´æ¥è¾“å‡ºç±»åˆ«1çš„æ¦‚ç‡ï¼ˆæˆ–logitï¼‰ï¼Œåˆ™æ˜¯ shap_values_for_instance.values[0, :]
            # å¯¹äº RandomForestClassifierï¼Œshap.Explainer è¿”å›çš„æ˜¯é’ˆå¯¹æ¯ä¸ªç±»è¾“å‡ºçš„ SHAP å€¼
            # è¿™é‡Œæˆ‘ä»¬æ‰“å°ç±»åˆ«1çš„SHAPå€¼
            class_index_to_explain = 1 
            if shap_values_for_instance.shape[2] > class_index_to_explain:
                for feature_idx, feature_name in enumerate(feature_names):
                    print(f"  ç‰¹å¾: {feature_name}, SHAPå€¼ (å¯¹ç±»åˆ« {class_names[class_index_to_explain]}): {shap_values_for_instance[0, feature_idx, class_index_to_explain]:.4f}")
            else:
                print("SHAPå€¼è¾“å‡ºçš„ç±»åˆ«ç»´åº¦ä¸è¶³ã€‚")

        elif shap_values_for_instance.ndim == 2: # (instances, features) - å¯èƒ½é’ˆå¯¹å•ä¸€è¾“å‡ºæˆ–ç‰¹å®šç±»åˆ«
             for feature_idx, feature_name in enumerate(feature_names):
                print(f"  ç‰¹å¾: {feature_name}, SHAPå€¼: {shap_values_for_instance[0, feature_idx]:.4f}")
        else:
            print("SHAPå€¼æ•°ç»„çš„ç»´åº¦ä¸ç¬¦åˆé¢„æœŸã€‚")


        # SHAPå›¾ (å¦‚æœç¯å¢ƒæ”¯æŒmatplotlib)
        try:
            print("\nå°è¯•ç”ŸæˆSHAPæ‘˜è¦å›¾ (å¯èƒ½éœ€è¦matplotlib)...")
            # è®¡ç®—æ•´ä¸ªæµ‹è¯•é›†çš„SHAPå€¼
            shap_values_test = shap_explainer.explain_model(X_test_df)
            # shap.summary_plot(shap_values_test.values[:,:,1], X_test_df, show=False) # å¯¹äºå¤šåˆ†ç±»è¾“å‡º
            # å¦‚æœæ˜¯å•ä¸€è¾“å‡ºæˆ–åªæƒ³è§£é‡Šä¸€ä¸ªç±»åˆ«çš„shapå€¼
            # plt.title("SHAP Summary Plot for Class 1")
            # plt.savefig("shap_summary_plot.png")
            # plt.close()
            # print("SHAPæ‘˜è¦å›¾å·²å°è¯•ä¿å­˜ä¸º shap_summary_plot.png (å¦‚æœmatplotlibå¯ç”¨ä¸”é…ç½®æ­£ç¡®)")
            
            # SHAPåŠ›å›¾ (force plot) for the first instance
            # shap.force_plot(shap_explainer.explainer.expected_value[1], shap_values_test.values[0,:,1], X_test_df.iloc[0,:], matplotlib=True, show=False)
            # plt.title("SHAP Force Plot for Instance 0, Class 1")
            # plt.savefig("shap_force_plot_instance0.png")
            # plt.close()
            # print("SHAPåŠ›å›¾å·²å°è¯•ä¿å­˜ä¸º shap_force_plot_instance0.png")
            print("ç”±äºç¯å¢ƒé™åˆ¶ï¼ŒSHAPç»˜å›¾éƒ¨åˆ†å·²æ³¨é‡Šæ‰ã€‚åœ¨æœ¬åœ°è¿è¡Œæ—¶å¯ä»¥å–æ¶ˆæ³¨é‡Šä»¥æŸ¥çœ‹å›¾å½¢ã€‚")

        except Exception as plot_e:
            print(f"âŒ ç”ŸæˆSHAPå›¾å¤±è´¥: {plot_e}")

    except Exception as e:
        print(f"âŒ SHAPè§£é‡Šå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n[é˜¶æ®µ4: æ¼”ç¤ºå®Œæˆ]")
    print("âœ… å¯è§£é‡Šæ€§AIæ¨¡å—æ¼”ç¤ºå®Œæˆã€‚")

if __name__ == "__main__":
    demo_explainable_ai()