#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# models/graph_neural_networks/basic_gnn_layers.py

"""
åŸºç¡€å›¾ç¥ç»ç½‘ç»œå±‚å®ç°
åŒ…å«GCNã€GATã€GraphSAGEã€GINç­‰ç»å…¸GNNæ¶æ„
æ”¯æŒå¤šæ¨¡æ€ç‰¹å¾å’ŒMR2æ•°æ®é›†
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv, global_mean_pool, global_max_pool
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops, degree
import torch_geometric
from typing import Dict, List, Tuple, Any, Optional, Union
import numpy as np
from pathlib import Path
import sys
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from utils.config_manager import get_config_manager, get_output_path
    USE_PROJECT_MODULES = True
    print("âœ… æˆåŠŸå¯¼å…¥é¡¹ç›®æ¨¡å—")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥é¡¹ç›®æ¨¡å—å¤±è´¥: {e}")
    USE_PROJECT_MODULES = False

import logging
logger = logging.getLogger(__name__)


class GCNLayer(nn.Module):
    """å›¾å·ç§¯ç½‘ç»œå±‚"""
    
    def __init__(self, input_dim: int, output_dim: int, 
                 dropout: float = 0.5, bias: bool = True):
        """
        åˆå§‹åŒ–GCNå±‚
        
        Args:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            output_dim: è¾“å‡ºç‰¹å¾ç»´åº¦
            dropout: Dropoutæ¦‚ç‡
            bias: æ˜¯å¦ä½¿ç”¨åç½®
        """
        super(GCNLayer, self).__init__()
        
        self.conv = GCNConv(input_dim, output_dim, bias=bias)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.ReLU()
        
    def forward(self, x, edge_index, edge_weight=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, input_dim]
            edge_index: è¾¹ç´¢å¼• [2, num_edges]
            edge_weight: è¾¹æƒé‡ [num_edges] (å¯é€‰)
            
        Returns:
            è¾“å‡ºç‰¹å¾ [num_nodes, output_dim]
        """
        x = self.conv(x, edge_index, edge_weight)
        x = self.activation(x)
        x = self.dropout(x)
        return x


class GATLayer(nn.Module):
    """å›¾æ³¨æ„åŠ›ç½‘ç»œå±‚"""
    
    def __init__(self, input_dim: int, output_dim: int, 
                 heads: int = 8, dropout: float = 0.5, 
                 concat: bool = True, bias: bool = True):
        """
        åˆå§‹åŒ–GATå±‚
        
        Args:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            output_dim: è¾“å‡ºç‰¹å¾ç»´åº¦
            heads: æ³¨æ„åŠ›å¤´æ•°
            dropout: Dropoutæ¦‚ç‡
            concat: æ˜¯å¦æ‹¼æ¥å¤šå¤´ç»“æœ
            bias: æ˜¯å¦ä½¿ç”¨åç½®
        """
        super(GATLayer, self).__init__()
        
        self.heads = heads
        self.concat = concat
        
        if concat:
            assert output_dim % heads == 0
            self.head_dim = output_dim // heads
        else:
            self.head_dim = output_dim
            
        self.conv = GATConv(
            input_dim, self.head_dim, heads=heads,
            dropout=dropout, concat=concat, bias=bias
        )
        self.dropout = nn.Dropout(dropout)
        
        if not concat:
            self.activation = nn.ELU()
        else:
            self.activation = nn.ReLU()
    
    def forward(self, x, edge_index, return_attention_weights=False):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, input_dim]
            edge_index: è¾¹ç´¢å¼• [2, num_edges]
            return_attention_weights: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
            
        Returns:
            è¾“å‡ºç‰¹å¾å’Œå¯é€‰çš„æ³¨æ„åŠ›æƒé‡
        """
        if return_attention_weights:
            x, attention_weights = self.conv(x, edge_index, return_attention_weights=True)
            x = self.activation(x)
            x = self.dropout(x)
            return x, attention_weights
        else:
            x = self.conv(x, edge_index)
            x = self.activation(x)
            x = self.dropout(x)
            return x


class GraphSAGELayer(nn.Module):
    """GraphSAGEå±‚"""
    
    def __init__(self, input_dim: int, output_dim: int, 
                 aggr: str = 'mean', dropout: float = 0.5, 
                 bias: bool = True, normalize: bool = True):
        """
        åˆå§‹åŒ–GraphSAGEå±‚
        
        Args:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            output_dim: è¾“å‡ºç‰¹å¾ç»´åº¦
            aggr: èšåˆæ–¹æ³• ('mean', 'max', 'add')
            dropout: Dropoutæ¦‚ç‡
            bias: æ˜¯å¦ä½¿ç”¨åç½®
            normalize: æ˜¯å¦L2æ ‡å‡†åŒ–
        """
        super(GraphSAGELayer, self).__init__()
        
        self.conv = SAGEConv(
            input_dim, output_dim, aggr=aggr, 
            bias=bias, normalize=normalize
        )
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.ReLU()
        
    def forward(self, x, edge_index):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, input_dim]
            edge_index: è¾¹ç´¢å¼• [2, num_edges]
            
        Returns:
            è¾“å‡ºç‰¹å¾ [num_nodes, output_dim]
        """
        x = self.conv(x, edge_index)
        x = self.activation(x)
        x = self.dropout(x)
        return x


class GINLayer(nn.Module):
    """å›¾åŒæ„ç½‘ç»œå±‚"""
    
    def __init__(self, input_dim: int, output_dim: int, 
                 hidden_dim: Optional[int] = None, dropout: float = 0.5,
                 eps: float = 0.0, train_eps: bool = False):
        """
        åˆå§‹åŒ–GINå±‚
        
        Args:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            output_dim: è¾“å‡ºç‰¹å¾ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦ï¼ˆé»˜è®¤ä¸output_dimç›¸åŒï¼‰
            dropout: Dropoutæ¦‚ç‡
            eps: epsilonå‚æ•°
            train_eps: æ˜¯å¦è®­ç»ƒepsilon
        """
        super(GINLayer, self).__init__()
        
        if hidden_dim is None:
            hidden_dim = output_dim
        
        # æ„å»ºMLP
        mlp = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, output_dim)
        )
        
        self.conv = GINConv(mlp, eps=eps, train_eps=train_eps)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.ReLU()
        
    def forward(self, x, edge_index):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, input_dim]
            edge_index: è¾¹ç´¢å¼• [2, num_edges]
            
        Returns:
            è¾“å‡ºç‰¹å¾ [num_nodes, output_dim]
        """
        x = self.conv(x, edge_index)
        x = self.activation(x)
        x = self.dropout(x)
        return x


class BasicGNN(nn.Module):
    """åŸºç¡€GNNæ¨¡å‹ï¼Œæ”¯æŒå¤šç§GNNæ¶æ„"""
    
    def __init__(self, input_dim: int, hidden_dims: List[int], 
                 output_dim: int, gnn_type: str = 'gcn',
                 dropout: float = 0.5, **kwargs):
        """
        åˆå§‹åŒ–åŸºç¡€GNNæ¨¡å‹
        
        Args:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨
            output_dim: è¾“å‡ºç‰¹å¾ç»´åº¦
            gnn_type: GNNç±»å‹ ('gcn', 'gat', 'graphsage', 'gin')
            dropout: Dropoutæ¦‚ç‡
            **kwargs: å…¶ä»–å‚æ•°
        """
        super(BasicGNN, self).__init__()
        
        self.gnn_type = gnn_type.lower()
        self.num_layers = len(hidden_dims)
        
        # æ„å»ºGNNå±‚
        self.gnn_layers = nn.ModuleList()
        
        # ç¬¬ä¸€å±‚
        first_layer = self._create_gnn_layer(
            input_dim, hidden_dims[0], dropout, **kwargs
        )
        self.gnn_layers.append(first_layer)
        
        # ä¸­é—´å±‚
        for i in range(1, self.num_layers):
            layer = self._create_gnn_layer(
                hidden_dims[i-1], hidden_dims[i], dropout, **kwargs
            )
            self.gnn_layers.append(layer)
        
        # è¾“å‡ºå±‚
        if output_dim > 0:
            self.output_layer = nn.Linear(hidden_dims[-1], output_dim)
        else:
            self.output_layer = None
        
        # Batch Normalization (å¯é€‰)
        self.use_bn = kwargs.get('use_bn', False)
        if self.use_bn:
            self.batch_norms = nn.ModuleList([
                nn.BatchNorm1d(dim) for dim in hidden_dims
            ])
        
        print(f"ğŸ”— åˆ›å»º {gnn_type.upper()} æ¨¡å‹:")
        print(f"   è¾“å…¥ç»´åº¦: {input_dim}")
        print(f"   éšè—ç»´åº¦: {hidden_dims}")
        print(f"   è¾“å‡ºç»´åº¦: {output_dim}")
        print(f"   å±‚æ•°: {self.num_layers + (1 if output_dim > 0 else 0)}")
    
    def _create_gnn_layer(self, input_dim: int, output_dim: int, 
                         dropout: float, **kwargs):
        """åˆ›å»ºGNNå±‚"""
        if self.gnn_type == 'gcn':
            return GCNLayer(input_dim, output_dim, dropout)
        elif self.gnn_type == 'gat':
            heads = kwargs.get('heads', 8)
            concat = kwargs.get('concat', True)
            return GATLayer(input_dim, output_dim, heads, dropout, concat)
        elif self.gnn_type == 'graphsage':
            aggr = kwargs.get('aggr', 'mean')
            normalize = kwargs.get('normalize', True)
            return GraphSAGELayer(input_dim, output_dim, aggr, dropout, normalize=normalize)
        elif self.gnn_type == 'gin':
            hidden_dim = kwargs.get('gin_hidden_dim', output_dim)
            eps = kwargs.get('eps', 0.0)
            return GINLayer(input_dim, output_dim, hidden_dim, dropout, eps)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„GNNç±»å‹: {self.gnn_type}")
    
    def forward(self, x, edge_index, batch=None, return_embeddings=False):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, input_dim]
            edge_index: è¾¹ç´¢å¼• [2, num_edges]
            batch: æ‰¹æ¬¡ç´¢å¼• [num_nodes] (å›¾çº§ä»»åŠ¡éœ€è¦)
            return_embeddings: æ˜¯å¦è¿”å›èŠ‚ç‚¹åµŒå…¥
            
        Returns:
            è¾“å‡ºç‰¹å¾æˆ–å›¾çº§è¡¨ç¤º
        """
        # é€šè¿‡GNNå±‚
        for i, gnn_layer in enumerate(self.gnn_layers):
            if self.gnn_type == 'gat' and i == 0:
                # GATå¯èƒ½è¿”å›æ³¨æ„åŠ›æƒé‡
                if hasattr(gnn_layer, 'return_attention_weights'):
                    x = gnn_layer(x, edge_index)
                else:
                    x = gnn_layer(x, edge_index)
            else:
                x = gnn_layer(x, edge_index)
            
            # Batch Normalization
            if self.use_bn and i < len(self.batch_norms):
                x = self.batch_norms[i](x)
        
        # ä¿å­˜èŠ‚ç‚¹åµŒå…¥
        node_embeddings = x.clone() if return_embeddings else None
        
        # å›¾çº§æ± åŒ–ï¼ˆå¦‚æœæœ‰batchå‚æ•°ï¼‰
        if batch is not None:
            # å›¾çº§ä»»åŠ¡ï¼šæ± åŒ–å¾—åˆ°å›¾è¡¨ç¤º
            x = global_mean_pool(x, batch)  # ä¹Ÿå¯ä»¥ä½¿ç”¨global_max_pool
        
        # è¾“å‡ºå±‚
        if self.output_layer is not None:
            x = self.output_layer(x)
        
        if return_embeddings:
            return x, node_embeddings
        else:
            return x
    
    def get_embeddings(self, x, edge_index, batch=None):
        """è·å–èŠ‚ç‚¹åµŒå…¥"""
        with torch.no_grad():
            _, embeddings = self.forward(x, edge_index, batch, return_embeddings=True)
            return embeddings


class GNNClassifier(nn.Module):
    """GNNåˆ†ç±»å™¨ï¼Œä¸“é—¨ç”¨äºè°£è¨€æ£€æµ‹ä»»åŠ¡"""
    
    def __init__(self, input_dim: int, hidden_dims: List[int] = [128, 64],
                 num_classes: int = 3, gnn_type: str = 'gat',
                 dropout: float = 0.5, use_residual: bool = True, **kwargs):
        """
        åˆå§‹åŒ–GNNåˆ†ç±»å™¨
        
        Args:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨
            num_classes: åˆ†ç±»ç±»åˆ«æ•°
            gnn_type: GNNç±»å‹
            dropout: Dropoutæ¦‚ç‡
            use_residual: æ˜¯å¦ä½¿ç”¨æ®‹å·®è¿æ¥
            **kwargs: å…¶ä»–å‚æ•°
        """
        super(GNNClassifier, self).__init__()
        
        self.use_residual = use_residual
        self.num_classes = num_classes
        
        # GNNä¸»ä½“
        self.gnn = BasicGNN(
            input_dim=input_dim,
            hidden_dims=hidden_dims,
            output_dim=0,  # ä¸éœ€è¦é¢å¤–çš„è¾“å‡ºå±‚
            gnn_type=gnn_type,
            dropout=dropout,
            **kwargs
        )
        
        # åˆ†ç±»å¤´
        classifier_input_dim = hidden_dims[-1]
        
        # å¯é€‰çš„ç‰¹å¾èåˆå±‚
        self.feature_fusion = kwargs.get('feature_fusion', False)
        if self.feature_fusion:
            self.fusion_layer = nn.Linear(classifier_input_dim * 2, classifier_input_dim)
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(classifier_input_dim, classifier_input_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout / 2),
            nn.Linear(classifier_input_dim // 2, num_classes)
        )
        
        # æ®‹å·®è¿æ¥ï¼ˆå¦‚æœè¾“å…¥è¾“å‡ºç»´åº¦åŒ¹é…ï¼‰
        if use_residual and input_dim == hidden_dims[-1]:
            self.residual_projection = None
        elif use_residual:
            self.residual_projection = nn.Linear(input_dim, hidden_dims[-1])
        else:
            self.residual_projection = None
        
        print(f"ğŸ¯ åˆ›å»ºGNNåˆ†ç±»å™¨:")
        print(f"   GNNç±»å‹: {gnn_type.upper()}")
        print(f"   è¾“å…¥ç»´åº¦: {input_dim}")
        print(f"   éšè—ç»´åº¦: {hidden_dims}")
        print(f"   åˆ†ç±»æ•°: {num_classes}")
        print(f"   æ®‹å·®è¿æ¥: {use_residual}")
    
    def forward(self, x, edge_index, batch=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, input_dim]
            edge_index: è¾¹ç´¢å¼• [2, num_edges]
            batch: æ‰¹æ¬¡ç´¢å¼• [num_nodes] (å›¾çº§ä»»åŠ¡éœ€è¦)
            
        Returns:
            åˆ†ç±»logits [batch_size, num_classes] æˆ– [num_nodes, num_classes]
        """
        # ä¿å­˜è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥
        residual = x
        
        # é€šè¿‡GNN
        x = self.gnn(x, edge_index, batch)
        
        # æ®‹å·®è¿æ¥
        if self.use_residual:
            if self.residual_projection is not None:
                residual = self.residual_projection(residual)
            
            # å¦‚æœæ˜¯å›¾çº§ä»»åŠ¡ï¼Œéœ€è¦å¯¹residualè¿›è¡Œæ± åŒ–
            if batch is not None:
                residual = global_mean_pool(residual, batch)
            
            x = x + residual
        
        # åˆ†ç±»
        logits = self.classifier(x)
        
        return logits
    
    def predict(self, x, edge_index, batch=None):
        """é¢„æµ‹"""
        with torch.no_grad():
            logits = self.forward(x, edge_index, batch)
            probabilities = F.softmax(logits, dim=-1)
            predictions = torch.argmax(logits, dim=-1)
            return predictions, probabilities
    
    def get_parameter_count(self):
        """è·å–æ¨¡å‹å‚æ•°æ•°é‡"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


class GraphPooling(nn.Module):
    """å›¾æ± åŒ–å±‚"""
    
    def __init__(self, pooling_type: str = 'mean'):
        """
        åˆå§‹åŒ–å›¾æ± åŒ–å±‚
        
        Args:
            pooling_type: æ± åŒ–ç±»å‹ ('mean', 'max', 'add', 'attention')
        """
        super(GraphPooling, self).__init__()
        self.pooling_type = pooling_type
        
        if pooling_type == 'attention':
            # æ³¨æ„åŠ›æ± åŒ–éœ€è¦é¢å¤–çš„å‚æ•°
            self.attention_weights = None
    
    def forward(self, x, batch):
        """
        å›¾æ± åŒ–
        
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, feature_dim]
            batch: æ‰¹æ¬¡ç´¢å¼• [num_nodes]
            
        Returns:
            å›¾çº§ç‰¹å¾ [batch_size, feature_dim]
        """
        if self.pooling_type == 'mean':
            return global_mean_pool(x, batch)
        elif self.pooling_type == 'max':
            return global_max_pool(x, batch)
        elif self.pooling_type == 'add':
            return torch_geometric.nn.global_add_pool(x, batch)
        elif self.pooling_type == 'attention':
            # ç®€å•çš„æ³¨æ„åŠ›æ± åŒ–å®ç°
            if self.attention_weights is None:
                self.attention_weights = nn.Linear(x.size(-1), 1)
            
            attention_scores = self.attention_weights(x)
            attention_weights = F.softmax(attention_scores, dim=0)
            
            # æŒ‰batchåŠ æƒæ±‚å’Œ
            graph_embeddings = []
            for i in range(batch.max().item() + 1):
                mask = batch == i
                if mask.sum() > 0:
                    graph_x = x[mask]
                    graph_weights = attention_weights[mask]
                    graph_emb = (graph_x * graph_weights).sum(dim=0)
                    graph_embeddings.append(graph_emb)
            
            return torch.stack(graph_embeddings)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ± åŒ–ç±»å‹: {self.pooling_type}")


def create_gnn_model(model_config: Dict[str, Any]) -> nn.Module:
    """
    æ ¹æ®é…ç½®åˆ›å»ºGNNæ¨¡å‹
    
    Args:
        model_config: æ¨¡å‹é…ç½®å­—å…¸
        
    Returns:
        GNNæ¨¡å‹å®ä¾‹
    """
    model_type = model_config.get('model_type', 'classifier')
    gnn_type = model_config.get('gnn_type', 'gat')
    
    if model_type == 'classifier':
        model = GNNClassifier(
            input_dim=model_config.get('input_dim', 768),
            hidden_dims=model_config.get('hidden_dims', [128, 64]),
            num_classes=model_config.get('num_classes', 3),
            gnn_type=gnn_type,
            dropout=model_config.get('dropout', 0.5),
            use_residual=model_config.get('use_residual', True),
            **model_config.get('gnn_params', {})
        )
    elif model_type == 'basic':
        model = BasicGNN(
            input_dim=model_config.get('input_dim', 768),
            hidden_dims=model_config.get('hidden_dims', [128, 64]),
            output_dim=model_config.get('output_dim', 3),
            gnn_type=gnn_type,
            dropout=model_config.get('dropout', 0.5),
            **model_config.get('gnn_params', {})
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
    
    return model


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ”— æµ‹è¯•åŸºç¡€å›¾ç¥ç»ç½‘ç»œå±‚")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    num_nodes = 100
    num_edges = 200
    input_dim = 768
    num_classes = 3
    
    # èŠ‚ç‚¹ç‰¹å¾
    x = torch.randn(num_nodes, input_dim)
    
    # è¾¹ç´¢å¼•
    edge_index = torch.randint(0, num_nodes, (2, num_edges))
    
    # æ‰¹æ¬¡ç´¢å¼•ï¼ˆå›¾çº§ä»»åŠ¡ï¼‰
    batch = torch.zeros(num_nodes, dtype=torch.long)  # å•å›¾
    
    print(f"ğŸ“Š æµ‹è¯•æ•°æ®:")
    print(f"   èŠ‚ç‚¹æ•°: {num_nodes}")
    print(f"   è¾¹æ•°: {num_edges}")
    print(f"   ç‰¹å¾ç»´åº¦: {input_dim}")
    print(f"   èŠ‚ç‚¹ç‰¹å¾å½¢çŠ¶: {x.shape}")
    print(f"   è¾¹ç´¢å¼•å½¢çŠ¶: {edge_index.shape}")
    
    # æµ‹è¯•å„ç§GNNå±‚
    gnn_types = ['gcn', 'gat', 'graphsage', 'gin']
    
    for gnn_type in gnn_types:
        print(f"\nğŸ§ª æµ‹è¯• {gnn_type.upper()} æ¨¡å‹:")
        
        try:
            # åˆ›å»ºåˆ†ç±»æ¨¡å‹
            model = GNNClassifier(
                input_dim=input_dim,
                hidden_dims=[128, 64],
                num_classes=num_classes,
                gnn_type=gnn_type,
                dropout=0.5
            )
            
            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                logits = model(x, edge_index, batch)
                predictions, probabilities = model.predict(x, edge_index, batch)
                
                print(f"   âœ… è¾“å‡ºå½¢çŠ¶: {logits.shape}")
                print(f"   âœ… é¢„æµ‹å½¢çŠ¶: {predictions.shape}")
                print(f"   âœ… å‚æ•°æ•°é‡: {model.get_parameter_count():,}")
                
                # èŠ‚ç‚¹çº§ä»»åŠ¡æµ‹è¯•
                node_logits = model(x, edge_index)  # ä¸æä¾›batch
                print(f"   âœ… èŠ‚ç‚¹çº§è¾“å‡ºå½¢çŠ¶: {node_logits.shape}")
                
        except Exception as e:
            print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•æ¨¡å‹é…ç½®åˆ›å»º
    print(f"\nğŸ”§ æµ‹è¯•æ¨¡å‹é…ç½®åˆ›å»º:")
    model_config = {
        'model_type': 'classifier',
        'gnn_type': 'gat',
        'input_dim': input_dim,
        'hidden_dims': [256, 128, 64],
        'num_classes': num_classes,
        'dropout': 0.3,
        'use_residual': True,
        'gnn_params': {
            'heads': 4,
            'concat': True,
            'use_bn': True
        }
    }
    
    try:
        model = create_gnn_model(model_config)
        print(f"   âœ… é…ç½®æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"   âœ… å‚æ•°æ•°é‡: {model.get_parameter_count():,}")
    except Exception as e:
        print(f"   âŒ é…ç½®æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
    
    print("\nâœ… åŸºç¡€å›¾ç¥ç»ç½‘ç»œå±‚æµ‹è¯•å®Œæˆ")