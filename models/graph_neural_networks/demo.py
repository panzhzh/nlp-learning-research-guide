#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# models/graph_neural_networks/demo.py

"""
å›¾ç¥ç»ç½‘ç»œæ¨¡å—æ¼”ç¤º
å±•ç¤ºåŸºç¡€GNNã€ç¤¾äº¤å›¾æ„å»ºã€å¤šæ¨¡æ€GNNçš„å®Œæ•´æµç¨‹
ä¸“é—¨ä¸ºMR2æ•°æ®é›†å’Œè°£è¨€æ£€æµ‹ä»»åŠ¡è®¾è®¡
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import sys
import time
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å¯¼å…¥æ¨¡å—
try:
    from basic_gnn_layers import BasicGNN, GNNClassifier, create_gnn_model
    from social_graph_builder import SocialGraphBuilder, GraphFeatureExtractor, create_multi_layer_graph
    from multimodal_gnn import MultimodalGNN, MultimodalGraphClassifier, create_multimodal_gnn_config
    LOCAL_IMPORT = True
    print("âœ… æˆåŠŸå¯¼å…¥æœ¬åœ°æ¨¡å—")
    
    # å°è¯•å¯¼å…¥é¡¹ç›®é…ç½®
    try:
        from utils.config_manager import get_output_path
        USE_PROJECT_CONFIG = True
        print("âœ… æˆåŠŸå¯¼å…¥é¡¹ç›®é…ç½®")
    except ImportError:
        USE_PROJECT_CONFIG = False
        print("âš ï¸  é¡¹ç›®é…ç½®ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„")
        
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    LOCAL_IMPORT = False
    USE_PROJECT_CONFIG = False
    sys.exit(1)

import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GNNDemoRunner:
    """å›¾ç¥ç»ç½‘ç»œæ¼”ç¤ºè¿è¡Œå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¼”ç¤ºè¿è¡Œå™¨"""
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.results = {}
        
        # è¾“å‡ºç›®å½•
        if USE_PROJECT_CONFIG:
            try:
                self.output_dir = get_output_path('graphs', 'demo_results')
            except:
                self.output_dir = Path('outputs/graphs/demo_results')
                self.output_dir.mkdir(parents=True, exist_ok=True)
        else:
            self.output_dir = Path('outputs/graphs/demo_results')
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ¯ å›¾ç¥ç»ç½‘ç»œæ¼”ç¤ºåˆå§‹åŒ–")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def demo_basic_gnn(self):
        """æ¼”ç¤ºåŸºç¡€GNNåŠŸèƒ½"""
        print(f"\n{'='*70}")
        print("ğŸ“Š åŸºç¡€å›¾ç¥ç»ç½‘ç»œæ¼”ç¤º")
        print(f"{'='*70}")
        
        try:
            # åˆ›å»ºç¤ºä¾‹å›¾æ•°æ®
            num_nodes = 100
            num_edges = 300
            input_dim = 64
            num_classes = 3
            
            print(f"ğŸ”§ åˆ›å»ºç¤ºä¾‹å›¾æ•°æ®:")
            print(f"   èŠ‚ç‚¹æ•°: {num_nodes}")
            print(f"   è¾¹æ•°: {num_edges}")
            print(f"   ç‰¹å¾ç»´åº¦: {input_dim}")
            
            # èŠ‚ç‚¹ç‰¹å¾ï¼ˆéšæœºï¼‰
            x = torch.randn(num_nodes, input_dim).to(self.device)
            
            # è¾¹ç´¢å¼•ï¼ˆåˆ›å»ºä¸€ä¸ªè¿é€šçš„éšæœºå›¾ï¼‰
            edge_list = []
            
            # é¦–å…ˆåˆ›å»ºä¸€ä¸ªé“¾ç¡®ä¿è¿é€šæ€§
            for i in range(num_nodes - 1):
                edge_list.append([i, i + 1])
                edge_list.append([i + 1, i])  # æ— å‘å›¾
            
            # æ·»åŠ éšæœºè¾¹
            remaining_edges = num_edges - (num_nodes - 1) * 2
            for _ in range(remaining_edges):
                src = np.random.randint(0, num_nodes)
                dst = np.random.randint(0, num_nodes)
                if src != dst:
                    edge_list.append([src, dst])
            
            edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous().to(self.device)
            
            # æ ‡ç­¾ï¼ˆéšæœºï¼‰
            labels = torch.randint(0, num_classes, (num_nodes,)).to(self.device)
            
            print(f"âœ… å›¾æ•°æ®åˆ›å»ºå®Œæˆ: {edge_index.size(1)} æ¡è¾¹")
            
            # æµ‹è¯•ä¸åŒç±»å‹çš„GNN
            gnn_types = ['gcn', 'gat', 'graphsage', 'gin']
            gnn_results = {}
            
            for gnn_type in gnn_types:
                print(f"\nğŸ§ª æµ‹è¯• {gnn_type.upper()} æ¨¡å‹:")
                
                start_time = time.time()
                
                # åˆ›å»ºæ¨¡å‹
                model = GNNClassifier(
                    input_dim=input_dim,
                    hidden_dims=[128, 64],
                    num_classes=num_classes,
                    gnn_type=gnn_type,
                    dropout=0.5
                ).to(self.device)
                
                # å‰å‘ä¼ æ’­æµ‹è¯•
                model.eval()
                with torch.no_grad():
                    # èŠ‚ç‚¹çº§é¢„æµ‹
                    node_logits = model(x, edge_index)
                    node_preds, node_probs = model.predict(x, edge_index)
                    
                    # å›¾çº§é¢„æµ‹ï¼ˆä½¿ç”¨batchï¼‰
                    batch = torch.zeros(num_nodes, dtype=torch.long, device=self.device)
                    graph_logits = model(x, edge_index, batch)
                    graph_preds, graph_probs = model.predict(x, edge_index, batch)
                
                end_time = time.time()
                inference_time = end_time - start_time
                
                # ç»Ÿè®¡ç»“æœ
                param_count = model.get_parameter_count()
                
                result = {
                    'model_type': gnn_type,
                    'parameters': param_count,
                    'inference_time': inference_time,
                    'node_output_shape': node_logits.shape,
                    'graph_output_shape': graph_logits.shape,
                    'node_predictions': node_preds[:10].cpu().numpy(),  # å‰10ä¸ªèŠ‚ç‚¹çš„é¢„æµ‹
                    'graph_prediction': graph_preds.cpu().numpy()
                }
                
                gnn_results[gnn_type] = result
                
                print(f"   âœ… å‚æ•°é‡: {param_count:,}")
                print(f"   âœ… æ¨ç†æ—¶é—´: {inference_time:.4f}s")
                print(f"   âœ… èŠ‚ç‚¹è¾“å‡ºå½¢çŠ¶: {node_logits.shape}")
                print(f"   âœ… å›¾è¾“å‡ºå½¢çŠ¶: {graph_logits.shape}")
            
            # ä¿å­˜ç»“æœ
            self.results['basic_gnn'] = gnn_results
            
            # æ€§èƒ½å¯¹æ¯”
            print(f"\nğŸ“Š GNNæ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
            print(f"{'æ¨¡å‹':<12} {'å‚æ•°é‡':<12} {'æ¨ç†æ—¶é—´(s)':<12}")
            print("-" * 40)
            
            for gnn_type, result in gnn_results.items():
                print(f"{gnn_type.upper():<12} {result['parameters']:<12,} {result['inference_time']:<12.4f}")
            
            print(f"âœ… åŸºç¡€GNNæ¼”ç¤ºå®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ åŸºç¡€GNNæ¼”ç¤ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def demo_social_graph_builder(self):
        """æ¼”ç¤ºç¤¾äº¤å›¾æ„å»ºåŠŸèƒ½"""
        print(f"\n{'='*70}")
        print("ğŸ”— ç¤¾äº¤å›¾æ„å»ºæ¼”ç¤º")
        print(f"{'='*70}")
        
        try:
            # åˆ›å»ºå›¾æ„å»ºå™¨
            graph_builder = SocialGraphBuilder()
            
            # åˆ›å»ºæ¼”ç¤ºæ•°æ®ï¼ˆæ¨¡æ‹ŸMR2æ•°æ®æ ¼å¼ï¼‰
            print(f"ğŸ”§ åˆ›å»ºæ¼”ç¤ºæ•°æ®...")
            demo_data = self._create_demo_mr2_data()
            
            print(f"ğŸ“š æ¼”ç¤ºæ•°æ®ç»Ÿè®¡:")
            print(f"   æ•°æ®é¡¹æ•°: {len(demo_data)}")
            
            # æ„å»ºä¸åŒç±»å‹çš„å›¾
            graphs = {}
            
            print(f"\nğŸ—ï¸  æ„å»ºå¤šå±‚å›¾ç»“æ„...")
            
            # 1. æ–‡æœ¬ç›¸ä¼¼åº¦å›¾
            try:
                print(f"\n1. æ„å»ºæ–‡æœ¬ç›¸ä¼¼åº¦å›¾...")
                text_graph = graph_builder.build_text_similarity_graph(demo_data, similarity_threshold=0.1)
                graphs['text_similarity'] = text_graph
                
                print(f"   âœ… æ–‡æœ¬å›¾: {text_graph.num_nodes} èŠ‚ç‚¹, {text_graph.edge_index.size(1)} è¾¹")
                print(f"   âœ… ç‰¹å¾ç»´åº¦: {text_graph.x.size(1)}")
                
            except Exception as e:
                print(f"   âŒ æ–‡æœ¬å›¾æ„å»ºå¤±è´¥: {e}")
                graphs['text_similarity'] = None
            
            # 2. å®ä½“å…±ç°å›¾
            try:
                print(f"\n2. æ„å»ºå®ä½“å…±ç°å›¾...")
                entity_graph = graph_builder.build_entity_cooccurrence_graph(demo_data)
                graphs['entity_cooccurrence'] = entity_graph
                
                print(f"   âœ… å®ä½“å›¾: {entity_graph.num_nodes} èŠ‚ç‚¹, {entity_graph.edge_index.size(1)} è¾¹")
                print(f"   âœ… å®ä½“æ•°: {len(entity_graph.entities) if hasattr(entity_graph, 'entities') else 'N/A'}")
                
            except Exception as e:
                print(f"   âŒ å®ä½“å›¾æ„å»ºå¤±è´¥: {e}")
                graphs['entity_cooccurrence'] = None
            
            # 3. åŸŸåå›¾
            try:
                print(f"\n3. æ„å»ºåŸŸåå›¾...")
                domain_graph = graph_builder.build_domain_graph(demo_data)
                graphs['domain'] = domain_graph
                
                print(f"   âœ… åŸŸåå›¾: {domain_graph.num_nodes} èŠ‚ç‚¹, {domain_graph.edge_index.size(1)} è¾¹")
                print(f"   âœ… åŸŸåæ•°: {len(domain_graph.domains) if hasattr(domain_graph, 'domains') else 'N/A'}")
                
            except Exception as e:
                print(f"   âŒ åŸŸåå›¾æ„å»ºå¤±è´¥: {e}")
                graphs['domain'] = None
            
            # å›¾ç‰¹å¾æå–
            print(f"\nğŸ“Š å›¾ç‰¹å¾æå–æ¼”ç¤º...")
            feature_extractor = GraphFeatureExtractor()
            
            for graph_name, graph_data in graphs.items():
                if graph_data is not None:
                    print(f"\nåˆ†æ {graph_name} å›¾:")
                    
                    try:
                        # æå–å›¾çº§ç‰¹å¾
                        graph_features = feature_extractor.extract_graph_features(graph_data)
                        
                        print(f"   å›¾çº§ç‰¹å¾:")
                        key_features = ['num_nodes', 'num_edges', 'density', 'avg_clustering', 'is_connected']
                        for feature_name in key_features:
                            if feature_name in graph_features:
                                value = graph_features[feature_name]
                                print(f"     {feature_name}: {value:.4f}" if isinstance(value, float) else f"     {feature_name}: {value}")
                        
                        # å¢å¼ºèŠ‚ç‚¹ç‰¹å¾
                        original_dim = graph_data.x.size(1)
                        augmented_graph = feature_extractor.augment_node_features(graph_data)
                        new_dim = augmented_graph.x.size(1)
                        
                        print(f"   èŠ‚ç‚¹ç‰¹å¾å¢å¼º: {original_dim} -> {new_dim}")
                        
                        # ä¿å­˜å›¾
                        filename = f"demo_{graph_name}_graph.pt"
                        graph_builder.save_graph(graph_data, filename)
                        print(f"   âœ… å›¾å·²ä¿å­˜: {filename}")
                        
                    except Exception as e:
                        print(f"   âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
            
            # ä¿å­˜ç»“æœ
            self.results['social_graphs'] = {
                'graphs_built': len([g for g in graphs.values() if g is not None]),
                'total_graphs': len(graphs),
                'graph_info': {
                    name: {
                        'nodes': graph.num_nodes if graph else 0,
                        'edges': graph.edge_index.size(1) if graph else 0,
                        'features': graph.x.size(1) if graph else 0
                    } for name, graph in graphs.items()
                }
            }
            
            print(f"\nâœ… ç¤¾äº¤å›¾æ„å»ºæ¼”ç¤ºå®Œæˆ")
            print(f"   æˆåŠŸæ„å»º: {len([g for g in graphs.values() if g is not None])}/{len(graphs)} ä¸ªå›¾")
            
            return graphs
            
        except Exception as e:
            print(f"âŒ ç¤¾äº¤å›¾æ„å»ºæ¼”ç¤ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    def demo_multimodal_gnn(self):
        """æ¼”ç¤ºå¤šæ¨¡æ€GNNåŠŸèƒ½"""
        print(f"\n{'='*70}")
        print("ğŸ¤– å¤šæ¨¡æ€å›¾ç¥ç»ç½‘ç»œæ¼”ç¤º")
        print(f"{'='*70}")
        
        try:
            # æµ‹è¯•ä¸åŒçš„é…ç½®
            test_configs = [
                {
                    'name': 'GAT + Attention Fusion',
                    'gnn_type': 'gat',
                    'fusion_method': 'attention'
                },
                {
                    'name': 'GCN + Concatenation Fusion',
                    'gnn_type': 'gcn',
                    'fusion_method': 'concat'
                },
                {
                    'name': 'GraphSAGE + Gate Fusion',
                    'gnn_type': 'graphsage',
                    'fusion_method': 'gate'
                }
            ]
            
            multimodal_results = {}
            
            for i, config in enumerate(test_configs):
                print(f"\nğŸ§ª æµ‹è¯•é…ç½® {i+1}: {config['name']}")
                print("-" * 50)
                
                try:
                    # åˆ›å»ºæ¨¡å‹é…ç½®
                    model_config = create_multimodal_gnn_config(
                        gnn_type=config['gnn_type'],
                        fusion_method=config['fusion_method']
                    )
                    
                    # åˆ›å»ºåˆ†ç±»å™¨
                    classifier = MultimodalGraphClassifier(model_config, device=str(self.device))
                    
                    # è·å–æ¨¡å‹ä¿¡æ¯
                    param_info = classifier.model.get_parameter_count()
                    print(f"   ğŸ“Š æ¨¡å‹å‚æ•°:")
                    print(f"     æ€»å‚æ•°: {param_info['total']:,}")
                    print(f"     å¯è®­ç»ƒå‚æ•°: {param_info['trainable']:,}")
                    print(f"     å†»ç»“å‚æ•°: {param_info['frozen']:,}")
                    
                    # å¿«é€Ÿè®­ç»ƒæ¼”ç¤º
                    print(f"   ğŸ‹ï¸ å¼€å§‹æ¼”ç¤ºè®­ç»ƒ...")
                    start_time = time.time()
                    
                    history = classifier.train_demo(epochs=3, learning_rate=1e-4)
                    
                    train_time = time.time() - start_time
                    
                    print(f"   â±ï¸ è®­ç»ƒæ—¶é—´: {train_time:.2f}s")
                    
                    # æ¨ç†æµ‹è¯•
                    print(f"   ğŸ” æ¨ç†æµ‹è¯•...")
                    classifier.test_inference()
                    
                    # ä¿å­˜ç»“æœ
                    result = {
                        'config_name': config['name'],
                        'gnn_type': config['gnn_type'],
                        'fusion_method': config['fusion_method'],
                        'parameters': param_info,
                        'training_time': train_time,
                        'final_train_loss': history['train_losses'][-1] if history['train_losses'] else 0,
                        'final_val_accuracy': history['val_accuracies'][-1] if history['val_accuracies'] else 0,
                        'final_val_f1': history['val_f1_scores'][-1] if history['val_f1_scores'] else 0
                    }
                    
                    multimodal_results[f"config_{i+1}"] = result
                    
                    print(f"   âœ… é…ç½® {i+1} æµ‹è¯•æˆåŠŸ")
                    
                    # ä¸ºäº†èŠ‚çœæ—¶é—´ï¼Œåªå®Œæ•´æµ‹è¯•ç¬¬ä¸€ä¸ªé…ç½®
                    if i == 0:
                        print(f"   âš ï¸  ä¸ºèŠ‚çœæ¼”ç¤ºæ—¶é—´ï¼Œå®Œæ•´æµ‹è¯•ç¬¬ä¸€ä¸ªé…ç½®")
                    else:
                        print(f"   âš ï¸  å¿«é€ŸéªŒè¯é…ç½® {i+1}")
                        break
                    
                except Exception as e:
                    print(f"   âŒ é…ç½® {i+1} æµ‹è¯•å¤±è´¥: {e}")
                    multimodal_results[f"config_{i+1}"] = {
                        'config_name': config['name'],
                        'error': str(e)
                    }
            
            # ç»“æœæ±‡æ€»
            print(f"\nğŸ“Š å¤šæ¨¡æ€GNNæµ‹è¯•ç»“æœæ±‡æ€»:")
            print(f"{'é…ç½®':<25} {'å‚æ•°é‡':<12} {'è®­ç»ƒæ—¶é—´(s)':<12} {'éªŒè¯F1':<10}")
            print("-" * 65)
            
            for config_key, result in multimodal_results.items():
                if 'error' not in result:
                    config_name = result['config_name'][:22] + "..." if len(result['config_name']) > 25 else result['config_name']
                    param_count = result['parameters']['total']
                    train_time = result['training_time']
                    val_f1 = result['final_val_f1']
                    
                    print(f"{config_name:<25} {param_count:<12,} {train_time:<12.2f} {val_f1:<10.4f}")
                else:
                    print(f"{result['config_name']:<25} {'ERROR':<12} {'ERROR':<12} {'ERROR':<10}")
            
            # ä¿å­˜ç»“æœ
            self.results['multimodal_gnn'] = multimodal_results
            
            print(f"\nâœ… å¤šæ¨¡æ€GNNæ¼”ç¤ºå®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ å¤šæ¨¡æ€GNNæ¼”ç¤ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _create_demo_mr2_data(self):
        """åˆ›å»ºæ¼”ç¤ºMR2æ•°æ®"""
        demo_data = {}
        
        # æ¨¡æ‹Ÿæ–‡æœ¬æ•°æ®
        demo_texts = [
            "è¿™æ˜¯ä¸€ä¸ªå…³äºç§‘æŠ€è¿›æ­¥çš„çœŸå®æ–°é—»æŠ¥é“ï¼ŒåŒ…å«äº†è¯¦ç»†çš„æŠ€æœ¯ç»†èŠ‚å’Œä¸“å®¶è§‚ç‚¹",
            "This is fake news about celebrity scandal without any credible sources or evidence",
            "æœªç»è¯å®çš„ä¼ è¨€éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥éªŒè¯ï¼Œç›®å‰æ— æ³•ç¡®å®šæ¶ˆæ¯çš„çœŸå®æ€§",
            "Breaking news: Major breakthrough in artificial intelligence announced by researchers",
            "ç½‘ä¼ æŸåœ°å‘ç”Ÿé‡å¤§äº‹æ•…ï¼Œä½†å®˜æ–¹å°šæœªç¡®è®¤æ¶ˆæ¯çš„å‡†ç¡®æ€§",
            "Scientific study reveals new insights about climate change effects",
            "è°£ä¼ æŸçŸ¥åå…¬å¸å³å°†å€’é—­ï¼Œä½†å…¬å¸å®˜æ–¹å·²ç»å‘å¸ƒè¾Ÿè°£å£°æ˜",
            "Weather alert issued by meteorological department for severe storms",
            "ç¤¾äº¤åª’ä½“å¹¿æ³›æµä¼ çš„æœªè¯å®æ¶ˆæ¯å¼•å‘äº†å…¬ä¼—çš„å…³æ³¨å’Œè®¨è®º",
            "Economic indicators show positive growth trends in technology sector",
            "æ–°ç ”ç©¶è¡¨æ˜äººå·¥æ™ºèƒ½æŠ€æœ¯å¯¹ç¤¾ä¼šäº§ç”Ÿäº†æ·±è¿œçš„å½±å“",
            "Unverified claims about health benefits spread rapidly online",
            "æ”¿åºœå‘å¸ƒå®˜æ–¹å£°æ˜æ¾„æ¸…ç½‘ç»œä¼ è¨€å¹¶æä¾›äº†å‡†ç¡®çš„ä¿¡æ¯",
            "False information about vaccine side effects causes public concern",
            "ä¸“å®¶å‘¼åå…¬ä¼—ç†æ€§å¯¹å¾…ç½‘ç»œä¿¡æ¯ï¼Œé¿å…ç›²ç›®ä¼ æ’­æœªç»è¯å®çš„æ¶ˆæ¯"
        ]
        
        # æ¨¡æ‹ŸåŸŸåæ•°æ®
        demo_domains = [
            "cnn.com", "bbc.com", "reuters.com", "nytimes.com", "washingtonpost.com",
            "fakesource.net", "unreliable.org", "suspicious.info", "dubious.co",
            "twitter.com", "facebook.com", "instagram.com", "youtube.com",
            "wikipedia.org", "github.com"
        ]
        
        # æ¨¡æ‹Ÿå®ä½“æ•°æ®
        demo_entities = [
            "artificial intelligence", "machine learning", "technology", "research",
            "climate change", "economy", "health", "vaccine", "government",
            "social media", "fake news", "misinformation", "verification"
        ]
        
        for i in range(len(demo_texts)):
            item_data = {
                'caption': demo_texts[i],
                'label': i % 3,  # 0: Non-rumor, 1: Rumor, 2: Unverified
                'direct_path': f'demo/direct/{i}',
                'inv_path': f'demo/inverse/{i}',
                'image_path': f'demo/img/{i}.jpg'
            }
            
            demo_data[str(i)] = item_data
        
        # ä¸ºäº†æ¼”ç¤ºå›¾æ„å»ºï¼Œæˆ‘ä»¬éœ€è¦åœ¨å†…å­˜ä¸­æ¨¡æ‹Ÿä¸€äº›æ ‡æ³¨æ•°æ®
        # è¿™é‡Œæˆ‘ä»¬ç›´æ¥åœ¨ demo_data ä¸­æ·»åŠ æ¨¡æ‹Ÿçš„å®ä½“å’ŒåŸŸåä¿¡æ¯
        import random
        
        for item_id, item_data in demo_data.items():
            # æ¨¡æ‹Ÿå®ä½“
            num_entities = random.randint(2, 5)
            item_entities = random.sample(demo_entities, min(num_entities, len(demo_entities)))
            item_data['mock_entities'] = item_entities
            
            # æ¨¡æ‹ŸåŸŸå
            num_domains = random.randint(1, 3)
            item_domains = random.sample(demo_domains, min(num_domains, len(demo_domains)))
            item_data['mock_domains'] = item_domains
        
        return demo_data
    
    def run_full_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print(f"ğŸš€ å›¾ç¥ç»ç½‘ç»œæ¨¡å—å®Œæ•´æ¼”ç¤º")
        print(f"{'='*70}")
        
        start_time = time.time()
        
        # 1. åŸºç¡€GNNæ¼”ç¤º
        print(f"\nğŸ”„ ç¬¬1éƒ¨åˆ†ï¼šåŸºç¡€å›¾ç¥ç»ç½‘ç»œ")
        basic_success = self.demo_basic_gnn()
        
        # 2. ç¤¾äº¤å›¾æ„å»ºæ¼”ç¤º
        print(f"\nğŸ”„ ç¬¬2éƒ¨åˆ†ï¼šç¤¾äº¤å›¾æ„å»º")
        social_graphs = self.demo_social_graph_builder()
        
        # 3. å¤šæ¨¡æ€GNNæ¼”ç¤º
        print(f"\nğŸ”„ ç¬¬3éƒ¨åˆ†ï¼šå¤šæ¨¡æ€å›¾ç¥ç»ç½‘ç»œ")
        multimodal_success = self.demo_multimodal_gnn()
        
        total_time = time.time() - start_time
        
        # ç”Ÿæˆæ¼”ç¤ºæŠ¥å‘Š
        self._generate_demo_report(basic_success, len(social_graphs) > 0, multimodal_success, total_time)
        
        return self.results
    
    def _generate_demo_report(self, basic_success, social_success, multimodal_success, total_time):
        """ç”Ÿæˆæ¼”ç¤ºæŠ¥å‘Š"""
        print(f"\n{'='*70}")
        print(f"ğŸ“‹ å›¾ç¥ç»ç½‘ç»œæ¨¡å—æ¼”ç¤ºæŠ¥å‘Š")
        print(f"{'='*70}")
        
        print(f"ğŸ• æ€»æ¼”ç¤ºæ—¶é—´: {total_time:.2f}ç§’")
        print(f"\nğŸ“Š æ¼”ç¤ºç»“æœæ±‡æ€»:")
        
        # åŸºç¡€GNNç»“æœ
        if basic_success:
            print(f"   âœ… åŸºç¡€GNN: æˆåŠŸ")
            if 'basic_gnn' in self.results:
                gnn_count = len(self.results['basic_gnn'])
                print(f"      - æµ‹è¯•äº† {gnn_count} ç§GNNæ¶æ„")
                
                fastest_model = min(self.results['basic_gnn'].items(), 
                                  key=lambda x: x[1]['inference_time'])
                print(f"      - æœ€å¿«æ¨¡å‹: {fastest_model[0].upper()} ({fastest_model[1]['inference_time']:.4f}s)")
        else:
            print(f"   âŒ åŸºç¡€GNN: å¤±è´¥")
        
        # ç¤¾äº¤å›¾æ„å»ºç»“æœ
        if social_success:
            print(f"   âœ… ç¤¾äº¤å›¾æ„å»º: æˆåŠŸ")
            if 'social_graphs' in self.results:
                built_count = self.results['social_graphs']['graphs_built']
                total_count = self.results['social_graphs']['total_graphs']
                print(f"      - æˆåŠŸæ„å»º: {built_count}/{total_count} ä¸ªå›¾")
        else:
            print(f"   âŒ ç¤¾äº¤å›¾æ„å»º: å¤±è´¥")
        
        # å¤šæ¨¡æ€GNNç»“æœ
        if multimodal_success:
            print(f"   âœ… å¤šæ¨¡æ€GNN: æˆåŠŸ")
            if 'multimodal_gnn' in self.results:
                config_count = len([r for r in self.results['multimodal_gnn'].values() 
                                  if 'error' not in r])
                print(f"      - æˆåŠŸæµ‹è¯•: {config_count} ä¸ªé…ç½®")
        else:
            print(f"   âŒ å¤šæ¨¡æ€GNN: å¤±è´¥")
        
        # ä¿å­˜æŠ¥å‘Š
        try:
            import json
            report_file = self.output_dir / 'demo_report.json'
            
            report_data = {
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'total_time': total_time,
                'results': {
                    'basic_gnn_success': basic_success,
                    'social_graph_success': social_success,
                    'multimodal_gnn_success': multimodal_success
                },
                'detailed_results': self.results
            }
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"\nğŸ’¾ æ¼”ç¤ºæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            print(f"\nâš ï¸  ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
        
        print(f"\nğŸ‰ å›¾ç¥ç»ç½‘ç»œæ¨¡å—æ¼”ç¤ºå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")


def quick_demo():
    """å¿«é€Ÿæ¼”ç¤ºï¼ˆ5åˆ†é’Ÿç‰ˆæœ¬ï¼‰"""
    print("âš¡ å›¾ç¥ç»ç½‘ç»œæ¨¡å—å¿«é€Ÿæ¼”ç¤ºï¼ˆ5åˆ†é’Ÿç‰ˆæœ¬ï¼‰")
    
    demo_runner = GNNDemoRunner()
    
    print(f"\nğŸ”„ è¿è¡Œå¿«é€Ÿæ¼”ç¤º...")
    
    # åªè¿è¡ŒåŸºç¡€GNNæ¼”ç¤º
    success = demo_runner.demo_basic_gnn()
    
    if success:
        print(f"\nâœ… å¿«é€Ÿæ¼”ç¤ºæˆåŠŸå®Œæˆï¼")
        print(f"ğŸ’¡ è¿è¡Œå®Œæ•´æ¼”ç¤º: python {__file__} --full")
    else:
        print(f"\nâŒ å¿«é€Ÿæ¼”ç¤ºå¤±è´¥")
    
    return success


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å›¾ç¥ç»ç½‘ç»œæ¨¡å—æ¼”ç¤º')
    parser.add_argument('--full', action='store_true', 
                       help='è¿è¡Œå®Œæ•´æ¼”ç¤ºï¼ˆåŒ…å«æ‰€æœ‰æ¨¡å—ï¼‰')
    parser.add_argument('--quick', action='store_true', 
                       help='è¿è¡Œå¿«é€Ÿæ¼”ç¤ºï¼ˆä»…åŸºç¡€åŠŸèƒ½ï¼‰')
    
    args = parser.parse_args()
    
    if args.full:
        print("ğŸš€ è¿è¡Œå®Œæ•´æ¼”ç¤º...")
        demo_runner = GNNDemoRunner()
        results = demo_runner.run_full_demo()
    elif args.quick:
        print("âš¡ è¿è¡Œå¿«é€Ÿæ¼”ç¤º...")
        quick_demo()
    else:
        # é»˜è®¤è¿è¡Œå¿«é€Ÿæ¼”ç¤º
        print("ğŸ¯ å›¾ç¥ç»ç½‘ç»œæ¨¡å—æ¼”ç¤º")
        print("ä½¿ç”¨ --full è¿è¡Œå®Œæ•´æ¼”ç¤ºï¼Œæˆ– --quick è¿è¡Œå¿«é€Ÿæ¼”ç¤º")
        print("é»˜è®¤è¿è¡Œå¿«é€Ÿæ¼”ç¤º...")
        quick_demo()


if __name__ == "__main__":
    main()