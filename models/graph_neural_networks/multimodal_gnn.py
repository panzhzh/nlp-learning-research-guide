#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# models/graph_neural_networks/multimodal_gnn.py

"""
å¤šæ¨¡æ€å›¾ç¥ç»ç½‘ç»œæ¨¡å—
ç»“åˆæ–‡æœ¬ã€å›¾åƒã€å›¾ç»“æ„çš„è”åˆå»ºæ¨¡
ä¸“é—¨ä¸ºMR2æ•°æ®é›†å’Œè°£è¨€æ£€æµ‹ä»»åŠ¡è®¾è®¡
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data, Batch
from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool
import torchvision.models as models
from transformers import AutoTokenizer, AutoModel
from typing import Dict, List, Tuple, Any, Optional, Union
import numpy as np
from pathlib import Path
import sys
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from .basic_gnn_layers import BasicGNN, GNNClassifier
    from .social_graph_builder import SocialGraphBuilder, GraphFeatureExtractor
    from utils.config_manager import get_config_manager, get_output_path
    from data_utils.data_loaders import create_all_dataloaders
    USE_PROJECT_MODULES = True
    print("âœ… æˆåŠŸå¯¼å…¥é¡¹ç›®æ¨¡å— (ç›¸å¯¹å¯¼å…¥)")
except ImportError as e:
    print(f"âš ï¸  ç›¸å¯¹å¯¼å…¥å¤±è´¥: {e}")
    try:
        from basic_gnn_layers import BasicGNN, GNNClassifier
        from social_graph_builder import SocialGraphBuilder, GraphFeatureExtractor
        # å°è¯•å¯¼å…¥é¡¹ç›®é…ç½®
        try:
            from utils.config_manager import get_config_manager, get_output_path
            from data_utils.data_loaders import create_all_dataloaders
            USE_PROJECT_MODULES = True
            print("âœ… æˆåŠŸå¯¼å…¥æœ¬åœ°æ¨¡å— (åŒ…å«é¡¹ç›®é…ç½®)")
        except ImportError:
            USE_PROJECT_MODULES = False
            print("âœ… æˆåŠŸå¯¼å…¥æœ¬åœ°æ¨¡å— (ä¸å«é¡¹ç›®é…ç½®)")
    except ImportError as e2:
        print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e2}")
        print("åˆ›å»ºç®€åŒ–ç‰ˆæœ¬...")
        USE_PROJECT_MODULES = False
        
        # åˆ›å»ºç®€åŒ–çš„ç±»å®šä¹‰ç”¨äºç‹¬ç«‹è¿è¡Œ
        class BasicGNN(nn.Module):
            def __init__(self, input_dim, hidden_dims, output_dim, gnn_type='simple', **kwargs):
                super().__init__()
                self.layers = nn.Sequential(
                    nn.Linear(input_dim, hidden_dims[0]),
                    nn.ReLU(),
                    nn.Dropout(0.5),
                    nn.Linear(hidden_dims[0], output_dim)
                )
            
            def forward(self, x, edge_index, batch=None):
                if batch is not None:
                    # å›¾çº§ä»»åŠ¡ï¼šå–å¹³å‡
                    x = torch.mean(x, dim=0, keepdim=True)
                    return self.layers(x)
                else:
                    # èŠ‚ç‚¹çº§ä»»åŠ¡
                    return self.layers(torch.mean(x, dim=0, keepdim=True).repeat(x.size(0), 1))
        
        class GNNClassifier(nn.Module):
            def __init__(self, input_dim, hidden_dims, num_classes, **kwargs):
                super().__init__()
                self.gnn = BasicGNN(input_dim, hidden_dims, hidden_dims[-1])
                self.classifier = nn.Linear(hidden_dims[-1], num_classes)
                
            def forward(self, x, edge_index, batch=None):
                x = self.gnn(x, edge_index, batch)
                return self.classifier(x)
        
        class SocialGraphBuilder:
            def __init__(self, *args, **kwargs):
                print("âš ï¸  ä½¿ç”¨ç®€åŒ–ç‰ˆç¤¾äº¤å›¾æ„å»ºå™¨")
                
        class GraphFeatureExtractor:
            def __init__(self, *args, **kwargs):
                print("âš ï¸  ä½¿ç”¨ç®€åŒ–ç‰ˆå›¾ç‰¹å¾æå–å™¨")

import logging
logger = logging.getLogger(__name__)


class TextEncoder(nn.Module):
    """æ–‡æœ¬ç¼–ç å™¨"""
    
    def __init__(self, model_name: str = 'bert-base-uncased', 
                 output_dim: int = 768, freeze_bert: bool = False):
        """
        åˆå§‹åŒ–æ–‡æœ¬ç¼–ç å™¨
        
        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
            output_dim: è¾“å‡ºç»´åº¦
            freeze_bert: æ˜¯å¦å†»ç»“BERTå‚æ•°
        """
        super(TextEncoder, self).__init__()
        
        self.model_name = model_name
        self.output_dim = output_dim
        
        try:
            # å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.bert = AutoModel.from_pretrained(model_name)
            
            if freeze_bert:
                for param in self.bert.parameters():
                    param.requires_grad = False
            
            # æŠ•å½±å±‚
            bert_dim = self.bert.config.hidden_size
            if bert_dim != output_dim:
                self.projection = nn.Linear(bert_dim, output_dim)
            else:
                self.projection = nn.Identity()
                
            self.use_pretrained = True
            print(f"âœ… åŠ è½½é¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨: {model_name}")
            
        except Exception as e:
            print(f"âš ï¸  åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¤±è´¥: {e}")
            print("ä½¿ç”¨ç®€å•çš„æ–‡æœ¬ç¼–ç å™¨")
            
            # ç®€å•çš„æ–‡æœ¬ç¼–ç å™¨
            self.vocab_size = 10000
            self.embedding_dim = 256
            self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=0)
            self.lstm = nn.LSTM(self.embedding_dim, output_dim // 2, 
                              batch_first=True, bidirectional=True)
            self.projection = nn.Identity()
            self.use_pretrained = False
    
    def forward(self, text_inputs):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            text_inputs: æ–‡æœ¬è¾“å…¥ï¼ˆå­—ç¬¦ä¸²åˆ—è¡¨æˆ–token idsï¼‰
            
        Returns:
            æ–‡æœ¬ç‰¹å¾ [batch_size, output_dim]
        """
        if self.use_pretrained:
            if isinstance(text_inputs, list):
                # å­—ç¬¦ä¸²åˆ—è¡¨
                encoded = self.tokenizer(
                    text_inputs, 
                    padding=True, 
                    truncation=True, 
                    max_length=512,
                    return_tensors='pt'
                )
                
                if next(self.bert.parameters()).is_cuda:
                    encoded = {k: v.cuda() for k, v in encoded.items()}
                
                with torch.no_grad() if not self.training else torch.enable_grad():
                    outputs = self.bert(**encoded)
                
                # ä½¿ç”¨[CLS] tokençš„è¡¨ç¤º
                text_features = outputs.last_hidden_state[:, 0, :]  # [batch_size, bert_dim]
            else:
                # å·²ç»æ˜¯tensor
                if len(text_inputs.shape) == 2:
                    # [batch_size, seq_len]
                    attention_mask = (text_inputs != 0).long()
                    outputs = self.bert(input_ids=text_inputs, attention_mask=attention_mask)
                    text_features = outputs.last_hidden_state[:, 0, :]
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥æ ¼å¼: {text_inputs.shape}")
            
            # æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
            text_features = self.projection(text_features)
            
        else:
            # ç®€å•ç¼–ç å™¨
            if isinstance(text_inputs, list):
                # ç®€å•çš„è¯æ±‡è¡¨æ˜ å°„
                max_len = 128
                batch_size = len(text_inputs)
                input_ids = torch.zeros(batch_size, max_len, dtype=torch.long)
                
                for i, text in enumerate(text_inputs):
                    tokens = text.lower().split()[:max_len]
                    for j, token in enumerate(tokens):
                        # ç®€å•çš„hashæ˜ å°„åˆ°è¯æ±‡è¡¨
                        token_id = hash(token) % (self.vocab_size - 1) + 1
                        input_ids[i, j] = token_id
                
                if next(self.embedding.parameters()).is_cuda:
                    input_ids = input_ids.cuda()
                
                text_inputs = input_ids
            
            # åµŒå…¥å’ŒLSTM
            embedded = self.embedding(text_inputs)  # [batch_size, seq_len, embedding_dim]
            lstm_out, (hidden, _) = self.lstm(embedded)
            
            # ä½¿ç”¨æœ€åä¸€ä¸ªéšè—çŠ¶æ€
            text_features = torch.cat([hidden[0], hidden[1]], dim=1)  # [batch_size, output_dim]
        
        return text_features


class ImageEncoder(nn.Module):
    """å›¾åƒç¼–ç å™¨"""
    
    def __init__(self, model_name: str = 'resnet50', output_dim: int = 768, 
                 pretrained: bool = True, freeze_backbone: bool = False):
        """
        åˆå§‹åŒ–å›¾åƒç¼–ç å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            output_dim: è¾“å‡ºç»´åº¦
            pretrained: æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            freeze_backbone: æ˜¯å¦å†»ç»“ä¸»å¹²ç½‘ç»œ
        """
        super(ImageEncoder, self).__init__()
        
        self.model_name = model_name
        self.output_dim = output_dim
        
        try:
            # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
            if model_name == 'resnet50':
                self.backbone = models.resnet50(pretrained=pretrained)
                backbone_dim = self.backbone.fc.in_features
                self.backbone.fc = nn.Identity()  # ç§»é™¤åˆ†ç±»å¤´
            elif model_name == 'resnet18':
                self.backbone = models.resnet18(pretrained=pretrained)
                backbone_dim = self.backbone.fc.in_features
                self.backbone.fc = nn.Identity()
            elif model_name == 'vgg16':
                self.backbone = models.vgg16(pretrained=pretrained)
                backbone_dim = self.backbone.classifier[0].in_features
                self.backbone.classifier = nn.Identity()
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
            
            if freeze_backbone:
                for param in self.backbone.parameters():
                    param.requires_grad = False
            
            # æŠ•å½±å±‚
            if backbone_dim != output_dim:
                self.projection = nn.Sequential(
                    nn.Linear(backbone_dim, output_dim),
                    nn.ReLU(),
                    nn.Dropout(0.1)
                )
            else:
                self.projection = nn.Identity()
                
            self.use_pretrained = True
            print(f"âœ… åŠ è½½é¢„è®­ç»ƒå›¾åƒç¼–ç å™¨: {model_name}")
            
        except Exception as e:
            print(f"âš ï¸  åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¤±è´¥: {e}")
            print("ä½¿ç”¨ç®€å•çš„å›¾åƒç¼–ç å™¨")
            
            # ç®€å•çš„CNNç¼–ç å™¨
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 64, 3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(64, 128, 3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(128, 256, 3, padding=1),
                nn.ReLU(),
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Flatten()
            )
            self.projection = nn.Linear(256, output_dim)
            self.use_pretrained = False
    
    def forward(self, image_inputs):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            image_inputs: å›¾åƒè¾“å…¥ [batch_size, 3, H, W]
            
        Returns:
            å›¾åƒç‰¹å¾ [batch_size, output_dim]
        """
        if image_inputs.dim() != 4:
            raise ValueError(f"æœŸæœ›4Dè¾“å…¥ï¼Œå¾—åˆ°{image_inputs.dim()}D")
        
        # é€šè¿‡ä¸»å¹²ç½‘ç»œ
        image_features = self.backbone(image_inputs)
        
        # æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
        image_features = self.projection(image_features)
        
        return image_features


class CrossModalAttention(nn.Module):
    """è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å—"""
    
    def __init__(self, text_dim: int, image_dim: int, hidden_dim: int = 256):
        """
        åˆå§‹åŒ–è·¨æ¨¡æ€æ³¨æ„åŠ›
        
        Args:
            text_dim: æ–‡æœ¬ç‰¹å¾ç»´åº¦
            image_dim: å›¾åƒç‰¹å¾ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
        """
        super(CrossModalAttention, self).__init__()
        
        self.text_proj = nn.Linear(text_dim, hidden_dim)
        self.image_proj = nn.Linear(image_dim, hidden_dim)
        
        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=8, batch_first=True)
        
        self.text_output = nn.Linear(hidden_dim, text_dim)
        self.image_output = nn.Linear(hidden_dim, image_dim)
        
        self.layer_norm_text = nn.LayerNorm(text_dim)
        self.layer_norm_image = nn.LayerNorm(image_dim)
        
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, text_features, image_features):
        """
        è·¨æ¨¡æ€æ³¨æ„åŠ›è®¡ç®—
        
        Args:
            text_features: æ–‡æœ¬ç‰¹å¾ [batch_size, text_dim]
            image_features: å›¾åƒç‰¹å¾ [batch_size, image_dim]
            
        Returns:
            èåˆåçš„æ–‡æœ¬å’Œå›¾åƒç‰¹å¾
        """
        batch_size = text_features.size(0)
        
        # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
        text_proj = self.text_proj(text_features).unsqueeze(1)  # [batch_size, 1, hidden_dim]
        image_proj = self.image_proj(image_features).unsqueeze(1)  # [batch_size, 1, hidden_dim]
        
        # æ–‡æœ¬-å›¾åƒæ³¨æ„åŠ›
        text_attended, _ = self.attention(text_proj, image_proj, image_proj)
        text_attended = text_attended.squeeze(1)  # [batch_size, hidden_dim]
        
        # å›¾åƒ-æ–‡æœ¬æ³¨æ„åŠ›
        image_attended, _ = self.attention(image_proj, text_proj, text_proj)
        image_attended = image_attended.squeeze(1)  # [batch_size, hidden_dim]
        
        # è¾“å‡ºæŠ•å½±
        text_output = self.text_output(self.dropout(text_attended))
        image_output = self.image_output(self.dropout(image_attended))
        
        # æ®‹å·®è¿æ¥å’Œå±‚æ ‡å‡†åŒ–
        text_fused = self.layer_norm_text(text_features + text_output)
        image_fused = self.layer_norm_image(image_features + image_output)
        
        return text_fused, image_fused


class MultimodalFusion(nn.Module):
    """å¤šæ¨¡æ€èåˆæ¨¡å—"""
    
    def __init__(self, text_dim: int, image_dim: int, graph_dim: int,
                 fusion_method: str = 'attention', output_dim: int = 512):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€èåˆ
        
        Args:
            text_dim: æ–‡æœ¬ç‰¹å¾ç»´åº¦
            image_dim: å›¾åƒç‰¹å¾ç»´åº¦
            graph_dim: å›¾ç‰¹å¾ç»´åº¦
            fusion_method: èåˆæ–¹æ³• ('concat', 'attention', 'gate', 'cross_modal')
            output_dim: è¾“å‡ºç»´åº¦
        """
        super(MultimodalFusion, self).__init__()
        
        self.fusion_method = fusion_method
        self.text_dim = text_dim
        self.image_dim = image_dim
        self.graph_dim = graph_dim
        self.output_dim = output_dim
        
        if fusion_method == 'concat':
            # ç®€å•æ‹¼æ¥
            total_dim = text_dim + image_dim + graph_dim
            self.fusion_layer = nn.Sequential(
                nn.Linear(total_dim, output_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            )
            
        elif fusion_method == 'attention':
            # æ³¨æ„åŠ›èåˆ
            self.text_proj = nn.Linear(text_dim, output_dim)
            self.image_proj = nn.Linear(image_dim, output_dim)
            self.graph_proj = nn.Linear(graph_dim, output_dim)
            
            self.attention_weights = nn.Linear(output_dim, 1)
            self.layer_norm = nn.LayerNorm(output_dim)
            
        elif fusion_method == 'gate':
            # é—¨æ§èåˆ
            self.text_gate = nn.Sequential(
                nn.Linear(text_dim, output_dim),
                nn.Sigmoid()
            )
            self.image_gate = nn.Sequential(
                nn.Linear(image_dim, output_dim),
                nn.Sigmoid()
            )
            self.graph_gate = nn.Sequential(
                nn.Linear(graph_dim, output_dim),
                nn.Sigmoid()
            )
            
            self.text_proj = nn.Linear(text_dim, output_dim)
            self.image_proj = nn.Linear(image_dim, output_dim)
            self.graph_proj = nn.Linear(graph_dim, output_dim)
            
        elif fusion_method == 'cross_modal':
            # è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆ
            self.cross_attention = CrossModalAttention(text_dim, image_dim)
            self.graph_proj = nn.Linear(graph_dim, output_dim)
            
            fusion_dim = text_dim + image_dim + output_dim
            self.final_fusion = nn.Sequential(
                nn.Linear(fusion_dim, output_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„èåˆæ–¹æ³•: {fusion_method}")
        
        print(f"ğŸ”€ å¤šæ¨¡æ€èåˆåˆå§‹åŒ–: {fusion_method}")
    
    def forward(self, text_features, image_features, graph_features):
        """
        å¤šæ¨¡æ€èåˆ
        
        Args:
            text_features: æ–‡æœ¬ç‰¹å¾ [batch_size, text_dim]
            image_features: å›¾åƒç‰¹å¾ [batch_size, image_dim]
            graph_features: å›¾ç‰¹å¾ [batch_size, graph_dim]
            
        Returns:
            èåˆç‰¹å¾ [batch_size, output_dim]
        """
        if self.fusion_method == 'concat':
            # æ‹¼æ¥èåˆ
            fused = torch.cat([text_features, image_features, graph_features], dim=1)
            fused = self.fusion_layer(fused)
            
        elif self.fusion_method == 'attention':
            # æ³¨æ„åŠ›èåˆ
            text_proj = self.text_proj(text_features)
            image_proj = self.image_proj(image_features)
            graph_proj = self.graph_proj(graph_features)
            
            # å †å ç‰¹å¾
            features = torch.stack([text_proj, image_proj, graph_proj], dim=1)  # [batch_size, 3, output_dim]
            
            # è®¡ç®—æ³¨æ„åŠ›æƒé‡
            attention_scores = self.attention_weights(features).squeeze(-1)  # [batch_size, 3]
            attention_weights = F.softmax(attention_scores, dim=1)  # [batch_size, 3]
            
            # åŠ æƒæ±‚å’Œ
            fused = torch.sum(features * attention_weights.unsqueeze(-1), dim=1)  # [batch_size, output_dim]
            fused = self.layer_norm(fused)
            
        elif self.fusion_method == 'gate':
            # é—¨æ§èåˆ
            text_gate = self.text_gate(text_features)
            image_gate = self.image_gate(image_features)
            graph_gate = self.graph_gate(graph_features)
            
            text_proj = self.text_proj(text_features)
            image_proj = self.image_proj(image_features)
            graph_proj = self.graph_proj(graph_features)
            
            # é—¨æ§æœºåˆ¶
            gated_text = text_gate * text_proj
            gated_image = image_gate * image_proj
            gated_graph = graph_gate * graph_proj
            
            fused = gated_text + gated_image + gated_graph
            
        elif self.fusion_method == 'cross_modal':
            # è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆ
            text_fused, image_fused = self.cross_attention(text_features, image_features)
            graph_proj = self.graph_proj(graph_features)
            
            # æœ€ç»ˆèåˆ
            all_features = torch.cat([text_fused, image_fused, graph_proj], dim=1)
            fused = self.final_fusion(all_features)
        
        return fused


class MultimodalGNN(nn.Module):
    """å¤šæ¨¡æ€å›¾ç¥ç»ç½‘ç»œ"""
    
    def __init__(self, text_encoder_config: Dict[str, Any],
                 image_encoder_config: Dict[str, Any],
                 gnn_config: Dict[str, Any],
                 fusion_config: Dict[str, Any],
                 num_classes: int = 3):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€GNN
        
        Args:
            text_encoder_config: æ–‡æœ¬ç¼–ç å™¨é…ç½®
            image_encoder_config: å›¾åƒç¼–ç å™¨é…ç½®
            gnn_config: GNNé…ç½®
            fusion_config: èåˆé…ç½®
            num_classes: åˆ†ç±»æ•°é‡
        """
        super(MultimodalGNN, self).__init__()
        
        self.num_classes = num_classes
        
        # æ£€æŸ¥æ˜¯å¦æœ‰çœŸæ­£çš„GNNå¯ç”¨
        self._has_real_gnn = USE_PROJECT_MODULES and hasattr(BasicGNN, '__module__')
        
        # æ–‡æœ¬ç¼–ç å™¨
        self.text_encoder = TextEncoder(
            model_name=text_encoder_config.get('model_name', 'bert-base-uncased'),
            output_dim=text_encoder_config.get('output_dim', 768),
            freeze_bert=text_encoder_config.get('freeze_bert', False)
        )
        
        # å›¾åƒç¼–ç å™¨
        self.image_encoder = ImageEncoder(
            model_name=image_encoder_config.get('model_name', 'resnet50'),
            output_dim=image_encoder_config.get('output_dim', 768),
            pretrained=image_encoder_config.get('pretrained', True),
            freeze_backbone=image_encoder_config.get('freeze_backbone', False)
        )
        
        # å›¾ç¥ç»ç½‘ç»œ
        if USE_PROJECT_MODULES and hasattr(self, '_has_real_gnn'):
            self.gnn = BasicGNN(
                input_dim=gnn_config.get('input_dim', 768),
                hidden_dims=gnn_config.get('hidden_dims', [256, 128]),
                output_dim=gnn_config.get('output_dim', 128),
                gnn_type=gnn_config.get('gnn_type', 'gat'),
                dropout=gnn_config.get('dropout', 0.5),
                **gnn_config.get('gnn_params', {})
            )
        else:
            # ç®€å•çš„å›¾ç½‘ç»œå®ç°
            input_dim = gnn_config.get('input_dim', 768)
            output_dim = gnn_config.get('output_dim', 128)
            self.gnn = nn.Sequential(
                nn.Linear(input_dim, 256),
                nn.ReLU(),
                nn.Dropout(0.5),
                nn.Linear(256, output_dim)
            )
            print("âš ï¸  ä½¿ç”¨ç®€åŒ–ç‰ˆGNNå®ç°")
        
        # å¤šæ¨¡æ€èåˆ
        self.fusion = MultimodalFusion(
            text_dim=text_encoder_config.get('output_dim', 768),
            image_dim=image_encoder_config.get('output_dim', 768),
            graph_dim=gnn_config.get('output_dim', 128),
            fusion_method=fusion_config.get('method', 'attention'),
            output_dim=fusion_config.get('output_dim', 512)
        )
        
        # åˆ†ç±»å™¨
        classifier_input_dim = fusion_config.get('output_dim', 512)
        self.classifier = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(classifier_input_dim, classifier_input_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(classifier_input_dim // 2, num_classes)
        )
        
        print(f"ğŸ¤– å¤šæ¨¡æ€GNNåˆå§‹åŒ–å®Œæˆ:")
        print(f"   æ–‡æœ¬ç¼–ç å™¨: {text_encoder_config.get('model_name', 'simple')}")
        print(f"   å›¾åƒç¼–ç å™¨: {image_encoder_config.get('model_name', 'simple')}")
        print(f"   GNNç±»å‹: {gnn_config.get('gnn_type', 'simple')}")
        print(f"   èåˆæ–¹æ³•: {fusion_config.get('method', 'attention')}")
        print(f"   åˆ†ç±»æ•°: {num_classes}")
    
    def forward(self, text_inputs, image_inputs, graph_data, return_features=False):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            text_inputs: æ–‡æœ¬è¾“å…¥
            image_inputs: å›¾åƒè¾“å…¥ [batch_size, 3, H, W]
            graph_data: å›¾æ•°æ® (PyG Dataå¯¹è±¡)
            return_features: æ˜¯å¦è¿”å›ä¸­é—´ç‰¹å¾
            
        Returns:
            åˆ†ç±»logitsæˆ–ç‰¹å¾å­—å…¸
        """
        # æ–‡æœ¬ç¼–ç 
        text_features = self.text_encoder(text_inputs)  # [batch_size, text_dim]
        
        # å›¾åƒç¼–ç 
        image_features = self.image_encoder(image_inputs)  # [batch_size, image_dim]
        
        # å›¾ç¼–ç 
        if self._has_real_gnn and hasattr(self.gnn, 'forward'):
            # ä½¿ç”¨çœŸå®çš„GNN
            if hasattr(graph_data, 'batch'):
                graph_features = self.gnn(graph_data.x, graph_data.edge_index, graph_data.batch)
            else:
                # å•å›¾æƒ…å†µï¼Œåˆ›å»ºbatch
                batch_size = text_features.size(0)
                num_nodes = graph_data.x.size(0)
                batch = torch.zeros(num_nodes, dtype=torch.long, device=graph_data.x.device)
                graph_features = self.gnn(graph_data.x, graph_data.edge_index, batch)
                
                # ç¡®ä¿å›¾ç‰¹å¾çš„batchç»´åº¦ä¸æ–‡æœ¬ã€å›¾åƒä¸€è‡´
                if graph_features.size(0) != batch_size:
                    # å¦‚æœæ˜¯å•ä¸ªå›¾ç‰¹å¾ï¼Œé‡å¤ä»¥åŒ¹é…batch_size
                    if graph_features.size(0) == 1:
                        graph_features = graph_features.repeat(batch_size, 1)
                    else:
                        # å¦‚æœæ˜¯å¤šèŠ‚ç‚¹è¾“å‡ºï¼Œå–å¹³å‡ä½œä¸ºå›¾è¡¨ç¤º
                        graph_features = torch.mean(graph_features, dim=0, keepdim=True)
                        graph_features = graph_features.repeat(batch_size, 1)
        else:
            # ä½¿ç”¨ç®€å•çš„å›¾ç‰¹å¾
            batch_size = text_features.size(0)
            if hasattr(graph_data, 'x'):
                # å–å›¾èŠ‚ç‚¹ç‰¹å¾çš„å¹³å‡å€¼ä½œä¸ºå›¾è¡¨ç¤º
                graph_repr = torch.mean(graph_data.x, dim=0, keepdim=True)  # [1, input_dim]
                graph_features = self.gnn(graph_repr)  # [1, output_dim]
                
                # é‡å¤ä»¥åŒ¹é…batch_size
                graph_features = graph_features.repeat(batch_size, 1)
            else:
                # åˆ›å»ºé›¶å›¾ç‰¹å¾
                graph_dim = 128  # é»˜è®¤å›¾ç‰¹å¾ç»´åº¦
                graph_features = torch.zeros(batch_size, graph_dim, device=text_features.device)
        
        # ç¡®ä¿æ‰€æœ‰ç‰¹å¾çš„batchç»´åº¦ä¸€è‡´
        batch_size = text_features.size(0)
        if image_features.size(0) != batch_size:
            image_features = image_features[:batch_size] if image_features.size(0) > batch_size else image_features.repeat(batch_size, 1)
        if graph_features.size(0) != batch_size:
            graph_features = graph_features[:batch_size] if graph_features.size(0) > batch_size else graph_features.repeat(batch_size, 1)
        
        # å¤šæ¨¡æ€èåˆ
        fused_features = self.fusion(text_features, image_features, graph_features)
        
        # åˆ†ç±»
        logits = self.classifier(fused_features)
        
        if return_features:
            return {
                'logits': logits,
                'text_features': text_features,
                'image_features': image_features,
                'graph_features': graph_features,
                'fused_features': fused_features
            }
        else:
            return logits
    
    def predict(self, text_inputs, image_inputs, graph_data):
        """é¢„æµ‹"""
        with torch.no_grad():
            logits = self.forward(text_inputs, image_inputs, graph_data)
            probabilities = F.softmax(logits, dim=-1)
            predictions = torch.argmax(logits, dim=-1)
            return predictions, probabilities
    
    def get_parameter_count(self):
        """è·å–æ¨¡å‹å‚æ•°æ•°é‡"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'total': total_params,
            'trainable': trainable_params,
            'frozen': total_params - trainable_params
        }


class MultimodalGraphClassifier:
    """å¤šæ¨¡æ€å›¾åˆ†ç±»å™¨è®­ç»ƒå™¨"""
    
    def __init__(self, model_config: Dict[str, Any], device: str = 'auto'):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€å›¾åˆ†ç±»å™¨
        
        Args:
            model_config: æ¨¡å‹é…ç½®
            device: è®¡ç®—è®¾å¤‡
        """
        # è®¾ç½®è®¾å¤‡
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = MultimodalGNN(
            text_encoder_config=model_config.get('text_encoder', {}),
            image_encoder_config=model_config.get('image_encoder', {}),
            gnn_config=model_config.get('gnn', {}),
            fusion_config=model_config.get('fusion', {}),
            num_classes=model_config.get('num_classes', 3)
        ).to(self.device)
        
        # åˆå§‹åŒ–å›¾æ„å»ºå™¨
        if USE_PROJECT_MODULES:
            self.graph_builder = SocialGraphBuilder()
        else:
            self.graph_builder = None
        
        # æ ‡ç­¾æ˜ å°„
        self.label_mapping = {0: 'Non-rumor', 1: 'Rumor', 2: 'Unverified'}
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'train_losses': [],
            'val_accuracies': [],
            'val_f1_scores': []
        }
        
        param_info = self.model.get_parameter_count()
        print(f"ğŸ¤– å¤šæ¨¡æ€å›¾åˆ†ç±»å™¨åˆå§‹åŒ–å®Œæˆ:")
        print(f"   æ€»å‚æ•°: {param_info['total']:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {param_info['trainable']:,}")
        print(f"   å†»ç»“å‚æ•°: {param_info['frozen']:,}")
    
    def create_demo_data(self, batch_size: int = 8):
        """åˆ›å»ºæ¼”ç¤ºæ•°æ® - ä¿®å¤ç»´åº¦é—®é¢˜"""
        print("ğŸ”§ åˆ›å»ºæ¼”ç¤ºæ•°æ®...")
        
        # æ–‡æœ¬æ•°æ®
        demo_texts = [
            "è¿™æ˜¯ä¸€ä¸ªå…³äºç§‘æŠ€çš„çœŸå®æ–°é—»",
            "This is fake news about celebrities",
            "æœªç»è¯å®çš„ä¼ è¨€éœ€è¦éªŒè¯",
            "Breaking news from reliable sources",
            "ç½‘ç»œè°£è¨€ä¼ æ’­é€Ÿåº¦å¾ˆå¿«",
            "Scientific research shows evidence",
            "å®˜æ–¹è¾Ÿè°£å£°æ˜å·²å‘å¸ƒ",
            "Verified information from experts"
        ]
        
        texts = demo_texts[:batch_size]
        
        # å›¾åƒæ•°æ®ï¼ˆç¡®ä¿batchç»´åº¦æ­£ç¡®ï¼‰
        images = torch.randn(batch_size, 3, 224, 224).to(self.device)
        
        # å›¾æ•°æ® - ç¡®ä¿èƒ½äº§ç”Ÿæ­£ç¡®çš„å›¾ç‰¹å¾ç»´åº¦
        num_nodes = batch_size  # è®©èŠ‚ç‚¹æ•°ç­‰äºbatch_sizeï¼Œé¿å…ç»´åº¦é—®é¢˜
        node_features = torch.randn(num_nodes, 768).to(self.device)
        
        # åˆ›å»ºç®€å•çš„ç¯å½¢è¿æ¥
        edge_index = []
        for i in range(num_nodes):
            edge_index.append([i, (i + 1) % num_nodes])
            edge_index.append([(i + 1) % num_nodes, i])  # åŒå‘è¾¹
        
        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous().to(self.device)
        
        from torch_geometric.data import Data
        graph_data = Data(
            x=node_features,
            edge_index=edge_index,
            num_nodes=num_nodes
        )
        
        # æ ‡ç­¾
        labels = torch.randint(0, 3, (batch_size,)).to(self.device)
        
        print(f"âœ… æ¼”ç¤ºæ•°æ®åˆ›å»ºå®Œæˆ:")
        print(f"   æ–‡æœ¬æ•°é‡: {len(texts)}")
        print(f"   å›¾åƒå½¢çŠ¶: {images.shape}")
        print(f"   å›¾èŠ‚ç‚¹æ•°: {num_nodes}")
        print(f"   æ ‡ç­¾æ•°é‡: {labels.shape[0]}")
        
        return texts, images, graph_data, labels
    
    def train_step(self, texts, images, graph_data, labels, optimizer, criterion):
        """å•æ­¥è®­ç»ƒ"""
        self.model.train()
        
        # å‰å‘ä¼ æ’­
        logits = self.model(texts, images, graph_data)
        
        # è®¡ç®—æŸå¤±
        loss = criterion(logits, labels)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # è®¡ç®—å‡†ç¡®ç‡
        predictions = torch.argmax(logits, dim=-1)
        accuracy = (predictions == labels).float().mean()
        
        return loss.item(), accuracy.item()
    
    def evaluate_step(self, texts, images, graph_data, labels, criterion):
        """å•æ­¥è¯„ä¼°"""
        self.model.eval()
        
        with torch.no_grad():
            logits = self.model(texts, images, graph_data)
            loss = criterion(logits, labels)
            
            predictions = torch.argmax(logits, dim=-1)
            accuracy = (predictions == labels).float().mean()
            
            # è®¡ç®—F1åˆ†æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            from sklearn.metrics import f1_score
            f1 = f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average='macro')
        
        return loss.item(), accuracy.item(), f1
    
    def train_demo(self, epochs: int = 10, learning_rate: float = 1e-4):
        """æ¼”ç¤ºè®­ç»ƒ"""
        print(f"ğŸš€ å¼€å§‹å¤šæ¨¡æ€GNNæ¼”ç¤ºè®­ç»ƒ...")
        
        # åˆ›å»ºæ¼”ç¤ºæ•°æ®
        train_texts, train_images, train_graph, train_labels = self.create_demo_data(8)
        val_texts, val_images, val_graph, val_labels = self.create_demo_data(4)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        
        print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {len(train_texts)} æ ·æœ¬")
        print(f"ğŸ“Š éªŒè¯æ•°æ®: {len(val_texts)} æ ·æœ¬")
        
        best_val_f1 = 0.0
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_step(
                train_texts, train_images, train_graph, train_labels,
                optimizer, criterion
            )
            
            # éªŒè¯
            val_loss, val_acc, val_f1 = self.evaluate_step(
                val_texts, val_images, val_graph, val_labels, criterion
            )
            
            # è®°å½•å†å²
            self.training_history['train_losses'].append(train_loss)
            self.training_history['val_accuracies'].append(val_acc)
            self.training_history['val_f1_scores'].append(val_f1)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_f1 > best_val_f1:
                best_val_f1 = val_f1
                best_epoch = epoch
            
            print(f"Epoch {epoch+1}/{epochs}: "
                  f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.3f}, "
                  f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.3f}, Val F1: {val_f1:.3f}")
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ä½³éªŒè¯F1: {best_val_f1:.4f} (Epoch {best_epoch+1})")
        
        return self.training_history
    
    def test_inference(self):
        """æµ‹è¯•æ¨ç†"""
        print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹æ¨ç†...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_texts, test_images, test_graph, test_labels = self.create_demo_data(4)
        
        # æ¨ç†
        predictions, probabilities = self.model.predict(test_texts, test_images, test_graph)
        
        print(f"ğŸ“Š æ¨ç†ç»“æœ:")
        for i in range(len(test_texts)):
            pred = predictions[i].item()
            prob = probabilities[i].max().item()
            true_label = test_labels[i].item()
            
            print(f"   æ ·æœ¬ {i+1}: é¢„æµ‹={self.label_mapping[pred]} (ç½®ä¿¡åº¦: {prob:.3f}), "
                  f"çœŸå®={self.label_mapping[true_label]}")
        
        # æµ‹è¯•ç‰¹å¾æå–
        print(f"\nğŸ” æµ‹è¯•ç‰¹å¾æå–...")
        features = self.model.forward(test_texts, test_images, test_graph, return_features=True)
        
        for feature_name, feature_tensor in features.items():
            if isinstance(feature_tensor, torch.Tensor):
                print(f"   {feature_name}: {feature_tensor.shape}")


def create_multimodal_gnn_config(gnn_type: str = 'gat', 
                                fusion_method: str = 'attention') -> Dict[str, Any]:
    """
    åˆ›å»ºå¤šæ¨¡æ€GNNé…ç½®
    
    Args:
        gnn_type: GNNç±»å‹
        fusion_method: èåˆæ–¹æ³•
        
    Returns:
        é…ç½®å­—å…¸
    """
    config = {
        'text_encoder': {
            'model_name': 'bert-base-uncased',
            'output_dim': 768,
            'freeze_bert': False
        },
        'image_encoder': {
            'model_name': 'resnet50',
            'output_dim': 768,
            'pretrained': True,
            'freeze_backbone': False
        },
        'gnn': {
            'input_dim': 768,
            'hidden_dims': [256, 128],
            'output_dim': 128,
            'gnn_type': gnn_type,
            'dropout': 0.5,
            'gnn_params': {
                'heads': 8 if gnn_type == 'gat' else None,
                'concat': True if gnn_type == 'gat' else None,
                'use_bn': True
            }
        },
        'fusion': {
            'method': fusion_method,
            'output_dim': 512
        },
        'num_classes': 3
    }
    
    return config


if __name__ == "__main__":
    print("ğŸ¤– æµ‹è¯•å¤šæ¨¡æ€å›¾ç¥ç»ç½‘ç»œ")
    
    # åªæµ‹è¯•ä¸€ä¸ªé…ç½®ï¼Œç¡®ä¿èƒ½è¿è¡Œ
    test_config = {'gnn_type': 'gat', 'fusion_method': 'attention'}
    
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•é…ç½®: GNN={test_config['gnn_type'].upper()}, "
          f"Fusion={test_config['fusion_method']}")
    print(f"{'='*60}")
    
    try:
        # åˆ›å»ºæ¨¡å‹é…ç½®
        model_config = create_multimodal_gnn_config(
            gnn_type=test_config['gnn_type'],
            fusion_method=test_config['fusion_method']
        )
        
        # åˆ›å»ºåˆ†ç±»å™¨
        classifier = MultimodalGraphClassifier(model_config)
        
        # æ¼”ç¤ºè®­ç»ƒï¼ˆå‡å°‘epochsï¼‰
        history = classifier.train_demo(epochs=3, learning_rate=1e-4)
        
        # æµ‹è¯•æ¨ç†
        classifier.test_inference()
        
        print(f"âœ… æµ‹è¯•æˆåŠŸå®Œæˆ!")
        print(f"   æœ€ç»ˆè®­ç»ƒæŸå¤±: {history['train_losses'][-1]:.4f}")
        print(f"   æœ€ç»ˆéªŒè¯F1: {history['val_f1_scores'][-1]:.4f}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\nâœ… å¤šæ¨¡æ€å›¾ç¥ç»ç½‘ç»œæµ‹è¯•å®Œæˆ")