#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# models/graph_neural_networks/social_graph_builder.py

"""
ç¤¾äº¤ç½‘ç»œå›¾æ„å»ºæ¨¡å—
ä»MR2æ•°æ®é›†æ„å»ºç¤¾äº¤å›¾ï¼Œæå–å›¾ç‰¹å¾ï¼Œæ”¯æŒå¤šç§å›¾ç»“æ„
ä¸“é—¨ä¸ºè°£è¨€æ£€æµ‹ä»»åŠ¡è®¾è®¡
"""

import torch
import torch.nn.functional as F
from torch_geometric.data import Data, Batch
from torch_geometric.utils import to_networkx, from_networkx
import networkx as nx
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional, Union
from pathlib import Path
import sys
import json
import re
from collections import defaultdict, Counter
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from datasets.data_loaders import create_all_dataloaders
    from utils.config_manager import get_config_manager, get_data_dir, get_output_path
    from preprocessing.text_processing import TextProcessor
    USE_PROJECT_MODULES = True
    print("âœ… æˆåŠŸå¯¼å…¥é¡¹ç›®æ¨¡å—")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥é¡¹ç›®æ¨¡å—å¤±è´¥: {e}")
    USE_PROJECT_MODULES = False

import logging
logger = logging.getLogger(__name__)


class SocialGraphBuilder:
    """ç¤¾äº¤ç½‘ç»œå›¾æ„å»ºå™¨"""
    
    def __init__(self, data_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–ç¤¾äº¤å›¾æ„å»ºå™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
        """
        if USE_PROJECT_MODULES:
            self.data_dir = get_data_dir() if data_dir is None else Path(data_dir)
            self.output_dir = get_output_path('graphs', 'social_networks')
        else:
            self.data_dir = Path(data_dir) if data_dir else Path('data')
            self.output_dir = Path('outputs/graphs/social_networks')
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨
        if USE_PROJECT_MODULES:
            self.text_processor = TextProcessor(language='mixed')
        else:
            self.text_processor = None
        
        # å›¾æ„å»ºå‚æ•°
        self.min_similarity = 0.1  # æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
        self.max_edges_per_node = 10  # æ¯ä¸ªèŠ‚ç‚¹æœ€å¤§è¾¹æ•°
        
        # ç¼“å­˜
        self.node_features_cache = {}
        self.edge_cache = {}
        
        print(f"ğŸ”— ç¤¾äº¤å›¾æ„å»ºå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ•°æ®ç›®å½•: {self.data_dir}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_mr2_data(self, split: str = 'train') -> Dict[str, Any]:
        """
        åŠ è½½MR2æ•°æ®é›†
        
        Args:
            split: æ•°æ®åˆ’åˆ† ('train', 'val', 'test')
            
        Returns:
            æ•°æ®å­—å…¸
        """
        dataset_file = self.data_dir / f'dataset_items_{split}.json'
        
        if not dataset_file.exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {dataset_file}")
        
        with open(dataset_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"ğŸ“š åŠ è½½ {split} æ•°æ®: {len(data)} æ¡è®°å½•")
        return data
    
    def extract_entities_from_annotations(self, item_data: Dict[str, Any]) -> List[str]:
        """
        ä»æ ‡æ³¨æ–‡ä»¶ä¸­æå–å®ä½“
        
        Args:
            item_data: æ•°æ®é¡¹
            
        Returns:
            å®ä½“åˆ—è¡¨
        """
        entities = set()
        
        # ä»inverse searchæ ‡æ³¨æå–å®ä½“
        if 'inv_path' in item_data:
            inv_annotation_file = self.data_dir / item_data['inv_path'] / 'inverse_annotation.json'
            if inv_annotation_file.exists():
                try:
                    with open(inv_annotation_file, 'r', encoding='utf-8') as f:
                        inv_data = json.load(f)
                    
                    # æå–å®ä½“
                    if 'entities' in inv_data:
                        entities.update(inv_data['entities'])
                    
                    # ä»best_guess_lblæå–
                    if 'best_guess_lbl' in inv_data:
                        entities.update(inv_data['best_guess_lbl'])
                        
                except Exception as e:
                    logger.warning(f"è¯»å–inverse annotationå¤±è´¥: {e}")
        
        # ä»direct searchæ ‡æ³¨æå–å®ä½“
        if 'direct_path' in item_data:
            direct_annotation_file = self.data_dir / item_data['direct_path'] / 'direct_annotation.json'
            if direct_annotation_file.exists():
                try:
                    with open(direct_annotation_file, 'r', encoding='utf-8') as f:
                        direct_data = json.load(f)
                    
                    # ä»å›¾åƒæ ‡æ³¨ä¸­æå–å®ä½“
                    for img_data in direct_data.get('images_with_captions', []):
                        if 'caption' in img_data:
                            caption_info = img_data['caption']
                            if isinstance(caption_info, dict):
                                for key, value in caption_info.items():
                                    if isinstance(value, str):
                                        # ç®€å•çš„å®ä½“æå–ï¼ˆå¯ä»¥æ”¹è¿›ï¼‰
                                        words = value.split()
                                        entities.update(word for word in words if len(word) > 2)
                        
                        # ä»åŸŸåæå–
                        if 'domain' in img_data:
                            entities.add(img_data['domain'])
                            
                except Exception as e:
                    logger.warning(f"è¯»å–direct annotationå¤±è´¥: {e}")
        
        return list(entities)
    
    def build_text_similarity_graph(self, data: Dict[str, Any], 
                                  similarity_threshold: float = 0.3) -> Data:
        """
        åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦æ„å»ºå›¾
        
        Args:
            data: MR2æ•°æ®
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            PyG Dataå¯¹è±¡
        """
        print("ğŸ”¤ æ„å»ºæ–‡æœ¬ç›¸ä¼¼åº¦å›¾...")
        
        # æå–æ–‡æœ¬å’Œæ ‡ç­¾
        texts = []
        labels = []
        item_ids = []
        
        for item_id, item_data in data.items():
            if 'caption' in item_data:
                texts.append(item_data['caption'])
                labels.append(item_data.get('label', 0))
                item_ids.append(item_id)
        
        if len(texts) == 0:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬æ•°æ®")
        
        # è®¡ç®—æ–‡æœ¬ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        node_features = self._compute_text_features(texts)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = self._compute_cosine_similarity(node_features)
        
        # æ„å»ºè¾¹
        edge_index, edge_weights = self._build_edges_from_similarity(
            similarity_matrix, similarity_threshold
        )
        
        # åˆ›å»ºå›¾æ•°æ®
        graph_data = Data(
            x=node_features,
            edge_index=edge_index,
            edge_attr=edge_weights,
            y=torch.tensor(labels, dtype=torch.long),
            num_nodes=len(texts)
        )
        
        # æ·»åŠ é¢å¤–ä¿¡æ¯
        graph_data.item_ids = item_ids
        graph_data.texts = texts
        
        print(f"âœ… æ–‡æœ¬ç›¸ä¼¼åº¦å›¾æ„å»ºå®Œæˆ:")
        print(f"   èŠ‚ç‚¹æ•°: {graph_data.num_nodes}")
        print(f"   è¾¹æ•°: {graph_data.edge_index.size(1)}")
        print(f"   ç‰¹å¾ç»´åº¦: {graph_data.x.size(1)}")
        
        return graph_data
    
    def build_entity_cooccurrence_graph(self, data: Dict[str, Any]) -> Data:
        """
        åŸºäºå®ä½“å…±ç°æ„å»ºå›¾
        
        Args:
            data: MR2æ•°æ®
            
        Returns:
            PyG Dataå¯¹è±¡
        """
        print("ğŸ·ï¸  æ„å»ºå®ä½“å…±ç°å›¾...")
        
        # æå–æ¯ä¸ªé¡¹ç›®çš„å®ä½“
        item_entities = {}
        all_entities = set()
        labels = []
        item_ids = []
        
        for item_id, item_data in data.items():
            entities = self.extract_entities_from_annotations(item_data)
            
            # ä»æ–‡æœ¬ä¸­æå–æ›´å¤šå®ä½“
            if 'caption' in item_data and self.text_processor:
                text_tokens = self.text_processor.tokenize(item_data['caption'])
                # ç®€å•çš„å®ä½“è¯†åˆ«ï¼šé•¿åº¦å¤§äº2çš„è¯
                text_entities = [token for token in text_tokens if len(token) > 2]
                entities.extend(text_entities)
            
            if entities:
                item_entities[item_id] = list(set(entities))
                all_entities.update(entities)
                labels.append(item_data.get('label', 0))
                item_ids.append(item_id)
        
        if len(all_entities) == 0:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°å®ä½“æ•°æ®")
        
        # åˆ›å»ºå®ä½“åˆ°ç´¢å¼•çš„æ˜ å°„
        entity_to_idx = {entity: idx for idx, entity in enumerate(all_entities)}
        
        # æ„å»ºå®ä½“ç‰¹å¾ï¼ˆç®€å•çš„one-hotç¼–ç ï¼‰
        num_entities = len(all_entities)
        entity_features = torch.eye(num_entities)
        
        # æ„å»ºå…±ç°è¾¹
        edges = []
        edge_weights = []
        entity_cooccurrence = defaultdict(lambda: defaultdict(int))
        
        # ç»Ÿè®¡å…±ç°
        for item_id, entities in item_entities.items():
            for i, entity1 in enumerate(entities):
                for j, entity2 in enumerate(entities):
                    if i != j and entity1 in entity_to_idx and entity2 in entity_to_idx:
                        entity_cooccurrence[entity1][entity2] += 1
        
        # æ„å»ºè¾¹ç´¢å¼•
        for entity1, cooccur_dict in entity_cooccurrence.items():
            for entity2, count in cooccur_dict.items():
                if count > 1:  # è‡³å°‘å…±ç°2æ¬¡
                    idx1 = entity_to_idx[entity1]
                    idx2 = entity_to_idx[entity2]
                    edges.append([idx1, idx2])
                    edge_weights.append(count)
        
        if len(edges) == 0:
            # å¦‚æœæ²¡æœ‰å…±ç°è¾¹ï¼Œåˆ›å»ºä¸€äº›åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦çš„è¾¹
            print("âš ï¸  æ²¡æœ‰è¶³å¤Ÿçš„å®ä½“å…±ç°ï¼Œä½¿ç”¨éšæœºè¿æ¥")
            edges = [[i, (i + 1) % num_entities] for i in range(min(num_entities, 10))]
            edge_weights = [1.0] * len(edges)
        
        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
        edge_weights = torch.tensor(edge_weights, dtype=torch.float)
        
        # ä¸ºæ¯ä¸ªæ•°æ®é¡¹åˆ›å»ºæ ‡ç­¾ï¼ˆä½¿ç”¨å®ä½“çš„å¹³å‡ï¼‰
        item_labels = torch.tensor(labels, dtype=torch.long) if labels else torch.zeros(num_entities, dtype=torch.long)
        
        # åˆ›å»ºå›¾æ•°æ®
        graph_data = Data(
            x=entity_features,
            edge_index=edge_index,
            edge_attr=edge_weights,
            y=item_labels,
            num_nodes=num_entities
        )
        
        # æ·»åŠ é¢å¤–ä¿¡æ¯
        graph_data.entities = list(all_entities)
        graph_data.entity_to_idx = entity_to_idx
        graph_data.item_entities = item_entities
        
        print(f"âœ… å®ä½“å…±ç°å›¾æ„å»ºå®Œæˆ:")
        print(f"   å®ä½“æ•°: {num_entities}")
        print(f"   è¾¹æ•°: {edge_index.size(1)}")
        print(f"   æ•°æ®é¡¹æ•°: {len(item_ids)}")
        
        return graph_data
    
    def build_domain_graph(self, data: Dict[str, Any]) -> Data:
        """
        åŸºäºåŸŸåæ„å»ºå›¾
        
        Args:
            data: MR2æ•°æ®
            
        Returns:
            PyG Dataå¯¹è±¡
        """
        print("ğŸŒ æ„å»ºåŸŸåå›¾...")
        
        # æå–åŸŸåä¿¡æ¯
        domains = set()
        item_domains = {}
        labels = []
        item_ids = []
        
        for item_id, item_data in data.items():
            item_domain_list = []
            
            # ä»direct annotationæå–åŸŸå
            if 'direct_path' in item_data:
                direct_annotation_file = self.data_dir / item_data['direct_path'] / 'direct_annotation.json'
                if direct_annotation_file.exists():
                    try:
                        with open(direct_annotation_file, 'r', encoding='utf-8') as f:
                            direct_data = json.load(f)
                        
                        # æå–åŸŸå
                        for img_data in direct_data.get('images_with_captions', []):
                            if 'domain' in img_data:
                                domain = img_data['domain']
                                domains.add(domain)
                                item_domain_list.append(domain)
                        
                        for img_data in direct_data.get('images_with_no_captions', []):
                            if 'domain' in img_data:
                                domain = img_data['domain']
                                domains.add(domain)
                                item_domain_list.append(domain)
                                
                    except Exception as e:
                        logger.warning(f"è¯»å–direct annotationå¤±è´¥: {e}")
            
            # ä»inverse annotationæå–åŸŸå
            if 'inv_path' in item_data:
                inv_annotation_file = self.data_dir / item_data['inv_path'] / 'inverse_annotation.json'
                if inv_annotation_file.exists():
                    try:
                        with open(inv_annotation_file, 'r', encoding='utf-8') as f:
                            inv_data = json.load(f)
                        
                        # ä»åŒ¹é…ç»“æœæå–åŸŸå
                        for match_data in inv_data.get('fully_matched_no_text', []):
                            if 'domain' in match_data:
                                domain = match_data['domain']
                                domains.add(domain)
                                item_domain_list.append(domain)
                                
                    except Exception as e:
                        logger.warning(f"è¯»å–inverse annotationå¤±è´¥: {e}")
            
            if item_domain_list:
                item_domains[item_id] = list(set(item_domain_list))
                labels.append(item_data.get('label', 0))
                item_ids.append(item_id)
        
        if len(domains) == 0:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°åŸŸåæ•°æ®")
        
        # åˆ›å»ºåŸŸååˆ°ç´¢å¼•çš„æ˜ å°„
        domain_to_idx = {domain: idx for idx, domain in enumerate(domains)}
        
        # æ„å»ºåŸŸåç‰¹å¾ï¼ˆåŸºäºåŸŸåçš„ç®€å•ç‰¹å¾ï¼‰
        domain_features = []
        for domain in domains:
            feature = self._extract_domain_features(domain)
            domain_features.append(feature)
        
        domain_features = torch.tensor(domain_features, dtype=torch.float)
        
        # æ„å»ºè¾¹ï¼ˆåŸŸåç›¸ä¼¼æ€§ï¼‰
        edges = []
        edge_weights = []
        
        domain_list = list(domains)
        for i, domain1 in enumerate(domain_list):
            for j, domain2 in enumerate(domain_list):
                if i < j:
                    similarity = self._compute_domain_similarity(domain1, domain2)
                    if similarity > 0.1:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                        edges.append([i, j])
                        edges.append([j, i])  # æ— å‘å›¾
                        edge_weights.extend([similarity, similarity])
        
        if len(edges) == 0:
            # å¦‚æœæ²¡æœ‰ç›¸ä¼¼è¾¹ï¼Œåˆ›å»ºä¸€äº›åŸºæœ¬è¿æ¥
            print("âš ï¸  æ²¡æœ‰è¶³å¤Ÿçš„åŸŸåç›¸ä¼¼æ€§ï¼Œä½¿ç”¨åŸºæœ¬è¿æ¥")
            for i in range(min(len(domains), 5)):
                for j in range(i + 1, min(len(domains), 5)):
                    edges.append([i, j])
                    edges.append([j, i])
                    edge_weights.extend([0.5, 0.5])
        
        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.zeros((2, 0), dtype=torch.long)
        edge_weights = torch.tensor(edge_weights, dtype=torch.float) if edge_weights else torch.zeros(0)
        
        # åˆ›å»ºåŸŸåçº§åˆ«çš„æ ‡ç­¾ï¼ˆåŸºäºå…³è”çš„æ•°æ®é¡¹ï¼‰
        domain_labels = []
        for domain in domains:
            # æ‰¾åˆ°ä¸è¯¥åŸŸåå…³è”çš„æ•°æ®é¡¹çš„æ ‡ç­¾
            related_labels = []
            for item_id, item_domain_list in item_domains.items():
                if domain in item_domain_list:
                    item_idx = item_ids.index(item_id)
                    related_labels.append(labels[item_idx])
            
            if related_labels:
                # ä½¿ç”¨ä¼—æ•°ä½œä¸ºåŸŸåæ ‡ç­¾
                domain_label = max(set(related_labels), key=related_labels.count)
            else:
                domain_label = 0
            domain_labels.append(domain_label)
        
        # åˆ›å»ºå›¾æ•°æ®
        graph_data = Data(
            x=domain_features,
            edge_index=edge_index,
            edge_attr=edge_weights,
            y=torch.tensor(domain_labels, dtype=torch.long),
            num_nodes=len(domains)
        )
        
        # æ·»åŠ é¢å¤–ä¿¡æ¯
        graph_data.domains = list(domains)
        graph_data.domain_to_idx = domain_to_idx
        graph_data.item_domains = item_domains
        
        print(f"âœ… åŸŸåå›¾æ„å»ºå®Œæˆ:")
        print(f"   åŸŸåæ•°: {len(domains)}")
        print(f"   è¾¹æ•°: {edge_index.size(1)}")
        print(f"   æ•°æ®é¡¹æ•°: {len(item_ids)}")
        
        return graph_data
    
    def _compute_text_features(self, texts: List[str]) -> torch.Tensor:
        """è®¡ç®—æ–‡æœ¬ç‰¹å¾"""
        if self.text_processor:
            # ä½¿ç”¨é¡¹ç›®çš„æ–‡æœ¬å¤„ç†å™¨
            features = []
            for text in texts:
                text_features = self.text_processor.extract_features(text)
                # è½¬æ¢ä¸ºå‘é‡
                feature_vector = [
                    text_features.get('text_length', 0),
                    text_features.get('word_count', 0),
                    text_features.get('char_count', 0),
                    text_features.get('token_count', 0),
                    text_features.get('exclamation_count', 0),
                    text_features.get('question_count', 0),
                    text_features.get('uppercase_ratio', 0),
                    text_features.get('digit_count', 0),
                    text_features.get('url_count', 0),
                    text_features.get('mention_count', 0),
                    text_features.get('hashtag_count', 0),
                    text_features.get('emoji_count', 0)
                ]
                features.append(feature_vector)
        else:
            # ç®€å•çš„æ–‡æœ¬ç‰¹å¾
            features = []
            for text in texts:
                feature_vector = [
                    len(text),  # æ–‡æœ¬é•¿åº¦
                    len(text.split()),  # è¯æ•°
                    text.count('!'),  # æ„Ÿå¹å·æ•°
                    text.count('?'),  # é—®å·æ•°
                    sum(1 for c in text if c.isupper()) / max(len(text), 1),  # å¤§å†™æ¯”ä¾‹
                    sum(1 for c in text if c.isdigit()),  # æ•°å­—æ•°
                    text.count('http'),  # URLæ•°ï¼ˆç®€å•ï¼‰
                    text.count('@'),  # æåŠæ•°
                ]
                features.append(feature_vector)
        
        return torch.tensor(features, dtype=torch.float)
    
    def _compute_cosine_similarity(self, features: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ"""
        # L2æ ‡å‡†åŒ–
        features_norm = F.normalize(features, p=2, dim=1)
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.mm(features_norm, features_norm.t())
        return similarity_matrix
    
    def _build_edges_from_similarity(self, similarity_matrix: torch.Tensor, 
                                   threshold: float) -> Tuple[torch.Tensor, torch.Tensor]:
        """ä»ç›¸ä¼¼åº¦çŸ©é˜µæ„å»ºè¾¹"""
        num_nodes = similarity_matrix.size(0)
        edges = []
        edge_weights = []
        
        for i in range(num_nodes):
            # è·å–ä¸èŠ‚ç‚¹iæœ€ç›¸ä¼¼çš„èŠ‚ç‚¹
            similarities = similarity_matrix[i]
            
            # æ’é™¤è‡ªèº«
            similarities[i] = -1
            
            # æ‰¾åˆ°è¶…è¿‡é˜ˆå€¼çš„é‚»å±…
            valid_neighbors = torch.where(similarities > threshold)[0]
            
            # é™åˆ¶æ¯ä¸ªèŠ‚ç‚¹çš„è¾¹æ•°
            if len(valid_neighbors) > self.max_edges_per_node:
                _, top_indices = torch.topk(similarities, self.max_edges_per_node)
                valid_neighbors = top_indices[similarities[top_indices] > threshold]
            
            for j in valid_neighbors:
                edges.append([i, j.item()])
                edge_weights.append(similarities[j].item())
        
        if len(edges) == 0:
            # å¦‚æœæ²¡æœ‰æ»¡è¶³é˜ˆå€¼çš„è¾¹ï¼Œåˆ›å»ºä¸€äº›åŸºæœ¬è¿æ¥
            print("âš ï¸  æ²¡æœ‰æ»¡è¶³é˜ˆå€¼çš„è¾¹ï¼Œåˆ›å»ºåŸºæœ¬è¿æ¥")
            for i in range(min(num_nodes, 10)):
                j = (i + 1) % num_nodes
                edges.append([i, j])
                edge_weights.append(0.5)
        
        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
        edge_weights = torch.tensor(edge_weights, dtype=torch.float)
        
        return edge_index, edge_weights
    
    def _extract_domain_features(self, domain: str) -> List[float]:
        """æå–åŸŸåç‰¹å¾"""
        features = [
            len(domain),  # åŸŸåé•¿åº¦
            domain.count('.'),  # ç‚¹çš„æ•°é‡
            1.0 if 'com' in domain else 0.0,  # æ˜¯å¦åŒ…å«com
            1.0 if 'org' in domain else 0.0,  # æ˜¯å¦åŒ…å«org
            1.0 if 'net' in domain else 0.0,  # æ˜¯å¦åŒ…å«net
            1.0 if 'edu' in domain else 0.0,  # æ˜¯å¦åŒ…å«edu
            1.0 if 'gov' in domain else 0.0,  # æ˜¯å¦åŒ…å«gov
            1.0 if any(char.isdigit() for char in domain) else 0.0,  # æ˜¯å¦åŒ…å«æ•°å­—
        ]
        return features
    
    def _compute_domain_similarity(self, domain1: str, domain2: str) -> float:
        """è®¡ç®—åŸŸåç›¸ä¼¼åº¦"""
        # åŸºäºç¼–è¾‘è·ç¦»çš„ç›¸ä¼¼åº¦
        def edit_distance(s1, s2):
            if len(s1) < len(s2):
                return edit_distance(s2, s1)
            
            if len(s2) == 0:
                return len(s1)
            
            previous_row = list(range(len(s2) + 1))
            for i, c1 in enumerate(s1):
                current_row = [i + 1]
                for j, c2 in enumerate(s2):
                    insertions = previous_row[j + 1] + 1
                    deletions = current_row[j] + 1
                    substitutions = previous_row[j] + (c1 != c2)
                    current_row.append(min(insertions, deletions, substitutions))
                previous_row = current_row
            
            return previous_row[-1]
        
        max_len = max(len(domain1), len(domain2))
        if max_len == 0:
            return 1.0
        
        distance = edit_distance(domain1, domain2)
        similarity = 1.0 - (distance / max_len)
        
        return max(0.0, similarity)
    
    def save_graph(self, graph_data: Data, filename: str):
        """ä¿å­˜å›¾æ•°æ® - ä¿®å¤PyTorch 2.6å…¼å®¹æ€§"""
        save_path = self.output_dir / filename
        
        # è§£å†³PyTorch 2.6çš„weights_only=Trueé—®é¢˜
        try:
            # å°è¯•ä½¿ç”¨weights_only=Falseï¼ˆé€‚ç”¨äºå¯ä¿¡æ¥æºï¼‰
            torch.save(graph_data, save_path, _use_new_zipfile_serialization=False)
            print(f"ğŸ’¾ å›¾æ•°æ®å·²ä¿å­˜: {save_path}")
        except Exception as e:
            print(f"âš ï¸  PyTorchä¿å­˜å¤±è´¥ï¼Œå°è¯•æ›¿ä»£æ–¹æ¡ˆ: {e}")
            # ä½¿ç”¨å­—å…¸æ ¼å¼ä¿å­˜å…³é”®æ•°æ®
            graph_dict = {
                'x': graph_data.x,
                'edge_index': graph_data.edge_index,
                'edge_attr': graph_data.edge_attr if hasattr(graph_data, 'edge_attr') else None,
                'y': graph_data.y if hasattr(graph_data, 'y') else None,
                'num_nodes': graph_data.num_nodes,
                'metadata': {
                    'item_ids': getattr(graph_data, 'item_ids', None),
                    'texts': getattr(graph_data, 'texts', None),
                    'entities': getattr(graph_data, 'entities', None),
                    'domains': getattr(graph_data, 'domains', None)
                }
            }
            
            # ä¿å­˜ä¸ºpickleæ–‡ä»¶
            import pickle
            pickle_path = save_path.with_suffix('.pkl')
            with open(pickle_path, 'wb') as f:
                pickle.dump(graph_dict, f)
            print(f"ğŸ’¾ å›¾æ•°æ®å·²ä¿å­˜ä¸ºpickle: {pickle_path}")
    
    def load_graph(self, filename: str) -> Data:
        """åŠ è½½å›¾æ•°æ® - ä¿®å¤PyTorch 2.6å…¼å®¹æ€§"""
        load_path = self.output_dir / filename
        
        if not load_path.exists():
            # å°è¯•æ‰¾pickleç‰ˆæœ¬
            pickle_path = load_path.with_suffix('.pkl')
            if pickle_path.exists():
                print(f"ğŸ“‚ åŠ è½½pickleæ ¼å¼å›¾æ•°æ®: {pickle_path}")
                import pickle
                with open(pickle_path, 'rb') as f:
                    graph_dict = pickle.load(f)
                
                # é‡å»ºDataå¯¹è±¡
                graph_data = Data(
                    x=graph_dict['x'],
                    edge_index=graph_dict['edge_index'],
                    edge_attr=graph_dict['edge_attr'],
                    y=graph_dict['y'],
                    num_nodes=graph_dict['num_nodes']
                )
                
                # æ·»åŠ metadata
                metadata = graph_dict.get('metadata', {})
                for key, value in metadata.items():
                    if value is not None:
                        setattr(graph_data, key, value)
                
                return graph_data
            else:
                raise FileNotFoundError(f"å›¾æ–‡ä»¶ä¸å­˜åœ¨: {load_path} æˆ– {pickle_path}")
        
        try:
            # å°è¯•ä½¿ç”¨weights_only=FalseåŠ è½½
            graph_data = torch.load(load_path, weights_only=False)
            print(f"ğŸ“‚ å›¾æ•°æ®å·²åŠ è½½: {load_path}")
            return graph_data
        except Exception as e:
            print(f"âš ï¸  PyTorchåŠ è½½å¤±è´¥: {e}")
            raise


class GraphFeatureExtractor:
    """å›¾ç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å›¾ç‰¹å¾æå–å™¨"""
        pass
    
    def extract_node_features(self, graph_data: Data) -> Dict[str, torch.Tensor]:
        """
        æå–èŠ‚ç‚¹ç‰¹å¾
        
        Args:
            graph_data: å›¾æ•°æ®
            
        Returns:
            ç‰¹å¾å­—å…¸
        """
        features = {}
        
        # è½¬æ¢ä¸ºNetworkXå›¾ä»¥ä¾¿è®¡ç®—å›¾ç‰¹å¾
        nx_graph = to_networkx(graph_data, to_undirected=True)
        
        # åº¦ä¸­å¿ƒæ€§
        degree_centrality = nx.degree_centrality(nx_graph)
        features['degree_centrality'] = torch.tensor(
            [degree_centrality.get(i, 0.0) for i in range(graph_data.num_nodes)],
            dtype=torch.float
        )
        
        # ä»‹æ•°ä¸­å¿ƒæ€§
        try:
            betweenness_centrality = nx.betweenness_centrality(nx_graph)
            features['betweenness_centrality'] = torch.tensor(
                [betweenness_centrality.get(i, 0.0) for i in range(graph_data.num_nodes)],
                dtype=torch.float
            )
        except:
            features['betweenness_centrality'] = torch.zeros(graph_data.num_nodes)
        
        # ç´§å¯†æ€§ä¸­å¿ƒæ€§
        try:
            closeness_centrality = nx.closeness_centrality(nx_graph)
            features['closeness_centrality'] = torch.tensor(
                [closeness_centrality.get(i, 0.0) for i in range(graph_data.num_nodes)],
                dtype=torch.float
            )
        except:
            features['closeness_centrality'] = torch.zeros(graph_data.num_nodes)
        
        # èŠ‚ç‚¹åº¦
        degrees = torch.zeros(graph_data.num_nodes)
        edge_index = graph_data.edge_index
        for i in range(graph_data.num_nodes):
            degrees[i] = (edge_index[0] == i).sum() + (edge_index[1] == i).sum()
        features['degree'] = degrees
        
        # èšç±»ç³»æ•°
        try:
            clustering = nx.clustering(nx_graph)
            features['clustering'] = torch.tensor(
                [clustering.get(i, 0.0) for i in range(graph_data.num_nodes)],
                dtype=torch.float
            )
        except:
            features['clustering'] = torch.zeros(graph_data.num_nodes)
        
        return features
    
    def extract_graph_features(self, graph_data: Data) -> Dict[str, float]:
        """
        æå–å›¾çº§ç‰¹å¾
        
        Args:
            graph_data: å›¾æ•°æ®
            
        Returns:
            ç‰¹å¾å­—å…¸
        """
        features = {}
        
        # è½¬æ¢ä¸ºNetworkXå›¾
        nx_graph = to_networkx(graph_data, to_undirected=True)
        
        # åŸºæœ¬ç»Ÿè®¡
        features['num_nodes'] = graph_data.num_nodes
        features['num_edges'] = graph_data.edge_index.size(1) // 2  # æ— å‘å›¾
        features['density'] = nx.density(nx_graph)
        
        # è¿é€šæ€§
        features['num_connected_components'] = nx.number_connected_components(nx_graph)
        features['is_connected'] = float(nx.is_connected(nx_graph))
        
        # åº¦åˆ†å¸ƒç»Ÿè®¡
        degrees = [nx_graph.degree(n) for n in nx_graph.nodes()]
        if degrees:
            features['avg_degree'] = np.mean(degrees)
            features['max_degree'] = np.max(degrees)
            features['min_degree'] = np.min(degrees)
            features['degree_std'] = np.std(degrees)
        else:
            features['avg_degree'] = 0.0
            features['max_degree'] = 0.0
            features['min_degree'] = 0.0
            features['degree_std'] = 0.0
        
        # è·¯å¾„ç›¸å…³ç‰¹å¾
        try:
            if nx.is_connected(nx_graph):
                features['avg_shortest_path_length'] = nx.average_shortest_path_length(nx_graph)
                features['diameter'] = nx.diameter(nx_graph)
                features['radius'] = nx.radius(nx_graph)
            else:
                # å¯¹äºéè¿é€šå›¾ï¼Œä½¿ç”¨æœ€å¤§è¿é€šåˆ†é‡
                largest_cc = max(nx.connected_components(nx_graph), key=len)
                subgraph = nx_graph.subgraph(largest_cc)
                features['avg_shortest_path_length'] = nx.average_shortest_path_length(subgraph)
                features['diameter'] = nx.diameter(subgraph)
                features['radius'] = nx.radius(subgraph)
        except:
            features['avg_shortest_path_length'] = 0.0
            features['diameter'] = 0.0
            features['radius'] = 0.0
        
        # èšç±»ç³»æ•°
        try:
            features['avg_clustering'] = nx.average_clustering(nx_graph)
            features['transitivity'] = nx.transitivity(nx_graph)
        except:
            features['avg_clustering'] = 0.0
            features['transitivity'] = 0.0
        
        # å°ä¸–ç•Œç‰¹æ€§
        try:
            if nx.is_connected(nx_graph) and len(nx_graph) > 3:
                random_graph = nx.erdos_renyi_graph(len(nx_graph), features['density'])
                if nx.is_connected(random_graph):
                    actual_clustering = features['avg_clustering']
                    random_clustering = nx.average_clustering(random_graph)
                    actual_path_length = features['avg_shortest_path_length']
                    random_path_length = nx.average_shortest_path_length(random_graph)
                    
                    if random_clustering > 0 and random_path_length > 0:
                        features['small_world_sigma'] = (actual_clustering / random_clustering) / (actual_path_length / random_path_length)
                    else:
                        features['small_world_sigma'] = 0.0
                else:
                    features['small_world_sigma'] = 0.0
            else:
                features['small_world_sigma'] = 0.0
        except:
            features['small_world_sigma'] = 0.0
        
        return features
    
    def augment_node_features(self, graph_data: Data) -> Data:
        """
        ç”¨å›¾ç‰¹å¾å¢å¼ºèŠ‚ç‚¹ç‰¹å¾
        
        Args:
            graph_data: åŸå§‹å›¾æ•°æ®
            
        Returns:
            å¢å¼ºåçš„å›¾æ•°æ®
        """
        # æå–å›¾ç‰¹å¾
        node_features = self.extract_node_features(graph_data)
        
        # å°†å›¾ç‰¹å¾ä¸åŸå§‹ç‰¹å¾æ‹¼æ¥
        original_features = graph_data.x
        augmented_features = [original_features]
        
        for feature_name, feature_values in node_features.items():
            # ç¡®ä¿ç‰¹å¾ç»´åº¦åŒ¹é…
            if feature_values.size(0) == graph_data.num_nodes:
                augmented_features.append(feature_values.unsqueeze(1))
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        graph_data.x = torch.cat(augmented_features, dim=1)
        
        print(f"âœ… èŠ‚ç‚¹ç‰¹å¾å¢å¼ºå®Œæˆ: {original_features.size(1)} -> {graph_data.x.size(1)}")
        
        return graph_data


def create_multi_layer_graph(data: Dict[str, Any], graph_builder: SocialGraphBuilder) -> Dict[str, Data]:
    """
    åˆ›å»ºå¤šå±‚å›¾ç»“æ„
    
    Args:
        data: MR2æ•°æ®
        graph_builder: å›¾æ„å»ºå™¨
        
    Returns:
        å¤šå±‚å›¾å­—å…¸
    """
    print("ğŸ—ï¸  åˆ›å»ºå¤šå±‚å›¾ç»“æ„...")
    
    graphs = {}
    
    try:
        # æ–‡æœ¬ç›¸ä¼¼åº¦å›¾
        print("\n1. æ„å»ºæ–‡æœ¬ç›¸ä¼¼åº¦å›¾...")
        text_graph = graph_builder.build_text_similarity_graph(data, similarity_threshold=0.2)
        graphs['text_similarity'] = text_graph
    except Exception as e:
        print(f"âŒ æ–‡æœ¬ç›¸ä¼¼åº¦å›¾æ„å»ºå¤±è´¥: {e}")
        graphs['text_similarity'] = None
    
    try:
        # å®ä½“å…±ç°å›¾
        print("\n2. æ„å»ºå®ä½“å…±ç°å›¾...")
        entity_graph = graph_builder.build_entity_cooccurrence_graph(data)
        graphs['entity_cooccurrence'] = entity_graph
    except Exception as e:
        print(f"âŒ å®ä½“å…±ç°å›¾æ„å»ºå¤±è´¥: {e}")
        graphs['entity_cooccurrence'] = None
    
    try:
        # åŸŸåå›¾
        print("\n3. æ„å»ºåŸŸåå›¾...")
        domain_graph = graph_builder.build_domain_graph(data)
        graphs['domain'] = domain_graph
    except Exception as e:
        print(f"âŒ åŸŸåå›¾æ„å»ºå¤±è´¥: {e}")
        graphs['domain'] = None
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š å¤šå±‚å›¾æ„å»ºå®Œæˆ:")
    for graph_name, graph_data in graphs.items():
        if graph_data is not None:
            print(f"   {graph_name}: {graph_data.num_nodes} èŠ‚ç‚¹, {graph_data.edge_index.size(1)} è¾¹")
        else:
            print(f"   {graph_name}: æ„å»ºå¤±è´¥")
    
    return graphs


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ”— æµ‹è¯•ç¤¾äº¤å›¾æ„å»ºæ¨¡å—")
    
    # åˆ›å»ºå›¾æ„å»ºå™¨
    try:
        builder = SocialGraphBuilder()
        
        # åŠ è½½æ•°æ®
        print("\nğŸ“š åŠ è½½MR2æ•°æ®...")
        train_data = builder.load_mr2_data('train')
        
        # é™åˆ¶æ•°æ®é‡ä»¥ä¾¿æµ‹è¯•
        limited_data = dict(list(train_data.items())[:50])  # åªä½¿ç”¨å‰50æ¡æ•°æ®
        print(f"ä½¿ç”¨ {len(limited_data)} æ¡æ•°æ®è¿›è¡Œæµ‹è¯•")
        
        # åˆ›å»ºå¤šå±‚å›¾
        print("\nğŸ—ï¸  åˆ›å»ºå¤šå±‚å›¾ç»“æ„...")
        graphs = create_multi_layer_graph(limited_data, builder)
        
        # æµ‹è¯•ç‰¹å¾æå–
        feature_extractor = GraphFeatureExtractor()
        
        for graph_name, graph_data in graphs.items():
            if graph_data is not None:
                print(f"\nğŸ“Š åˆ†æ {graph_name} å›¾:")
                
                # æå–å›¾çº§ç‰¹å¾
                graph_features = feature_extractor.extract_graph_features(graph_data)
                print(f"   å›¾çº§ç‰¹å¾:")
                for feature_name, value in list(graph_features.items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªç‰¹å¾
                    print(f"     {feature_name}: {value:.4f}")
                
                # å¢å¼ºèŠ‚ç‚¹ç‰¹å¾
                try:
                    augmented_graph = feature_extractor.augment_node_features(graph_data)
                    print(f"   èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {graph_data.x.size(1)} -> {augmented_graph.x.size(1)}")
                except Exception as e:
                    print(f"   èŠ‚ç‚¹ç‰¹å¾å¢å¼ºå¤±è´¥: {e}")
                
                # ä¿å­˜å›¾
                try:
                    filename = f"{graph_name}_graph.pt"
                    builder.save_graph(graph_data, filename)
                except Exception as e:
                    print(f"   ä¿å­˜å›¾å¤±è´¥: {e}")
        
        # æµ‹è¯•å›¾åŠ è½½
        print(f"\nğŸ’¾ æµ‹è¯•å›¾åŠ è½½...")
        try:
            for graph_name in graphs.keys():
                if graphs[graph_name] is not None:
                    filename = f"{graph_name}_graph.pt"
                    loaded_graph = builder.load_graph(filename)
                    print(f"   âœ… {graph_name} å›¾åŠ è½½æˆåŠŸ: {loaded_graph.num_nodes} èŠ‚ç‚¹")
                    break  # åªæµ‹è¯•ä¸€ä¸ª
        except Exception as e:
            print(f"   âŒ å›¾åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        
        print(f"\nâœ… ç¤¾äº¤å›¾æ„å»ºæ¨¡å—æµ‹è¯•å®Œæˆ")
        print(f"   è¾“å‡ºç›®å½•: {builder.output_dir}")
        
    except Exception as e:
        print(f"âŒ ç¤¾äº¤å›¾æ„å»ºæµ‹è¯•å¤±è´¥: {e}")
        
        # åˆ›å»ºæ¼”ç¤ºå›¾æ•°æ®
        print("\nğŸ”§ åˆ›å»ºæ¼”ç¤ºå›¾æ•°æ®...")
        
        # ç®€å•çš„æ¼”ç¤ºå›¾
        num_nodes = 20
        edge_index = torch.tensor([
            [i for i in range(num_nodes-1)] + [i for i in range(1, num_nodes)],
            [i for i in range(1, num_nodes)] + [i for i in range(num_nodes-1)]
        ], dtype=torch.long)
        
        node_features = torch.randn(num_nodes, 8)
        labels = torch.randint(0, 3, (num_nodes,))
        
        demo_graph = Data(
            x=node_features,
            edge_index=edge_index,
            y=labels,
            num_nodes=num_nodes
        )
        
        print(f"âœ… æ¼”ç¤ºå›¾åˆ›å»ºæˆåŠŸ:")
        print(f"   èŠ‚ç‚¹æ•°: {demo_graph.num_nodes}")
        print(f"   è¾¹æ•°: {demo_graph.edge_index.size(1)}")
        print(f"   ç‰¹å¾ç»´åº¦: {demo_graph.x.size(1)}")
        
        # æµ‹è¯•ç‰¹å¾æå–
        feature_extractor = GraphFeatureExtractor()
        
        print(f"\nğŸ“Š æµ‹è¯•ç‰¹å¾æå–...")
        try:
            graph_features = feature_extractor.extract_graph_features(demo_graph)
            print(f"   å›¾çº§ç‰¹å¾æ•°: {len(graph_features)}")
            
            augmented_graph = feature_extractor.augment_node_features(demo_graph)
            print(f"   å¢å¼ºåç‰¹å¾ç»´åº¦: {augmented_graph.x.size(1)}")
            
        except Exception as e:
            print(f"   âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
    
    print("\nâœ… ç¤¾äº¤å›¾æ„å»ºæ¨¡å—æµ‹è¯•å®Œæˆ")