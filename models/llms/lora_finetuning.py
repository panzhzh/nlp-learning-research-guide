#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# models/llms/lora_finetuning.py

"""
åŸºäº Qwen3-0.6B çš„ LoRA å¾®è°ƒæ¨¡å—
ä¸“é—¨ç”¨äºè°£è¨€æ£€æµ‹ä»»åŠ¡çš„å‚æ•°é«˜æ•ˆå¾®è°ƒ
æ”¯æŒå¤šç§å¾®è°ƒç­–ç•¥å’Œè¯„ä¼°åŠŸèƒ½
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer,
    DataCollatorForLanguageModeling,
    get_linear_schedule_with_warmup
)
from peft import (
    LoraConfig, get_peft_model, TaskType,
    PeftModel, prepare_model_for_kbit_training
)
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, classification_report
from tqdm import tqdm
import json
import warnings
from typing import Dict, List, Tuple, Any, Optional, Union
from pathlib import Path
import sys
import logging
from datetime import datetime
import os

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

print(project_root)
# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from data_utils.data_loaders import create_all_dataloaders
    from utils.config_manager import get_config_manager, get_output_path
    from models.llms.prompt_engineering import PromptManager
    USE_PROJECT_MODULES = True
    print("âœ… æˆåŠŸå¯¼å…¥é¡¹ç›®æ¨¡å—")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥é¡¹ç›®æ¨¡å—å¤±è´¥: {e}")
    USE_PROJECT_MODULES = False


class RumorDetectionDataset(Dataset):
    """è°£è¨€æ£€æµ‹æ•°æ®é›†ç±»"""
    
    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 512):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨
            tokenizer: åˆ†è¯å™¨
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.label_mapping = {0: 'Non-rumor', 1: 'Rumor', 2: 'Unverified'}
        
        # åˆ›å»ºæç¤ºæ¨¡æ¿
        self.prompt_template = "è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬æ˜¯å¦ä¸ºè°£è¨€ã€‚\n\næ–‡æœ¬: {text}\n\nè¯·ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©:\n- Non-rumor: éè°£è¨€\n- Rumor: è°£è¨€\n- Unverified: æœªéªŒè¯\n\nç­”æ¡ˆ: {label}"
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        label_text = self.label_mapping[label]
        
        # åˆ›å»ºå®Œæ•´çš„æç¤º
        prompt = self.prompt_template.format(text=text, label=label_text)
        
        # åˆ†è¯
        encoding = self.tokenizer(
            prompt,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['input_ids'].flatten().clone()
        }


class QwenLoRATrainer:
    """Qwen LoRA å¾®è°ƒè®­ç»ƒå™¨"""
    
    def __init__(self, 
                 model_name: str = "Qwen/Qwen3-0.6B",
                 max_length: int = 512,
                 device: str = "auto"):
        """
        åˆå§‹åŒ– LoRA è®­ç»ƒå™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            device: è®¡ç®—è®¾å¤‡
        """
        self.model_name = model_name
        self.max_length = max_length
        
        # è®¾ç½®è®¾å¤‡
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"ğŸ¤– æ¨¡å‹: {model_name}")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.tokenizer = None
        self.base_model = None
        self.peft_model = None
        self.lora_config = None
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if USE_PROJECT_MODULES:
            config_manager = get_config_manager()
            self.output_dir = get_output_path('models', 'llms') / 'lora_checkpoints'
        else:
            self.output_dir = Path('outputs/models/llms/lora_checkpoints')
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        self._load_model_and_tokenizer()
    
    def _load_model_and_tokenizer(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        try:
            print("ğŸ“¥ åŠ è½½åˆ†è¯å™¨...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                pad_token='<|extra_0|>',
                eos_token='<|im_end|>',
                use_fast=False
            )
            
            # ç¡®ä¿æœ‰pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("ğŸ“¥ åŠ è½½åŸºç¡€æ¨¡å‹...")
            self.base_model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32,
                device_map="auto" if self.device.type == 'cuda' else None
            )
            
            # å¦‚æœä¸æ˜¯è‡ªåŠ¨è®¾å¤‡æ˜ å°„ï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
            if self.device.type != 'cuda':
                self.base_model = self.base_model.to(self.device)
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   å‚æ•°é‡: {sum(p.numel() for p in self.base_model.parameters()):,}")
            print(f"   è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer)}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def setup_lora_config(self,
                         r: int = 16,
                         lora_alpha: int = 32,
                         lora_dropout: float = 0.1,
                         target_modules: Optional[List[str]] = None,
                         bias: str = "none",
                         task_type: str = "CAUSAL_LM"):
        """
        è®¾ç½® LoRA é…ç½®
        
        Args:
            r: LoRA rank
            lora_alpha: LoRA alpha
            lora_dropout: LoRA dropout
            target_modules: ç›®æ ‡æ¨¡å—
            bias: åç½®å¤„ç†æ–¹å¼
            task_type: ä»»åŠ¡ç±»å‹
        """
        if target_modules is None:
            # Qwen3 æ¨¡å‹çš„æ³¨æ„åŠ›æ¨¡å—
            target_modules = [
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ]
        
        self.lora_config = LoraConfig(
            task_type=getattr(TaskType, task_type),
            r=r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            target_modules=target_modules,
            bias=bias,
            inference_mode=False
        )
        
        print(f"ğŸ”§ LoRA é…ç½®:")
        print(f"   Rank (r): {r}")
        print(f"   Alpha: {lora_alpha}")
        print(f"   Dropout: {lora_dropout}")
        print(f"   ç›®æ ‡æ¨¡å—: {target_modules}")
        
        return self.lora_config
    
    def create_peft_model(self):
        """åˆ›å»º PEFT æ¨¡å‹"""
        if self.lora_config is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ setup_lora_config() è®¾ç½® LoRA é…ç½®")
        
        try:
            print("ğŸ”§ åˆ›å»º PEFT æ¨¡å‹...")
            
            # å‡†å¤‡æ¨¡å‹è¿›è¡Œé‡åŒ–è®­ç»ƒï¼ˆå¦‚æœéœ€è¦ï¼‰
            if hasattr(self.base_model, 'gradient_checkpointing_enable'):
                self.base_model.gradient_checkpointing_enable()
            
            # åˆ›å»º PEFT æ¨¡å‹
            self.peft_model = get_peft_model(self.base_model, self.lora_config)
            
            # ç»Ÿè®¡å‚æ•°
            total_params = sum(p.numel() for p in self.peft_model.parameters())
            trainable_params = sum(p.numel() for p in self.peft_model.parameters() if p.requires_grad)
            
            print(f"âœ… PEFT æ¨¡å‹åˆ›å»ºæˆåŠŸ")
            print(f"   æ€»å‚æ•°: {total_params:,}")
            print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
            print(f"   å¯è®­ç»ƒæ¯”ä¾‹: {100 * trainable_params / total_params:.2f}%")
            
            return self.peft_model
            
        except Exception as e:
            print(f"âŒ PEFT æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            raise
    
    def prepare_datasets(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®é›†"""
        try:
            print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®é›†...")
            
            if USE_PROJECT_MODULES:
                # ç›´æ¥è¯»å–JSONæ–‡ä»¶ï¼Œä½¿ç”¨å…¨é‡è®­ç»ƒæ•°æ®
                from utils.config_manager import get_data_dir
                data_dir = get_data_dir()
                
                datasets = {}
                
                # è¯»å–è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ–‡ä»¶
                for split in ['train', 'test']:
                    file_path = data_dir / f'dataset_items_{split}.json'
                    
                    if file_path.exists():
                        print(f"ğŸ“„ è¯»å– {split} æ•°æ®æ–‡ä»¶: {file_path}")
                        
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        
                        texts = []
                        labels = []
                        
                        # æå–captionå’Œlabelå­—æ®µ
                        for item_id, item_data in data.items():
                            caption = item_data.get('caption', '')
                            label = item_data.get('label', 0)
                            
                            if caption and caption.strip():  # ç¡®ä¿captionä¸ä¸ºç©º
                                texts.append(caption.strip())
                                labels.append(int(label))
                        
                        # åˆ›å»ºæ•°æ®é›†
                        datasets[split] = RumorDetectionDataset(
                            texts=texts,
                            labels=labels,
                            tokenizer=self.tokenizer,
                            max_length=self.max_length
                        )
                        
                        print(f"   {split}: {len(datasets[split])} æ ·æœ¬")
                        
                        # æ‰“å°æ ‡ç­¾åˆ†å¸ƒ
                        label_counts = {}
                        for label in labels:
                            label_counts[label] = label_counts.get(label, 0) + 1
                        print(f"   æ ‡ç­¾åˆ†å¸ƒ: {label_counts}")
                    
                    else:
                        print(f"âš ï¸  æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                
                # å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œä»è®­ç»ƒé›†ä¸­åˆ†å‰²ä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯é›†
                if 'train' in datasets and 'val' not in datasets:
                    train_dataset = datasets['train']
                    train_size = len(train_dataset)
                    val_size = min(100, train_size // 10)  # éªŒè¯é›†å¤§å°ä¸ºè®­ç»ƒé›†çš„10%æˆ–100ä¸ªæ ·æœ¬
                    
                    # ç®€å•åˆ†å‰²
                    val_texts = train_dataset.texts[:val_size]
                    val_labels = train_dataset.labels[:val_size]
                    
                    datasets['val'] = RumorDetectionDataset(
                        texts=val_texts,
                        labels=val_labels,
                        tokenizer=self.tokenizer,
                        max_length=self.max_length
                    )
                    
                    # æ›´æ–°è®­ç»ƒé›†ï¼ˆç§»é™¤éªŒè¯é›†éƒ¨åˆ†ï¼‰
                    train_texts = train_dataset.texts[val_size:]
                    train_labels = train_dataset.labels[val_size:]
                    
                    datasets['train'] = RumorDetectionDataset(
                        texts=train_texts,
                        labels=train_labels,
                        tokenizer=self.tokenizer,
                        max_length=self.max_length
                    )
                    
                    print(f"   ä»è®­ç»ƒé›†åˆ†å‰²éªŒè¯é›†: {len(datasets['val'])} æ ·æœ¬")
                    print(f"   æ›´æ–°åè®­ç»ƒé›†: {len(datasets['train'])} æ ·æœ¬")
            
            else:
                # ä½¿ç”¨æ¼”ç¤ºæ•°æ®
                demo_data = {
                    'train': {
                        'texts': [
                            "ç§‘å­¦å®¶åœ¨å®éªŒå®¤å‘ç°äº†æ–°çš„æ²»ç–—æ–¹æ³•ï¼Œç»è¿‡ä¸¥æ ¼çš„ä¸´åºŠè¯•éªŒè¯å®æœ‰æ•ˆ",
                            "ç½‘ä¼ æŸåœ°å‘ç”Ÿé‡å¤§åœ°éœ‡ï¼Œä½†å®˜æ–¹æ°”è±¡å±€å°šæœªå‘å¸ƒç›¸å…³ä¿¡æ¯", 
                            "è°£ä¼ æ–°å† ç–«è‹—å«æœ‰å¾®èŠ¯ç‰‡ï¼Œè¿™ä¸€è¯´æ³•å·²è¢«å¤šé¡¹ç§‘å­¦ç ”ç©¶è¯æ˜ä¸ºè™šå‡ä¿¡æ¯",
                            "æ•™è‚²éƒ¨æ­£å¼å‘å¸ƒæ–°çš„é«˜è€ƒæ”¹é©æ–¹æ¡ˆï¼Œå°†äºæ˜å¹´å¼€å§‹å®æ–½",
                            "æ®ä¸å®Œå…¨ç»Ÿè®¡ï¼Œæ–°äº§å“åœ¨å¸‚åœºä¸Šåå“è‰¯å¥½",
                            "ä¸–ç•Œå«ç”Ÿç»„ç»‡ç¡®è®¤æ–°å† ç–«è‹—å¯¹å˜å¼‚æ ªä»ç„¶æœ‰æ•ˆ"
                        ],
                        'labels': [0, 2, 1, 0, 2, 0]
                    },
                    'val': {
                        'texts': [
                            "ä¸­å›½ç§‘å­¦é™¢å‘å¸ƒæœ€æ–°ç ”ç©¶æˆæœï¼Œåœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå–å¾—é‡å¤§çªç ´",
                            "ç½‘ä¸Šæµä¼ æŸæ˜æ˜Ÿæ¶‰å«Œè¿æ³•çŠ¯ç½ªï¼Œä½†å½“äº‹äººå·²å‘å£°æ˜è¾Ÿè°£"
                        ],
                        'labels': [0, 1]
                    },
                    'test': {
                        'texts': [
                            "æ”¿åºœéƒ¨é—¨å‘å¸ƒå®˜æ–¹å£°æ˜ï¼Œæ¾„æ¸…ç½‘ç»œä¼ è¨€",
                            "ä¸šå†…äººå£«é€éœ²ï¼ŒæŸè¡Œä¸šå¯èƒ½é¢ä¸´é‡å¤§æ”¿ç­–è°ƒæ•´"
                        ],
                        'labels': [0, 2]
                    }
                }
                
                datasets = {}
                for split in ['train', 'val', 'test']:
                    datasets[split] = RumorDetectionDataset(
                        texts=demo_data[split]['texts'],
                        labels=demo_data[split]['labels'],
                        tokenizer=self.tokenizer,
                        max_length=self.max_length
                    )
                    print(f"   {split}: {len(datasets[split])} æ ·æœ¬")
            
            print("âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ")
            return datasets
            
        except Exception as e:
            print(f"âŒ æ•°æ®é›†å‡†å¤‡å¤±è´¥: {e}")
            raise
    
    def create_training_arguments(self,
                                output_dir: Optional[str] = None,
                                num_train_epochs: int = 3,
                                per_device_train_batch_size: int = 4,
                                per_device_eval_batch_size: int = 8,
                                learning_rate: float = 2e-4,
                                warmup_steps: int = 100,
                                logging_steps: int = 10,
                                save_steps: int = 500,
                                eval_steps: int = 500,
                                save_total_limit: int = 2,
                                load_best_model_at_end: bool = True,
                                metric_for_best_model: str = "eval_loss",
                                greater_is_better: bool = False):
        """
        åˆ›å»ºè®­ç»ƒå‚æ•°
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            num_train_epochs: è®­ç»ƒè½®æ•°
            per_device_train_batch_size: è®­ç»ƒæ‰¹æ¬¡å¤§å°
            per_device_eval_batch_size: è¯„ä¼°æ‰¹æ¬¡å¤§å°
            learning_rate: å­¦ä¹ ç‡
            warmup_steps: é¢„çƒ­æ­¥æ•°
            logging_steps: æ—¥å¿—è®°å½•æ­¥æ•°
            save_steps: ä¿å­˜æ­¥æ•°
            eval_steps: è¯„ä¼°æ­¥æ•°
            save_total_limit: æœ€å¤§ä¿å­˜æ£€æŸ¥ç‚¹æ•°
            load_best_model_at_end: æ˜¯å¦åœ¨ç»“æŸæ—¶åŠ è½½æœ€ä½³æ¨¡å‹
            metric_for_best_model: æœ€ä½³æ¨¡å‹è¯„ä¼°æŒ‡æ ‡
            greater_is_better: æŒ‡æ ‡æ˜¯å¦è¶Šå¤§è¶Šå¥½
            
        Returns:
            TrainingArguments å¯¹è±¡
        """
        if output_dir is None:
            output_dir = self.output_dir / f"qwen_lora_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        training_args = TrainingArguments(
            output_dir=str(output_dir),
            num_train_epochs=num_train_epochs,
            per_device_train_batch_size=per_device_train_batch_size,
            per_device_eval_batch_size=per_device_eval_batch_size,
            learning_rate=learning_rate,
            warmup_steps=warmup_steps,
            logging_steps=logging_steps,
            save_steps=save_steps,
            eval_steps=eval_steps,
            eval_strategy="steps",  # ä¿®å¤: ä½¿ç”¨ eval_strategy è€Œä¸æ˜¯ evaluation_strategy
            save_strategy="steps",
            save_total_limit=save_total_limit,
            load_best_model_at_end=load_best_model_at_end,
            metric_for_best_model=metric_for_best_model,
            greater_is_better=greater_is_better,
            remove_unused_columns=False,
            report_to=None,  # ç¦ç”¨ wandb ç­‰å¤–éƒ¨æŠ¥å‘Š
            dataloader_pin_memory=False,
            fp16=True if self.device.type == 'cuda' else False,
            gradient_checkpointing=True
        )
        
        print(f"ğŸ”§ è®­ç»ƒå‚æ•°é…ç½®:")
        print(f"   è¾“å‡ºç›®å½•: {output_dir}")
        print(f"   è®­ç»ƒè½®æ•°: {num_train_epochs}")
        print(f"   è®­ç»ƒæ‰¹æ¬¡å¤§å°: {per_device_train_batch_size}")
        print(f"   å­¦ä¹ ç‡: {learning_rate}")
        print(f"   é¢„çƒ­æ­¥æ•°: {warmup_steps}")
        
        return training_args
    
    def train(self,
              train_dataset,
              eval_dataset,
              training_args,
              compute_metrics_fn=None):
        """
        å¼€å§‹è®­ç»ƒ
        
        Args:
            train_dataset: è®­ç»ƒæ•°æ®é›†
            eval_dataset: éªŒè¯æ•°æ®é›†
            training_args: è®­ç»ƒå‚æ•°
            compute_metrics_fn: æŒ‡æ ‡è®¡ç®—å‡½æ•°
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        if self.peft_model is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ create_peft_model() åˆ›å»º PEFT æ¨¡å‹")
        
        try:
            print("ğŸš€ å¼€å§‹ LoRA å¾®è°ƒè®­ç»ƒ...")
            
            # åˆ›å»ºæ•°æ®æ•´ç†å™¨
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False,
                pad_to_multiple_of=8
            )
            
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = Trainer(
                model=self.peft_model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                data_collator=data_collator,
                compute_metrics=compute_metrics_fn
            )
            
            # å¼€å§‹è®­ç»ƒ
            train_result = trainer.train()
            
            # ä¿å­˜æ¨¡å‹
            trainer.save_model()
            
            print("âœ… LoRA å¾®è°ƒè®­ç»ƒå®Œæˆ")
            print(f"   è®­ç»ƒæŸå¤±: {train_result.training_loss:.4f}")
            print(f"   è®­ç»ƒæ­¥æ•°: {train_result.global_step}")
            
            return train_result
            
        except Exception as e:
            print(f"âŒ LoRA å¾®è°ƒè®­ç»ƒå¤±è´¥: {e}")
            raise
    
    def evaluate_model(self, test_dataset, model_path: Optional[str] = None):
        """
        è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹
        
        Args:
            test_dataset: æµ‹è¯•æ•°æ®é›†
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è¯„ä¼°ç»“æœ
        """
        try:
            print("ğŸ“Š è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹...")
            
            # å¦‚æœæŒ‡å®šäº†æ¨¡å‹è·¯å¾„ï¼ŒåŠ è½½æ¨¡å‹
            if model_path:
                model = PeftModel.from_pretrained(self.base_model, model_path)
            else:
                model = self.peft_model
            
            if model is None:
                raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°")
            
            model.eval()
            
            predictions = []
            true_labels = []
            
            # é€ä¸ªæ ·æœ¬è¿›è¡Œé¢„æµ‹ï¼ˆé¿å…æ‰¹å¤„ç†çš„å¤æ‚æ€§ï¼‰
            print(f"å¼€å§‹è¯„ä¼° {len(test_dataset)} ä¸ªæµ‹è¯•æ ·æœ¬...")
            
            for i in tqdm(range(len(test_dataset)), desc="è¯„ä¼°ä¸­"):
                try:
                    # è·å–å•ä¸ªæ ·æœ¬
                    sample = test_dataset[i]
                    input_ids = sample['input_ids'].unsqueeze(0).to(self.device)
                    attention_mask = sample['attention_mask'].unsqueeze(0).to(self.device)
                    
                    # æ„å»ºåŸå§‹æç¤ºï¼ˆä¸åŒ…å«ç­”æ¡ˆï¼‰
                    text = test_dataset.texts[i]
                    prompt = f"è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬æ˜¯å¦ä¸ºè°£è¨€ã€‚\n\næ–‡æœ¬: {text}\n\nè¯·ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©:\n- Non-rumor: éè°£è¨€\n- Rumor: è°£è¨€\n- Unverified: æœªéªŒè¯\n\nç­”æ¡ˆ: "
                    
                    # é‡æ–°ç¼–ç æç¤ºï¼ˆä¸åŒ…å«ç­”æ¡ˆï¼‰
                    prompt_encoding = self.tokenizer(
                        prompt,
                        return_tensors="pt",
                        max_length=self.max_length,
                        truncation=True,
                        padding=True
                    ).to(self.device)
                    
                    # ç”Ÿæˆé¢„æµ‹
                    with torch.no_grad():
                        outputs = model.generate(
                            input_ids=prompt_encoding['input_ids'],
                            attention_mask=prompt_encoding['attention_mask'],
                            max_new_tokens=20,
                            do_sample=False,
                            pad_token_id=self.tokenizer.pad_token_id,
                            eos_token_id=self.tokenizer.eos_token_id,
                            temperature=0.1
                        )
                    
                    # è§£ç å¹¶è§£æé¢„æµ‹ç»“æœ
                    generated = outputs[0][prompt_encoding['input_ids'].shape[1]:]
                    generated_text = self.tokenizer.decode(generated, skip_special_tokens=True).strip()
                    
                    # è§£ææ ‡ç­¾
                    predicted_label = self._parse_generated_label(generated_text)
                    predictions.append(predicted_label)
                    
                    # è·å–çœŸå®æ ‡ç­¾
                    true_labels.append(test_dataset.labels[i])
                    
                except Exception as e:
                    print(f"âš ï¸  è¯„ä¼°ç¬¬ {i} ä¸ªæ ·æœ¬æ—¶å‡ºé”™: {e}")
                    # ä½¿ç”¨é»˜è®¤é¢„æµ‹
                    predictions.append(0)
                    true_labels.append(test_dataset.labels[i])
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            accuracy = accuracy_score(true_labels, predictions)
            f1_macro = f1_score(true_labels, predictions, average='macro', zero_division=0)
            f1_weighted = f1_score(true_labels, predictions, average='weighted', zero_division=0)
            
            # åˆ†ç±»æŠ¥å‘Š
            class_names = ['Non-rumor', 'Rumor', 'Unverified']
            report = classification_report(
                true_labels, predictions,
                target_names=class_names,
                output_dict=True,
                zero_division=0
            )
            
            evaluation_result = {
                'accuracy': accuracy,
                'f1_macro': f1_macro,
                'f1_weighted': f1_weighted,
                'classification_report': report,
                'num_samples': len(true_labels),
                'predictions': predictions,
                'true_labels': true_labels
            }
            
            print(f"âœ… æ¨¡å‹è¯„ä¼°å®Œæˆ:")
            print(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
            print(f"   F1åˆ†æ•°(macro): {f1_macro:.4f}")
            print(f"   F1åˆ†æ•°(weighted): {f1_weighted:.4f}")
            
            # æ‰“å°ä¸€äº›é¢„æµ‹æ ·ä¾‹
            print(f"\nğŸ“‹ é¢„æµ‹æ ·ä¾‹:")
            for i in range(min(3, len(true_labels))):
                print(f"   æ ·ä¾‹ {i+1}:")
                print(f"     æ–‡æœ¬: {test_dataset.texts[i][:50]}...")
                print(f"     çœŸå®: {class_names[true_labels[i]]}")
                print(f"     é¢„æµ‹: {class_names[predictions[i]]}")
            
            return evaluation_result
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            raise
    
    def _parse_generated_label(self, generated_text: str) -> int:
        """è§£æç”Ÿæˆçš„æ–‡æœ¬ä¸­çš„æ ‡ç­¾"""
        text_lower = generated_text.lower().strip()
        
        if 'rumor' in text_lower and 'non-rumor' not in text_lower:
            return 1  # Rumor
        elif 'non-rumor' in text_lower:
            return 0  # Non-rumor
        elif 'unverified' in text_lower:
            return 2  # Unverified
        else:
            return 0  # é»˜è®¤ä¸º Non-rumor
    
    def save_lora_model(self, save_path: Optional[str] = None):
        """
        ä¿å­˜ LoRA æ¨¡å‹
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
        """
        if self.peft_model is None:
            raise ValueError("æ²¡æœ‰å¯ä¿å­˜çš„ PEFT æ¨¡å‹")
        
        if save_path is None:
            save_path = self.output_dir / f"qwen_lora_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        try:
            save_path = Path(save_path)
            save_path.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ LoRA æ¨¡å‹
            self.peft_model.save_pretrained(save_path)
            
            # ä¿å­˜åˆ†è¯å™¨
            self.tokenizer.save_pretrained(save_path)
            
            # ä¿å­˜é…ç½®
            config = {
                'model_name': self.model_name,
                'max_length': self.max_length,
                'lora_config': {
                    'r': self.lora_config.r,
                    'lora_alpha': self.lora_config.lora_alpha,
                    'lora_dropout': self.lora_config.lora_dropout,
                    'target_modules': list(self.lora_config.target_modules) if hasattr(self.lora_config.target_modules, '__iter__') else self.lora_config.target_modules,
                    'bias': self.lora_config.bias,
                    'task_type': str(self.lora_config.task_type)
                } if self.lora_config else None,
                'timestamp': datetime.now().isoformat()
            }
            
            with open(save_path / 'training_config.json', 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… LoRA æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ LoRA æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def load_lora_model(self, model_path: str):
        """
        åŠ è½½ LoRA æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
        """
        try:
            print(f"ğŸ“¥ åŠ è½½ LoRA æ¨¡å‹: {model_path}")
            
            # åŠ è½½ PEFT æ¨¡å‹
            self.peft_model = PeftModel.from_pretrained(self.base_model, model_path)
            
            # åŠ è½½é…ç½®
            config_path = Path(model_path) / 'training_config.json'
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                print(f"   é…ç½®: {config}")
            
            print("âœ… LoRA æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ åŠ è½½ LoRA æ¨¡å‹å¤±è´¥: {e}")
            raise


def create_compute_metrics_fn(tokenizer):
    """åˆ›å»ºæŒ‡æ ‡è®¡ç®—å‡½æ•°"""
    def compute_metrics(eval_pred):
        predictions, labels = eval_pred
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„æŒ‡æ ‡è®¡ç®—
        # ç›®å‰è¿”å›ç®€å•çš„æŸå¤±
        return {"eval_loss": predictions.mean()}
    
    return compute_metrics


def demo_lora_finetuning():
    """æ¼”ç¤º LoRA å¾®è°ƒåŠŸèƒ½"""
    print("ğŸš€ Qwen3-0.6B LoRA å¾®è°ƒæ¼”ç¤º")
    print("=" * 60)
    
    try:
        # 1. åˆ›å»ºè®­ç»ƒå™¨
        print("1. åˆ›å»º LoRA è®­ç»ƒå™¨...")
        trainer = QwenLoRATrainer(
            model_name="Qwen/Qwen3-0.6B",
            max_length=512
        )
        
        # 2. è®¾ç½® LoRA é…ç½®
        print("\n2. è®¾ç½® LoRA é…ç½®...")
        trainer.setup_lora_config(
            r=16,
            lora_alpha=32,
            lora_dropout=0.1
        )
        
        # 3. åˆ›å»º PEFT æ¨¡å‹
        print("\n3. åˆ›å»º PEFT æ¨¡å‹...")
        trainer.create_peft_model()
        
        # 4. å‡†å¤‡æ•°æ®é›†
        print("\n4. å‡†å¤‡æ•°æ®é›†...")
        datasets = trainer.prepare_datasets()
        
        # 5. åˆ›å»ºè®­ç»ƒå‚æ•°
        print("\n5. åˆ›å»ºè®­ç»ƒå‚æ•°...")
        training_args = trainer.create_training_arguments(
            num_train_epochs=2,
            per_device_train_batch_size=2,
            per_device_eval_batch_size=4,
            learning_rate=2e-4,
            warmup_steps=50,
            logging_steps=5,
            save_steps=100,
            eval_steps=100
        )
        
        # 6. å¼€å§‹è®­ç»ƒ
        print("\n6. å¼€å§‹ LoRA å¾®è°ƒè®­ç»ƒ...")
        compute_metrics = create_compute_metrics_fn(trainer.tokenizer)
        
        train_result = trainer.train(
            train_dataset=datasets['train'],
            eval_dataset=datasets['val'],
            training_args=training_args,
            compute_metrics_fn=compute_metrics
        )
        
        # 7. è¯„ä¼°æ¨¡å‹
        print("\n7. è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹...")
        eval_result = trainer.evaluate_model(datasets['test'])
        
        # 8. ä¿å­˜æ¨¡å‹
        print("\n8. ä¿å­˜ LoRA æ¨¡å‹...")
        trainer.save_lora_model()
        
        print(f"\nâœ… LoRA å¾®è°ƒæ¼”ç¤ºå®Œæˆ!")
        print(f"   æœ€ç»ˆå‡†ç¡®ç‡: {eval_result['accuracy']:.4f}")
        print(f"   æœ€ç»ˆ F1 åˆ†æ•°: {eval_result['f1_macro']:.4f}")
        
    except Exception as e:
        print(f"âŒ LoRA å¾®è°ƒæ¼”ç¤ºå¤±è´¥: {e}")
        raise


def quick_lora_training():
    """å¿«é€Ÿ LoRA è®­ç»ƒå‡½æ•°"""
    print("âš¡ å¿«é€Ÿ LoRA è®­ç»ƒ")
    print("=" * 40)
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = QwenLoRATrainer()
        
        # ç®€åŒ–é…ç½®
        trainer.setup_lora_config(r=8, lora_alpha=16)
        trainer.create_peft_model()
        
        # å‡†å¤‡æ•°æ®
        datasets = trainer.prepare_datasets()
        
        # å¿«é€Ÿè®­ç»ƒå‚æ•°
        training_args = trainer.create_training_arguments(
            num_train_epochs=1,
            per_device_train_batch_size=1,
            per_device_eval_batch_size=2,
            learning_rate=1e-4,
            warmup_steps=10,
            logging_steps=2,
            save_steps=50,
            eval_steps=50
        )
        
        # è®­ç»ƒ
        trainer.train(
            train_dataset=datasets['train'],
            eval_dataset=datasets['val'],
            training_args=training_args
        )
        
        # å¿«é€Ÿè¯„ä¼°
        result = trainer.evaluate_model(datasets['test'])
        print(f"å¿«é€Ÿè®­ç»ƒç»“æœ - å‡†ç¡®ç‡: {result['accuracy']:.4f}")
        
        return trainer
        
    except Exception as e:
        print(f"âŒ å¿«é€Ÿè®­ç»ƒå¤±è´¥: {e}")
        return None


class LoRAInferenceEngine:
    """LoRA æ¨ç†å¼•æ“"""
    
    def __init__(self, base_model_name: str = "Qwen/Qwen3-0.6B", lora_model_path: str = None):
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“
        
        Args:
            base_model_name: åŸºç¡€æ¨¡å‹åç§°
            lora_model_path: LoRA æ¨¡å‹è·¯å¾„
        """
        self.base_model_name = base_model_name
        self.lora_model_path = lora_model_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.tokenizer = None
        self.model = None
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        try:
            print(f"ğŸ“¥ åŠ è½½æ¨ç†æ¨¡å‹...")
            
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.base_model_name,
                trust_remote_code=True,
                use_fast=False
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                trust_remote_code=True,
                torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32,
                device_map="auto" if self.device.type == 'cuda' else None
            )
            
            # å¦‚æœæœ‰ LoRA æ¨¡å‹è·¯å¾„ï¼ŒåŠ è½½ LoRA æƒé‡
            if self.lora_model_path and Path(self.lora_model_path).exists():
                print(f"ğŸ“¥ åŠ è½½ LoRA æƒé‡: {self.lora_model_path}")
                self.model = PeftModel.from_pretrained(base_model, self.lora_model_path)
            else:
                print("âš ï¸  æœªæŒ‡å®š LoRA æ¨¡å‹è·¯å¾„ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹")
                self.model = base_model
            
            self.model.eval()
            
            if self.device.type != 'cuda':
                self.model = self.model.to(self.device)
            
            print("âœ… æ¨ç†æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æ¨ç†æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def predict(self, text: str, max_new_tokens: int = 20) -> Dict[str, Any]:
        """
        å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œæ¨ç†é¢„æµ‹
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        try:
            # åˆ›å»ºæç¤º
            prompt = f"è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬æ˜¯å¦ä¸ºè°£è¨€ã€‚\n\næ–‡æœ¬: {text}\n\nè¯·ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©:\n- Non-rumor: éè°£è¨€\n- Rumor: è°£è¨€\n- Unverified: æœªéªŒè¯\n\nç­”æ¡ˆ: "
            
            # åˆ†è¯
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=512,
                truncation=True,
                padding=True
            )
            
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆé¢„æµ‹
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    temperature=0.1
                )
            
            # è§£ç ç»“æœ
            generated = outputs[0][inputs['input_ids'].shape[1]:]
            generated_text = self.tokenizer.decode(generated, skip_special_tokens=True).strip()
            
            # è§£ææ ‡ç­¾
            predicted_label = self._parse_prediction(generated_text)
            label_mapping = {0: 'Non-rumor', 1: 'Rumor', 2: 'Unverified'}
            
            return {
                'text': text,
                'predicted_label': predicted_label,
                'predicted_class': label_mapping[predicted_label],
                'raw_output': generated_text,
                'confidence': self._calculate_confidence(generated_text)
            }
            
        except Exception as e:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            return {
                'text': text,
                'predicted_label': 0,
                'predicted_class': 'Non-rumor',
                'raw_output': '',
                'confidence': 0.0,
                'error': str(e)
            }
    
    def _parse_prediction(self, generated_text: str) -> int:
        """è§£æé¢„æµ‹ç»“æœ"""
        text_lower = generated_text.lower()
        
        if 'rumor' in text_lower and 'non-rumor' not in text_lower:
            return 1  # Rumor
        elif 'non-rumor' in text_lower:
            return 0  # Non-rumor
        elif 'unverified' in text_lower:
            return 2  # Unverified
        else:
            return 0  # é»˜è®¤
    
    def _calculate_confidence(self, generated_text: str) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦ï¼ˆç®€å•å¯å‘å¼ï¼‰"""
        if any(keyword in generated_text.lower() for keyword in ['non-rumor', 'rumor', 'unverified']):
            return 0.8
        else:
            return 0.5
    
    def batch_predict(self, texts: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡é¢„æµ‹"""
        results = []
        for text in tqdm(texts, desc="æ‰¹é‡é¢„æµ‹"):
            result = self.predict(text)
            results.append(result)
        return results


def demo_lora_inference():
    """æ¼”ç¤º LoRA æ¨ç†åŠŸèƒ½"""
    print("ğŸ” LoRA æ¨ç†æ¼”ç¤º")
    print("=" * 40)
    
    try:
        # åˆ›å»ºæ¨ç†å¼•æ“ï¼ˆä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼Œå› ä¸ºå¯èƒ½æ²¡æœ‰è®­ç»ƒå¥½çš„ LoRA æ¨¡å‹ï¼‰
        engine = LoRAInferenceEngine()
        
        # æµ‹è¯•æ–‡æœ¬
        test_texts = [
            "ç§‘å­¦å®¶å‘ç°æ–°çš„æ²»ç–—æ–¹æ³•ï¼Œç»è¿‡ä¸¥æ ¼ä¸´åºŠè¯•éªŒéªŒè¯",
            "ç½‘ä¼ æŸåœ°å‘ç”Ÿé‡å¤§äº‹æ•…ï¼Œä½†å®˜æ–¹å°šæœªç¡®è®¤",
            "è°£ä¼ æŸäº§å“å«æœ‰æœ‰å®³æˆåˆ†ï¼Œå·²è¢«ç§‘å­¦ç ”ç©¶è¯å®ä¸ºè™šå‡ä¿¡æ¯"
        ]
        
        print("ğŸ“ å•ä¸ªé¢„æµ‹æµ‹è¯•:")
        for i, text in enumerate(test_texts, 1):
            result = engine.predict(text)
            print(f"\næ–‡æœ¬ {i}: {text}")
            print(f"é¢„æµ‹: {result['predicted_class']} (ç½®ä¿¡åº¦: {result['confidence']:.2f})")
            print(f"åŸå§‹è¾“å‡º: {result['raw_output']}")
        
        print(f"\nğŸ“Š æ‰¹é‡é¢„æµ‹æµ‹è¯•:")
        batch_results = engine.batch_predict(test_texts)
        for i, result in enumerate(batch_results, 1):
            print(f"æ–‡æœ¬ {i}: {result['predicted_class']}")
        
        print("âœ… LoRA æ¨ç†æ¼”ç¤ºå®Œæˆ")
        
    except Exception as e:
        print(f"âŒ LoRA æ¨ç†æ¼”ç¤ºå¤±è´¥: {e}")


def advanced_lora_training():
    """é«˜çº§ LoRA è®­ç»ƒåŠŸèƒ½"""
    print("ğŸ¯ é«˜çº§ LoRA è®­ç»ƒ")
    print("=" * 40)
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = QwenLoRATrainer()
        
        # é«˜çº§ LoRA é…ç½®
        trainer.setup_lora_config(
            r=32,  # æ›´å¤§çš„ rank
            lora_alpha=64,
            lora_dropout=0.05,
            target_modules=[
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj",
                "embed_tokens", "lm_head"  # åŒ…å«æ›´å¤šå±‚
            ]
        )
        
        trainer.create_peft_model()
        datasets = trainer.prepare_datasets()
        
        # é«˜çº§è®­ç»ƒå‚æ•°
        training_args = trainer.create_training_arguments(
            num_train_epochs=5,
            per_device_train_batch_size=4,
            per_device_eval_batch_size=8,
            learning_rate=1e-4,
            warmup_steps=100,
            logging_steps=10,
            save_steps=200,
            eval_steps=200,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False
        )
        
        # è®­ç»ƒ
        train_result = trainer.train(
            train_dataset=datasets['train'],
            eval_dataset=datasets['val'],
            training_args=training_args
        )
        
        # è¯¦ç»†è¯„ä¼°
        eval_result = trainer.evaluate_model(datasets['test'])
        
        # ä¿å­˜æ¨¡å‹
        save_path = trainer.output_dir / "advanced_lora_model"
        trainer.save_lora_model(save_path)
        
        print(f"âœ… é«˜çº§ LoRA è®­ç»ƒå®Œæˆ")
        print(f"   è®­ç»ƒæŸå¤±: {train_result.training_loss:.4f}")
        print(f"   æµ‹è¯•å‡†ç¡®ç‡: {eval_result['accuracy']:.4f}")
        print(f"   æ¨¡å‹ä¿å­˜è‡³: {save_path}")
        
        return trainer, eval_result
        
    except Exception as e:
        print(f"âŒ é«˜çº§ LoRA è®­ç»ƒå¤±è´¥: {e}")
        return None, None


# ä¸»æ‰§è¡Œä»£ç 
if __name__ == "__main__":
    print("ğŸš€ Qwen3-0.6B LoRA å¾®è°ƒç³»ç»Ÿ")
    print("=" * 60)
    
    import argparse
    parser = argparse.ArgumentParser(description="LoRA å¾®è°ƒç³»ç»Ÿ")
    parser.add_argument("--mode", type=str, default="demo", 
                       choices=["demo", "quick", "advanced", "inference"],
                       help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--model_path", type=str, default=None,
                       help="LoRA æ¨¡å‹è·¯å¾„ï¼ˆæ¨ç†æ¨¡å¼ä½¿ç”¨ï¼‰")
    
    args = parser.parse_args()
    
    try:
        if args.mode == "demo":
            demo_lora_finetuning()
        elif args.mode == "quick":
            quick_lora_training()
        elif args.mode == "advanced":
            advanced_lora_training()
        elif args.mode == "inference":
            demo_lora_inference()
        
        print(f"\nğŸ‰ {args.mode.upper()} æ¨¡å¼æ‰§è¡Œå®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()