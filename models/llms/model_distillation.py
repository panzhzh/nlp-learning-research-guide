#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# models/llms/model_distillation.py

"""
æ¨¡å‹è’¸é¦å’Œå‹ç¼©æ¨¡å—
å®ç°çŸ¥è¯†è’¸é¦ã€æ¨¡å‹å‰ªæã€é‡åŒ–ç­‰å‹ç¼©æŠ€æœ¯
å°†å¤§æ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°å°æ¨¡å‹ä¸­ï¼Œæå‡éƒ¨ç½²æ•ˆç‡
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, AutoConfig,
    TrainingArguments, Trainer, BitsAndBytesConfig
)
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union
from pathlib import Path
import json
import sys
import logging
from tqdm import tqdm
from dataclasses import dataclass
import warnings

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from data_utils.data_loaders import create_all_dataloaders
    from utils.config_manager import get_config_manager, get_output_path
    from models.llms.open_source_llms import QwenRumorClassifier
    USE_PROJECT_MODULES = True
    print("âœ… æˆåŠŸå¯¼å…¥é¡¹ç›®æ¨¡å—")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥é¡¹ç›®æ¨¡å—å¤±è´¥: {e}")
    USE_PROJECT_MODULES = False


@dataclass
class DistillationConfig:
    """è’¸é¦é…ç½®"""
    teacher_model: str = "Qwen/Qwen3-0.6B"
    student_model: str = "Qwen/Qwen3-0.6B"  # æˆ–è€…æ›´å°çš„æ¨¡å‹
    temperature: float = 4.0
    alpha: float = 0.7  # è’¸é¦æŸå¤±æƒé‡
    beta: float = 0.3   # çœŸå®æ ‡ç­¾æŸå¤±æƒé‡
    max_length: int = 512
    learning_rate: float = 5e-5
    num_epochs: int = 3
    batch_size: int = 8
    save_dir: str = "outputs/distillation"
    use_quantization: bool = False
    use_pruning: bool = False


class KnowledgeDistillationDataset(Dataset):
    """çŸ¥è¯†è’¸é¦æ•°æ®é›†"""
    
    def __init__(self, texts: List[str], labels: List[int], 
                 tokenizer, max_length: int = 512):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨
            tokenizer: åˆ†è¯å™¨
            max_length: æœ€å¤§é•¿åº¦
        """
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        
        # åˆ†è¯
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }


class TeacherStudentLoss(nn.Module):
    """å¸ˆç”Ÿç½‘ç»œæŸå¤±å‡½æ•°"""
    
    def __init__(self, temperature: float = 4.0, alpha: float = 0.7):
        """
        åˆå§‹åŒ–æŸå¤±å‡½æ•°
        
        Args:
            temperature: è’¸é¦æ¸©åº¦
            alpha: è’¸é¦æŸå¤±æƒé‡
        """
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.beta = 1.0 - alpha
        self.kl_div = nn.KLDivLoss(reduction='batchmean')
        self.ce_loss = nn.CrossEntropyLoss()
    
    def forward(self, student_logits: torch.Tensor, 
                teacher_logits: torch.Tensor,
                true_labels: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—è’¸é¦æŸå¤±
        
        Args:
            student_logits: å­¦ç”Ÿæ¨¡å‹è¾“å‡º
            teacher_logits: æ•™å¸ˆæ¨¡å‹è¾“å‡º
            true_labels: çœŸå®æ ‡ç­¾
            
        Returns:
            æŸå¤±å­—å…¸
        """
        # è®¡ç®—è½¯ç›®æ ‡æŸå¤± (è’¸é¦æŸå¤±)
        student_soft = F.log_softmax(student_logits / self.temperature, dim=1)
        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=1)
        distillation_loss = self.kl_div(student_soft, teacher_soft) * (self.temperature ** 2)
        
        # è®¡ç®—ç¡¬ç›®æ ‡æŸå¤± (çœŸå®æ ‡ç­¾æŸå¤±)
        hard_loss = self.ce_loss(student_logits, true_labels)
        
        # æ€»æŸå¤±
        total_loss = self.alpha * distillation_loss + self.beta * hard_loss
        
        return {
            'total_loss': total_loss,
            'distillation_loss': distillation_loss,
            'hard_loss': hard_loss
        }


class ModelPruner:
    """æ¨¡å‹å‰ªæå™¨"""
    
    def __init__(self, pruning_ratio: float = 0.1):
        """
        åˆå§‹åŒ–å‰ªæå™¨
        
        Args:
            pruning_ratio: å‰ªææ¯”ä¾‹
        """
        self.pruning_ratio = pruning_ratio
    
    def magnitude_pruning(self, model: nn.Module) -> None:
        """
        åŸºäºæƒé‡å¤§å°çš„å‰ªæ
        
        Args:
            model: è¦å‰ªæçš„æ¨¡å‹
        """
        for name, module in model.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):
                # è®¡ç®—æƒé‡çš„ç»å¯¹å€¼
                weight_abs = torch.abs(module.weight.data)
                
                # è®¡ç®—é˜ˆå€¼
                threshold = torch.quantile(weight_abs.flatten(), self.pruning_ratio)
                
                # åˆ›å»ºæ©ç 
                mask = weight_abs > threshold
                
                # åº”ç”¨å‰ªæ
                module.weight.data *= mask.float()
                
                # å¦‚æœæœ‰åç½®ï¼Œä¹Ÿè¿›è¡Œå‰ªæ
                if module.bias is not None:
                    bias_abs = torch.abs(module.bias.data)
                    bias_threshold = torch.quantile(bias_abs.flatten(), self.pruning_ratio)
                    bias_mask = bias_abs > bias_threshold
                    module.bias.data *= bias_mask.float()
        
        print(f"âœ… å®ŒæˆåŸºäºæƒé‡å¤§å°çš„å‰ªæ (æ¯”ä¾‹: {self.pruning_ratio})")
    
    def structured_pruning(self, model: nn.Module, layers_to_prune: List[str]) -> None:
        """
        ç»“æ„åŒ–å‰ªæ
        
        Args:
            model: è¦å‰ªæçš„æ¨¡å‹
            layers_to_prune: è¦å‰ªæçš„å±‚åç§°åˆ—è¡¨
        """
        for layer_name in layers_to_prune:
            if hasattr(model, layer_name):
                layer = getattr(model, layer_name)
                if isinstance(layer, nn.Linear):
                    # è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„é‡è¦æ€§
                    neuron_importance = torch.norm(layer.weight.data, dim=0)
                    
                    # ç¡®å®šè¦ä¿ç•™çš„ç¥ç»å…ƒæ•°é‡
                    num_neurons = layer.weight.size(1)
                    num_keep = int(num_neurons * (1 - self.pruning_ratio))
                    
                    # é€‰æ‹©æœ€é‡è¦çš„ç¥ç»å…ƒ
                    _, important_indices = torch.topk(neuron_importance, num_keep)
                    
                    # åˆ›å»ºæ–°çš„æƒé‡å’Œåç½®
                    new_weight = layer.weight.data[:, important_indices]
                    new_bias = layer.bias.data if layer.bias is not None else None
                    
                    # æ›¿æ¢å±‚
                    new_layer = nn.Linear(num_keep, layer.weight.size(0), bias=layer.bias is not None)
                    new_layer.weight.data = new_weight
                    if new_bias is not None:
                        new_layer.bias.data = new_bias
                    
                    setattr(model, layer_name, new_layer)
        
        print(f"âœ… å®Œæˆç»“æ„åŒ–å‰ªæ")
    
    def get_model_sparsity(self, model: nn.Module) -> float:
        """
        è®¡ç®—æ¨¡å‹ç¨€ç–åº¦
        
        Args:
            model: æ¨¡å‹
            
        Returns:
            ç¨€ç–åº¦
        """
        total_params = 0
        zero_params = 0
        
        for param in model.parameters():
            total_params += param.numel()
            zero_params += (param.data == 0).sum().item()
        
        sparsity = zero_params / total_params
        return sparsity


class ModelQuantizer:
    """æ¨¡å‹é‡åŒ–å™¨"""
    
    def __init__(self, quantization_type: str = "dynamic"):
        """
        åˆå§‹åŒ–é‡åŒ–å™¨
        
        Args:
            quantization_type: é‡åŒ–ç±»å‹ ("dynamic", "static", "qat")
        """
        self.quantization_type = quantization_type
    
    def dynamic_quantization(self, model: nn.Module) -> nn.Module:
        """
        åŠ¨æ€é‡åŒ–
        
        Args:
            model: è¦é‡åŒ–çš„æ¨¡å‹
            
        Returns:
            é‡åŒ–åçš„æ¨¡å‹
        """
        quantized_model = torch.quantization.quantize_dynamic(
            model, 
            {nn.Linear}, 
            dtype=torch.qint8
        )
        
        print(f"âœ… å®ŒæˆåŠ¨æ€é‡åŒ–")
        return quantized_model
    
    def static_quantization(self, model: nn.Module, 
                          calibration_loader: DataLoader) -> nn.Module:
        """
        é™æ€é‡åŒ–
        
        Args:
            model: è¦é‡åŒ–çš„æ¨¡å‹
            calibration_loader: æ ¡å‡†æ•°æ®åŠ è½½å™¨
            
        Returns:
            é‡åŒ–åçš„æ¨¡å‹
        """
        # è®¾ç½®é‡åŒ–é…ç½®
        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
        
        # å‡†å¤‡é‡åŒ–
        quantized_model = torch.quantization.prepare(model)
        
        # æ ¡å‡†
        quantized_model.eval()
        with torch.no_grad():
            for batch in tqdm(calibration_loader, desc="é‡åŒ–æ ¡å‡†"):
                if isinstance(batch, dict):
                    input_ids = batch.get('input_ids')
                    if input_ids is not None:
                        quantized_model(input_ids)
        
        # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
        quantized_model = torch.quantization.convert(quantized_model)
        
        print(f"âœ… å®Œæˆé™æ€é‡åŒ–")
        return quantized_model
    
    def get_model_size(self, model: nn.Module) -> float:
        """
        è®¡ç®—æ¨¡å‹å¤§å° (MB)
        
        Args:
            model: æ¨¡å‹
            
        Returns:
            æ¨¡å‹å¤§å° (MB)
        """
        param_size = 0
        buffer_size = 0
        
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
        
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        
        size_mb = (param_size + buffer_size) / 1024 / 1024
        return size_mb


class KnowledgeDistiller:
    """çŸ¥è¯†è’¸é¦å™¨"""
    
    def __init__(self, config: DistillationConfig):
        """
        åˆå§‹åŒ–è’¸é¦å™¨
        
        Args:
            config: è’¸é¦é…ç½®
        """
        self.config = config
        
        # åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
        self.teacher_model = None
        self.student_model = None
        self.tokenizer = None
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.loss_fn = TeacherStudentLoss(
            temperature=config.temperature,
            alpha=config.alpha
        )
        self.pruner = ModelPruner() if config.use_pruning else None
        self.quantizer = ModelQuantizer() if config.use_quantization else None
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        self.output_dir = Path(config.save_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“ çŸ¥è¯†è’¸é¦å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ•™å¸ˆæ¨¡å‹: {config.teacher_model}")
        print(f"   å­¦ç”Ÿæ¨¡å‹: {config.student_model}")
        print(f"   æ¸©åº¦: {config.temperature}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_models(self) -> None:
        """åŠ è½½æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹"""
        try:
            print("ğŸ“¥ åŠ è½½åˆ†è¯å™¨...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.teacher_model,
                trust_remote_code=True,
                pad_token='<|extra_0|>',
                eos_token='<|im_end|>'
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("ğŸ“¥ åŠ è½½æ•™å¸ˆæ¨¡å‹...")
            self.teacher_model = AutoModelForCausalLM.from_pretrained(
                self.config.teacher_model,
                trust_remote_code=True,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None
            )
            
            print("ğŸ“¥ åŠ è½½å­¦ç”Ÿæ¨¡å‹...")
            if self.config.student_model == self.config.teacher_model:
                # å¦‚æœæ˜¯åŒä¸€ä¸ªæ¨¡å‹ï¼Œåˆ›å»ºå‰¯æœ¬
                student_config = AutoConfig.from_pretrained(self.config.student_model)
                # å¯ä»¥ä¿®æ”¹é…ç½®æ¥åˆ›å»ºæ›´å°çš„æ¨¡å‹
                # student_config.num_hidden_layers = student_config.num_hidden_layers // 2
                
                self.student_model = AutoModelForCausalLM.from_config(
                    student_config,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
                )
            else:
                self.student_model = AutoModelForCausalLM.from_pretrained(
                    self.config.student_model,
                    trust_remote_code=True,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    device_map="auto" if torch.cuda.is_available() else None
                )
            
            # è®¾ç½®ä¸ºè¯„ä¼°/è®­ç»ƒæ¨¡å¼
            self.teacher_model.eval()
            self.student_model.train()
            
            # å†»ç»“æ•™å¸ˆæ¨¡å‹
            for param in self.teacher_model.parameters():
                param.requires_grad = False
            
            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            print(f"   æ•™å¸ˆæ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.teacher_model.parameters()):,}")
            print(f"   å­¦ç”Ÿæ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.student_model.parameters()):,}")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def prepare_dataset(self) -> Tuple[Dataset, Dataset]:
        """å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†"""
        if USE_PROJECT_MODULES:
            try:
                # ä»çœŸå®æ•°æ®é›†åŠ è½½
                dataloaders = create_all_dataloaders(
                    batch_sizes={'train': 32, 'val': 32}
                )
                
                # æå–è®­ç»ƒæ•°æ®
                train_texts, train_labels = [], []
                for batch in dataloaders['train']:
                    texts = batch.get('text', batch.get('caption', []))
                    labels = batch.get('labels', batch.get('label', []))
                    
                    if hasattr(labels, 'tolist'):
                        labels = labels.tolist()
                    
                    train_texts.extend(texts)
                    train_labels.extend(labels)
                
                # æå–éªŒè¯æ•°æ®
                val_texts, val_labels = [], []
                for batch in dataloaders['val']:
                    texts = batch.get('text', batch.get('caption', []))
                    labels = batch.get('labels', batch.get('label', []))
                    
                    if hasattr(labels, 'tolist'):
                        labels = labels.tolist()
                    
                    val_texts.extend(texts)
                    val_labels.extend(labels)
                
                # é™åˆ¶æ•°æ®é‡ä»¥åŠ å¿«è®­ç»ƒ
                train_texts = train_texts[:1000]
                train_labels = train_labels[:1000]
                val_texts = val_texts[:200]
                val_labels = val_labels[:200]
                
            except Exception as e:
                logger.warning(f"åŠ è½½çœŸå®æ•°æ®é›†å¤±è´¥: {e}ï¼Œä½¿ç”¨æ¼”ç¤ºæ•°æ®")
                train_texts, train_labels, val_texts, val_labels = self._get_demo_data()
        else:
            train_texts, train_labels, val_texts, val_labels = self._get_demo_data()
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = KnowledgeDistillationDataset(
            train_texts, train_labels, self.tokenizer, self.config.max_length
        )
        val_dataset = KnowledgeDistillationDataset(
            val_texts, val_labels, self.tokenizer, self.config.max_length
        )
        
        print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ")
        print(f"   è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        print(f"   éªŒè¯æ ·æœ¬: {len(val_dataset)}")
        
        return train_dataset, val_dataset
    
    def _get_demo_data(self) -> Tuple[List[str], List[int], List[str], List[int]]:
        """è·å–æ¼”ç¤ºæ•°æ®"""
        train_texts = [
            "æ”¿åºœå®˜æ–¹å‘å¸ƒæ–°çš„æ”¿ç­–å…¬å‘Š",
            "ç§‘å­¦æœŸåˆŠå‘è¡¨çš„ç ”ç©¶æˆæœ",
            "ç½‘ä¼ æŸåœ°å‘ç”Ÿé‡å¤§äº‹æ•…",
            "è°£ä¼ ç–«è‹—å«æœ‰å®³ç‰©è´¨",
            "æ®ä¸å®Œå…¨ç»Ÿè®¡å¸‚åœºåå“è‰¯å¥½",
            "ä¸“å®¶å­¦è€…çš„æƒå¨è§‚ç‚¹",
            "æœªç»è¯å®çš„ç½‘ç»œä¼ è¨€",
            "å®˜æ–¹åª’ä½“çš„æ–°é—»æŠ¥é“"
        ] * 25  # é‡å¤ä»¥å¢åŠ æ•°æ®é‡
        
        train_labels = [0, 0, 1, 1, 2, 0, 1, 0] * 25
        
        val_texts = [
            "æ•™è‚²éƒ¨å‘å¸ƒé«˜è€ƒæ”¹é©æ–¹æ¡ˆ",
            "ç½‘ä¼ æ˜å¤©å°†å‘ç”Ÿåœ°éœ‡",
            "ä¸šå†…äººå£«é€éœ²çš„æ¶ˆæ¯"
        ] * 20
        
        val_labels = [0, 1, 2] * 20
        
        return train_texts, train_labels, val_texts, val_labels
    
    def distill_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """å•æ­¥è’¸é¦"""
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        true_labels = batch['labels']
        
        # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­
        with torch.no_grad():
            teacher_outputs = self.teacher_model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            teacher_logits = teacher_outputs.logits[:, -1, :3]  # å–æœ€åä¸€ä¸ªtokençš„å‰3ç»´ä½œä¸ºåˆ†ç±»logits
        
        # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
        student_outputs = self.student_model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        student_logits = student_outputs.logits[:, -1, :3]  # å–æœ€åä¸€ä¸ªtokençš„å‰3ç»´ä½œä¸ºåˆ†ç±»logits
        
        # è®¡ç®—æŸå¤±
        loss_dict = self.loss_fn(student_logits, teacher_logits, true_labels)
        
        return {
            'total_loss': loss_dict['total_loss'].item(),
            'distillation_loss': loss_dict['distillation_loss'].item(),
            'hard_loss': loss_dict['hard_loss'].item()
        }
    
    def train_distillation(self) -> Dict[str, Any]:
        """è®­ç»ƒè’¸é¦æ¨¡å‹"""
        print("ğŸ“ å¼€å§‹çŸ¥è¯†è’¸é¦è®­ç»ƒ...")
        
        # å‡†å¤‡æ•°æ®é›†
        train_dataset, val_dataset = self.prepare_dataset()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=0
        )
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            self.student_model.parameters(),
            lr=self.config.learning_rate
        )
        
        # è®­ç»ƒå†å²
        training_history = {
            'train_losses': [],
            'val_losses': [],
            'epochs': []
        }
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.config.num_epochs):
            print(f"\nğŸ“š Epoch {epoch + 1}/{self.config.num_epochs}")
            
            # è®­ç»ƒé˜¶æ®µ
            self.student_model.train()
            train_losses = []
            
            train_bar = tqdm(train_loader, desc=f"è®­ç»ƒ Epoch {epoch + 1}")
            for batch in train_bar:
                # ç§»åŠ¨åˆ°è®¾å¤‡
                device = next(self.student_model.parameters()).device
                batch = {k: v.to(device) for k, v in batch.items()}
                
                # å‰å‘ä¼ æ’­
                loss_dict = self.distill_step(batch)
                total_loss = loss_dict['total_loss']
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                
                # é‡æ–°è®¡ç®—æŸå¤±ç”¨äºåå‘ä¼ æ’­
                input_ids = batch['input_ids']
                attention_mask = batch['attention_mask']
                true_labels = batch['labels']
                
                with torch.no_grad():
                    teacher_outputs = self.teacher_model(
                        input_ids=input_ids,
                        attention_mask=attention_mask
                    )
                    teacher_logits = teacher_outputs.logits[:, -1, :3]
                
                student_outputs = self.student_model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                student_logits = student_outputs.logits[:, -1, :3]
                
                loss_dict_bp = self.loss_fn(student_logits, teacher_logits, true_labels)
                loss_dict_bp['total_loss'].backward()
                
                optimizer.step()
                
                train_losses.append(total_loss)
                train_bar.set_postfix({'loss': f'{total_loss:.4f}'})
            
            avg_train_loss = np.mean(train_losses)
            
            # éªŒè¯é˜¶æ®µ
            self.student_model.eval()
            val_losses = []
            
            with torch.no_grad():
                for batch in tqdm(val_loader, desc="éªŒè¯"):
                    device = next(self.student_model.parameters()).device
                    batch = {k: v.to(device) for k, v in batch.items()}
                    
                    loss_dict = self.distill_step(batch)
                    val_losses.append(loss_dict['total_loss'])
            
            avg_val_loss = np.mean(val_losses)
            
            # è®°å½•å†å²
            training_history['train_losses'].append(avg_train_loss)
            training_history['val_losses'].append(avg_val_loss)
            training_history['epochs'].append(epoch + 1)
            
            print(f"   è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
            print(f"   éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
        
        print(f"âœ… çŸ¥è¯†è’¸é¦è®­ç»ƒå®Œæˆ")
        
        return training_history
    
    def compress_model(self) -> Dict[str, Any]:
        """å‹ç¼©æ¨¡å‹"""
        print("ğŸ—œï¸  å¼€å§‹æ¨¡å‹å‹ç¼©...")
        
        compression_results = {}
        
        # åŸå§‹æ¨¡å‹å¤§å°
        if self.quantizer:
            original_size = self.quantizer.get_model_size(self.student_model)
            compression_results['original_size_mb'] = original_size
            print(f"   åŸå§‹æ¨¡å‹å¤§å°: {original_size:.2f} MB")
        
        # å‰ªæ
        if self.config.use_pruning and self.pruner:
            print("âœ‚ï¸  æ‰§è¡Œæ¨¡å‹å‰ªæ...")
            
            # è®¡ç®—åŸå§‹ç¨€ç–åº¦
            original_sparsity = self.pruner.get_model_sparsity(self.student_model)
            
            # æ‰§è¡Œå‰ªæ
            self.pruner.magnitude_pruning(self.student_model)
            
            # è®¡ç®—å‰ªæåç¨€ç–åº¦
            pruned_sparsity = self.pruner.get_model_sparsity(self.student_model)
            
            compression_results.update({
                'original_sparsity': original_sparsity,
                'pruned_sparsity': pruned_sparsity,
                'pruning_ratio': pruned_sparsity - original_sparsity
            })
            
            print(f"   å‰ªæå‰ç¨€ç–åº¦: {original_sparsity:.4f}")
            print(f"   å‰ªæåç¨€ç–åº¦: {pruned_sparsity:.4f}")
        
        # é‡åŒ–
        if self.config.use_quantization and self.quantizer:
            print("ğŸ“ æ‰§è¡Œæ¨¡å‹é‡åŒ–...")
            
            # åŠ¨æ€é‡åŒ–
            quantized_model = self.quantizer.dynamic_quantization(self.student_model)
            
            # è®¡ç®—é‡åŒ–åå¤§å°
            quantized_size = self.quantizer.get_model_size(quantized_model)
            compression_ratio = original_size / quantized_size
            
            compression_results.update({
                'quantized_size_mb': quantized_size,
                'compression_ratio': compression_ratio
            })
            
            print(f"   é‡åŒ–åå¤§å°: {quantized_size:.2f} MB")
            print(f"   å‹ç¼©æ¯”: {compression_ratio:.2f}x")
            
            # æ›´æ–°å­¦ç”Ÿæ¨¡å‹
            self.student_model = quantized_model
        
        return compression_results
    
    def evaluate_student_model(self) -> Dict[str, Any]:
        """è¯„ä¼°å­¦ç”Ÿæ¨¡å‹"""
        print("ğŸ“Š è¯„ä¼°å­¦ç”Ÿæ¨¡å‹æ€§èƒ½...")
        
        if USE_PROJECT_MODULES:
            try:
                # ä½¿ç”¨çœŸå®æµ‹è¯•æ•°æ®
                dataloaders = create_all_dataloaders(
                    batch_sizes={'test': 16}
                )
                
                test_texts, test_labels = [], []
                for batch in dataloaders['test']:
                    texts = batch.get('text', batch.get('caption', []))
                    labels = batch.get('labels', batch.get('label', []))
                    
                    if hasattr(labels, 'tolist'):
                        labels = labels.tolist()
                    
                    test_texts.extend(texts)
                    test_labels.extend(labels)
                
                # é™åˆ¶æµ‹è¯•æ•°é‡
                test_texts = test_texts[:100]
                test_labels = test_labels[:100]
                
            except Exception as e:
                logger.warning(f"åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {e}ï¼Œä½¿ç”¨æ¼”ç¤ºæ•°æ®")
                test_texts, test_labels = self._get_test_demo_data()
        else:
            test_texts, test_labels = self._get_test_demo_data()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        test_dataset = KnowledgeDistillationDataset(
            test_texts, test_labels, self.tokenizer, self.config.max_length
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=0
        )
        
        # è¯„ä¼°
        self.student_model.eval()
        predictions = []
        true_labels_list = []
        
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="è¯„ä¼°"):
                device = next(self.student_model.parameters()).device
                batch = {k: v.to(device) for k, v in batch.items()}
                
                input_ids = batch['input_ids']
                attention_mask = batch['attention_mask']
                true_labels = batch['labels']
                
                # å­¦ç”Ÿæ¨¡å‹é¢„æµ‹
                outputs = self.student_model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                logits = outputs.logits[:, -1, :3]
                preds = torch.argmax(logits, dim=1)
                
                predictions.extend(preds.cpu().tolist())
                true_labels_list.extend(true_labels.cpu().tolist())
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        from sklearn.metrics import accuracy_score, f1_score, classification_report
        
        accuracy = accuracy_score(true_labels_list, predictions)
        f1_macro = f1_score(true_labels_list, predictions, average='macro')
        
        label_mapping = {0: 'Non-rumor', 1: 'Rumor', 2: 'Unverified'}
        report = classification_report(
            true_labels_list, predictions,
            target_names=list(label_mapping.values()),
            output_dict=True
        )
        
        evaluation_result = {
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'classification_report': report,
            'num_samples': len(test_texts),
            'predictions': predictions,
            'true_labels': true_labels_list
        }
        
        print(f"âœ… å­¦ç”Ÿæ¨¡å‹è¯„ä¼°å®Œæˆ:")
        print(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"   F1åˆ†æ•°: {f1_macro:.4f}")
        
        return evaluation_result
    
    def _get_test_demo_data(self) -> Tuple[List[str], List[int]]:
        """è·å–æµ‹è¯•æ¼”ç¤ºæ•°æ®"""
        test_texts = [
            "æƒå¨æœºæ„å‘å¸ƒçš„å®˜æ–¹å£°æ˜",
            "ç½‘ç»œä¸Šæµä¼ çš„æœªè¯å®æ¶ˆæ¯",
            "ä¸“å®¶å­¦è€…çš„ç ”ç©¶è§‚ç‚¹",
            "ç¤¾äº¤åª’ä½“ä¸Šçš„ä¼ è¨€",
            "æ”¿åºœéƒ¨é—¨çš„æ”¿ç­–å…¬å‘Š"
        ] * 10
        
        test_labels = [0, 1, 0, 1, 0] * 10
        
        return test_texts, test_labels
    
    def save_distilled_model(self, model_name: str = "distilled_student") -> None:
        """ä¿å­˜è’¸é¦åçš„æ¨¡å‹"""
        save_path = self.output_dir / model_name
        save_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜å­¦ç”Ÿæ¨¡å‹
        self.student_model.save_pretrained(save_path / "model")
        self.tokenizer.save_pretrained(save_path / "tokenizer")
        
        # ä¿å­˜é…ç½®
        config_dict = {
            'teacher_model': self.config.teacher_model,
            'student_model': self.config.student_model,
            'temperature': self.config.temperature,
            'alpha': self.config.alpha,
            'beta': self.config.beta,
            'use_pruning': self.config.use_pruning,
            'use_quantization': self.config.use_quantization
        }
        
        with open(save_path / "distillation_config.json", 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… è’¸é¦æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    def run_full_distillation(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„è’¸é¦æµç¨‹"""
        print("ğŸš€ å¼€å§‹å®Œæ•´çš„çŸ¥è¯†è’¸é¦æµç¨‹...")
        
        results = {}
        
        try:
            # 1. åŠ è½½æ¨¡å‹
            self.load_models()
            
            # 2. è®­ç»ƒè’¸é¦
            training_history = self.train_distillation()
            results['training_history'] = training_history
            
            # 3. æ¨¡å‹å‹ç¼©
            compression_results = self.compress_model()
            results['compression_results'] = compression_results
            
            # 4. è¯„ä¼°å­¦ç”Ÿæ¨¡å‹
            evaluation_results = self.evaluate_student_model()
            results['evaluation_results'] = evaluation_results
            
            # 5. ä¿å­˜æ¨¡å‹
            self.save_distilled_model()
            
            # 6. æ±‡æ€»ç»“æœ
            results['summary'] = {
                'final_accuracy': evaluation_results['accuracy'],
                'final_f1': evaluation_results['f1_macro'],
                'training_epochs': len(training_history['epochs']),
                'final_train_loss': training_history['train_losses'][-1],
                'final_val_loss': training_history['val_losses'][-1]
            }
            
            if compression_results:
                results['summary'].update({
                    'compression_achieved': True,
                    'original_size_mb': compression_results.get('original_size_mb', 0),
                    'final_size_mb': compression_results.get('quantized_size_mb', compression_results.get('original_size_mb', 0)),
                    'compression_ratio': compression_results.get('compression_ratio', 1.0)
                })
            
            print(f"\nâœ… å®Œæ•´è’¸é¦æµç¨‹å®Œæˆ!")
            print(f"   æœ€ç»ˆå‡†ç¡®ç‡: {results['summary']['final_accuracy']:.4f}")
            print(f"   æœ€ç»ˆF1åˆ†æ•°: {results['summary']['final_f1']:.4f}")
            
            return results
            
        except Exception as e:
            logger.error(f"è’¸é¦æµç¨‹å¤±è´¥: {e}")
            results['error'] = str(e)
            return results


def create_distillation_system(use_compression: bool = True) -> KnowledgeDistiller:
    """
    åˆ›å»ºçŸ¥è¯†è’¸é¦ç³»ç»Ÿçš„ä¾¿æ·å‡½æ•°
    
    Args:
        use_compression: æ˜¯å¦ä½¿ç”¨å‹ç¼©æŠ€æœ¯
        
    Returns:
        çŸ¥è¯†è’¸é¦å™¨å®ä¾‹
    """
    print("ğŸš€ åˆ›å»ºçŸ¥è¯†è’¸é¦ç³»ç»Ÿ...")
    
    config = DistillationConfig(
        teacher_model="Qwen/Qwen3-0.6B",
        student_model="Qwen/Qwen3-0.6B",
        temperature=4.0,
        alpha=0.7,
        num_epochs=3,
        batch_size=4,  # å‡å°æ‰¹æ¬¡å¤§å°
        use_pruning=use_compression,
        use_quantization=use_compression
    )
    
    distiller = KnowledgeDistiller(config)
    
    return distiller


def demo_model_distillation():
    """æ¼”ç¤ºæ¨¡å‹è’¸é¦åŠŸèƒ½"""
    print("ğŸ“ æ¨¡å‹è’¸é¦å’Œå‹ç¼©æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # åˆ›å»ºè’¸é¦ç³»ç»Ÿ
        distiller = create_distillation_system(use_compression=True)
        
        # è¿è¡Œå®Œæ•´è’¸é¦æµç¨‹
        results = distiller.run_full_distillation()
        
        # æ˜¾ç¤ºç»“æœ
        if 'error' not in results:
            summary = results['summary']
            
            print(f"\nğŸ“Š è’¸é¦ç»“æœæ±‡æ€»:")
            print(f"   è®­ç»ƒè½®æ•°: {summary['training_epochs']}")
            print(f"   æœ€ç»ˆå‡†ç¡®ç‡: {summary['final_accuracy']:.4f}")
            print(f"   æœ€ç»ˆF1åˆ†æ•°: {summary['final_f1']:.4f}")
            print(f"   æœ€ç»ˆè®­ç»ƒæŸå¤±: {summary['final_train_loss']:.4f}")
            print(f"   æœ€ç»ˆéªŒè¯æŸå¤±: {summary['final_val_loss']:.4f}")
            
            if summary.get('compression_achieved'):
                print(f"\nğŸ—œï¸  æ¨¡å‹å‹ç¼©ç»“æœ:")
                print(f"   åŸå§‹å¤§å°: {summary['original_size_mb']:.2f} MB")
                print(f"   å‹ç¼©åå¤§å°: {summary['final_size_mb']:.2f} MB")
                print(f"   å‹ç¼©æ¯”: {summary['compression_ratio']:.2f}x")
            
            # ä¿å­˜ç»“æœ
            results_file = distiller.output_dir / "distillation_results.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        else:
            print(f"âŒ è’¸é¦è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {results['error']}")
        
        print(f"\nâœ… æ¨¡å‹è’¸é¦æ¼”ç¤ºå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ è’¸é¦æ¼”ç¤ºå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    demo_model_distillation()