#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# models/llms/open_source_llms.py

"""
å¼€æºå¤§è¯­è¨€æ¨¡å‹å®ç°
ä½¿ç”¨ Qwen3-0.6B è¿›è¡Œè°£è¨€æ£€æµ‹ä»»åŠ¡
æ”¯æŒå¤šç§æ¨ç†æ–¹å¼å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒ
"""

import torch
import torch.nn as nn
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    AutoConfig, BitsAndBytesConfig,
    TrainingArguments, Trainer
)
from peft import LoraConfig, get_peft_model, TaskType
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, classification_report
from tqdm import tqdm
import json
import warnings
from typing import Dict, List, Tuple, Any, Optional, Union
from pathlib import Path
import sys
import logging

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from datasets.data_loaders import create_all_dataloaders
    from utils.config_manager import get_config_manager, get_output_path
    from models.llms.prompt_engineering import RumorPromptTemplate, PromptManager
    USE_PROJECT_MODULES = True
    print("âœ… æˆåŠŸå¯¼å…¥é¡¹ç›®æ¨¡å—")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥é¡¹ç›®æ¨¡å—å¤±è´¥: {e}")
    USE_PROJECT_MODULES = False


class QwenRumorClassifier:
    """åŸºäºQwen3-0.6Bçš„è°£è¨€æ£€æµ‹åˆ†ç±»å™¨"""
    
    def __init__(self, 
                 model_name: str = "Qwen/Qwen3-0.6B",
                 device: str = "auto",
                 load_in_8bit: bool = False,
                 load_in_4bit: bool = False,
                 use_lora: bool = True,
                 max_length: int = 512):
        """
        åˆå§‹åŒ–Qwenè°£è¨€åˆ†ç±»å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            device: è®¡ç®—è®¾å¤‡
            load_in_8bit: æ˜¯å¦ä½¿ç”¨8bité‡åŒ–
            load_in_4bit: æ˜¯å¦ä½¿ç”¨4bité‡åŒ–
            use_lora: æ˜¯å¦ä½¿ç”¨LoRAå¾®è°ƒ
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.model_name = model_name
        self.max_length = max_length
        self.use_lora = use_lora
        
        # è®¾ç½®è®¾å¤‡
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
            
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {model_name}")
        
        # è®¾ç½®é‡åŒ–é…ç½®
        self.quantization_config = None
        if load_in_4bit:
            self.quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            print("ğŸ”§ å¯ç”¨4bité‡åŒ–")
        elif load_in_8bit:
            self.quantization_config = BitsAndBytesConfig(load_in_8bit=True)
            print("ğŸ”§ å¯ç”¨8bité‡åŒ–")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.tokenizer = None
        self.model = None
        self.peft_model = None
        self.prompt_manager = None
        
        # æ ‡ç­¾æ˜ å°„
        self.label_mapping = {0: 'Non-rumor', 1: 'Rumor', 2: 'Unverified'}
        self.reverse_label_mapping = {v: k for k, v in self.label_mapping.items()}
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if USE_PROJECT_MODULES:
            config_manager = get_config_manager()
            self.output_dir = get_output_path('models', 'llms')
        else:
            self.output_dir = Path('outputs/models/llms')
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        self._load_model_and_tokenizer()
        
        # åˆå§‹åŒ–æç¤ºç®¡ç†å™¨
        self._init_prompt_manager()
    
    def _load_model_and_tokenizer(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        try:
            print("ğŸ“¥ åŠ è½½åˆ†è¯å™¨...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                pad_token='<|extra_0|>',
                eos_token='<|im_end|>',
                use_fast=False
            )
            
            # ç¡®ä¿æœ‰pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32,
                quantization_config=self.quantization_config,
                device_map="auto" if self.device.type == 'cuda' else None
            )
            
            # å¦‚æœä¸æ˜¯è‡ªåŠ¨è®¾å¤‡æ˜ å°„ï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
            if self.quantization_config is None and self.device.type != 'cuda':
                self.model = self.model.to(self.device)
            
            # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
            print(f"   è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer)}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _init_prompt_manager(self):
        """åˆå§‹åŒ–æç¤ºç®¡ç†å™¨"""
        try:
            if USE_PROJECT_MODULES:
                self.prompt_manager = PromptManager()
            else:
                # åˆ›å»ºç®€å•çš„æç¤ºç®¡ç†å™¨
                self.prompt_manager = type('PromptManager', (), {
                    'create_classification_prompt': self._create_simple_prompt,
                    'create_few_shot_prompt': self._create_simple_few_shot_prompt
                })()
            
            print("âœ… æç¤ºç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸  æç¤ºç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.prompt_manager = None
    
    def _create_simple_prompt(self, text: str, task_type: str = "classification") -> str:
        """åˆ›å»ºç®€å•çš„æç¤ºæ¨¡æ¿"""
        return f"""è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬æ˜¯å¦ä¸ºè°£è¨€ã€‚

æ–‡æœ¬å†…å®¹: {text}

è¯·ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ªç­”æ¡ˆï¼š
- Non-rumor: éè°£è¨€ï¼Œå†…å®¹çœŸå®å¯ä¿¡
- Rumor: è°£è¨€ï¼Œå†…å®¹è™šå‡æˆ–è¯¯å¯¼
- Unverified: æœªéªŒè¯ï¼Œæ— æ³•ç¡®å®šçœŸä¼ª

ç­”æ¡ˆ: """
    
    def _create_simple_few_shot_prompt(self, text: str, examples: List[Dict] = None) -> str:
        """åˆ›å»ºç®€å•çš„å°‘æ ·æœ¬æç¤º"""
        prompt = "ä»¥ä¸‹æ˜¯ä¸€äº›è°£è¨€æ£€æµ‹çš„ä¾‹å­ï¼š\n\n"
        
        if examples:
            for i, example in enumerate(examples[:3], 1):  # æœ€å¤š3ä¸ªä¾‹å­
                prompt += f"ä¾‹å­{i}:\n"
                prompt += f"æ–‡æœ¬: {example.get('text', '')}\n"
                prompt += f"æ ‡ç­¾: {example.get('label', '')}\n\n"
        else:
            # é»˜è®¤ä¾‹å­
            prompt += """ä¾‹å­1:
æ–‡æœ¬: ç§‘å­¦å®¶å‘ç°æ–°çš„æ²»ç–—æ–¹æ³•ï¼Œä¸´åºŠè¯•éªŒæ˜¾ç¤ºæ˜¾è‘—æ•ˆæœ
æ ‡ç­¾: Non-rumor

ä¾‹å­2:
æ–‡æœ¬: ç½‘ä¼ æŸåœ°å‘ç”Ÿé‡å¤§äº‹æ•…ï¼Œä½†å®˜æ–¹å°šæœªç¡®è®¤
æ ‡ç­¾: Unverified

ä¾‹å­3:
æ–‡æœ¬: è°£ä¼ ç–«è‹—å«æœ‰æœ‰å®³ç‰©è´¨ï¼Œå·²è¢«ç§‘å­¦ç ”ç©¶è¯å®ä¸ºè™šå‡ä¿¡æ¯
æ ‡ç­¾: Rumor

"""
        
        prompt += f"ç°åœ¨è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬:\næ–‡æœ¬: {text}\næ ‡ç­¾: "
        return prompt
    
    def setup_lora(self, 
                   r: int = 16,
                   lora_alpha: int = 32,
                   lora_dropout: float = 0.1,
                   target_modules: List[str] = None) -> None:
        """è®¾ç½®LoRAå‚æ•°é«˜æ•ˆå¾®è°ƒ"""
        if not self.use_lora:
            print("âš ï¸  LoRAæœªå¯ç”¨")
            return
        
        if target_modules is None:
            # Qwenæ¨¡å‹çš„æ³¨æ„åŠ›æ¨¡å—
            target_modules = [
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ]
        
        try:
            print("ğŸ”§ è®¾ç½®LoRAé…ç½®...")
            
            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=r,
                lora_alpha=lora_alpha,
                lora_dropout=lora_dropout,
                target_modules=target_modules,
                bias="none"
            )
            
            self.peft_model = get_peft_model(self.model, lora_config)
            
            # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
            trainable_params = sum(p.numel() for p in self.peft_model.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in self.peft_model.parameters())
            
            print(f"âœ… LoRAè®¾ç½®å®Œæˆ")
            print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
            print(f"   æ€»å‚æ•°: {total_params:,}")
            
        except Exception as e:
            print(f"âŒ LoRAè®¾ç½®å¤±è´¥: {e}")
            self.peft_model = None
    
    def generate_response(self, 
                         prompt: str, 
                         max_new_tokens: int = 50,
                         temperature: float = 0.1,
                         do_sample: bool = True,
                         top_p: float = 0.9) -> str:
        """ç”Ÿæˆæ¨¡å‹å“åº”"""
        try:
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=self.max_length,
                truncation=True,
                padding=True
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # é€‰æ‹©ä½¿ç”¨çš„æ¨¡å‹
            model_to_use = self.peft_model if self.peft_model is not None else self.model
            
            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                outputs = model_to_use.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=do_sample,
                    top_p=top_p,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç å“åº”
            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            ).strip()
            
            return response
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå“åº”å¤±è´¥: {e}")
            return ""
    
    def classify_text(self, text: str, use_few_shot: bool = False, examples: List[Dict] = None) -> Dict[str, Any]:
        """å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œåˆ†ç±»"""
        try:
            # åˆ›å»ºæç¤º
            if use_few_shot and self.prompt_manager:
                prompt = self.prompt_manager.create_few_shot_prompt(text, examples)
            elif self.prompt_manager:
                prompt = self.prompt_manager.create_classification_prompt(text)
            else:
                if use_few_shot:
                    prompt = self._create_simple_few_shot_prompt(text, examples)
                else:
                    prompt = self._create_simple_prompt(text)
            
            # ç”Ÿæˆå“åº”
            response = self.generate_response(prompt, max_new_tokens=20)
            
            # è§£æå“åº”
            predicted_label = self._parse_response(response)
            confidence = self._calculate_confidence(response, predicted_label)
            
            return {
                'text': text,
                'predicted_label': predicted_label,
                'predicted_class': self.label_mapping.get(predicted_label, 'Unknown'),
                'confidence': confidence,
                'raw_response': response,
                'prompt_used': 'few_shot' if use_few_shot else 'standard'
            }
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬åˆ†ç±»å¤±è´¥: {e}")
            return {
                'text': text,
                'predicted_label': 0,  # é»˜è®¤ä¸ºNon-rumor
                'predicted_class': 'Non-rumor',
                'confidence': 0.0,
                'raw_response': '',
                'error': str(e)
            }
    
    def _parse_response(self, response: str) -> int:
        """è§£ææ¨¡å‹å“åº”ï¼Œæå–é¢„æµ‹æ ‡ç­¾"""
        response_lower = response.lower().strip()
        
        # ç›´æ¥åŒ¹é…æ ‡ç­¾
        if 'rumor' in response_lower and 'non-rumor' not in response_lower:
            return 1  # Rumor
        elif 'non-rumor' in response_lower:
            return 0  # Non-rumor
        elif 'unverified' in response_lower:
            return 2  # Unverified
        
        # åŒ¹é…ä¸­æ–‡
        if 'è°£è¨€' in response_lower and 'éè°£è¨€' not in response_lower:
            return 1
        elif 'éè°£è¨€' in response_lower or 'çœŸå®' in response_lower:
            return 0
        elif 'æœªéªŒè¯' in response_lower or 'ä¸ç¡®å®š' in response_lower:
            return 2
        
        # é»˜è®¤è¿”å›Non-rumor
        return 0
    
    def _calculate_confidence(self, response: str, predicted_label: int) -> float:
        """è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦ï¼ˆç®€å•å¯å‘å¼æ–¹æ³•ï¼‰"""
        response_lower = response.lower()
        predicted_class = self.label_mapping[predicted_label].lower()
        
        # å¦‚æœå“åº”ä¸­åŒ…å«é¢„æµ‹çš„ç±»åˆ«ï¼Œç½®ä¿¡åº¦è¾ƒé«˜
        if predicted_class.replace('-', '').replace('_', '') in response_lower.replace('-', '').replace('_', ''):
            return 0.8
        else:
            return 0.5
    
    def batch_classify(self, texts: List[str], use_few_shot: bool = False, 
                      examples: List[Dict] = None, batch_size: int = 8) -> List[Dict[str, Any]]:
        """æ‰¹é‡æ–‡æœ¬åˆ†ç±»"""
        results = []
        
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡åˆ†ç±» {len(texts)} ä¸ªæ–‡æœ¬...")
        
        for i in tqdm(range(0, len(texts), batch_size), desc="æ‰¹é‡åˆ†ç±»"):
            batch_texts = texts[i:i+batch_size]
            
            for text in batch_texts:
                result = self.classify_text(text, use_few_shot, examples)
                results.append(result)
        
        return results
    
    def evaluate_on_dataset(self, use_few_shot: bool = False) -> Dict[str, Any]:
        """åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("ğŸ“Š å¼€å§‹è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        try:
            # åŠ è½½æ•°æ®
            if USE_PROJECT_MODULES:
                dataloaders = create_all_dataloaders(
                    batch_sizes={'train': 32, 'val': 32, 'test': 32}
                )
                
                # æå–æµ‹è¯•æ•°æ®
                test_texts = []
                test_labels = []
                
                for batch in dataloaders['test']:
                    if 'text' in batch:
                        test_texts.extend(batch['text'])
                    elif 'caption' in batch:
                        test_texts.extend(batch['caption'])
                    
                    if 'labels' in batch:
                        test_labels.extend(batch['labels'].tolist())
                    elif 'label' in batch:
                        test_labels.extend(batch['label'])
                
            else:
                # ä½¿ç”¨æ¼”ç¤ºæ•°æ®
                test_texts = [
                    "è¿™æ˜¯ä¸€ä¸ªå…³äºç§‘æŠ€è¿›æ­¥çš„çœŸå®æ–°é—»æŠ¥é“",
                    "ç½‘ä¼ æŸåœ°å‘ç”Ÿé‡å¤§äº‹æ•…ï¼Œä½†å°šæœªå¾—åˆ°å®˜æ–¹ç¡®è®¤",
                    "è°£ä¼ æŸçŸ¥åå…¬å¸å³å°†å€’é—­ï¼Œå·²è¢«å®˜æ–¹è¾Ÿè°£",
                    "ç§‘å­¦ç ”ç©¶è¡¨æ˜æ–°è¯ç‰©å…·æœ‰æ˜¾è‘—ç–—æ•ˆ",
                    "æœªç»è¯å®çš„ä¼ è¨€åœ¨ç¤¾äº¤åª’ä½“å¹¿æ³›ä¼ æ’­"
                ]
                test_labels = [0, 2, 1, 0, 2]
            
            print(f"ğŸ“ æµ‹è¯•æ•°æ®: {len(test_texts)} ä¸ªæ ·æœ¬")
            
            # å°‘æ ·æœ¬ä¾‹å­
            few_shot_examples = [
                {'text': 'å®˜æ–¹å‘å¸ƒçš„æƒå¨æ–°é—»æŠ¥é“', 'label': 'Non-rumor'},
                {'text': 'ç½‘ä¸Šæµä¼ çš„æœªè¯å®è°£è¨€', 'label': 'Rumor'},
                {'text': 'éœ€è¦è¿›ä¸€æ­¥æ ¸å®çš„ä¿¡æ¯', 'label': 'Unverified'}
            ] if use_few_shot else None
            
            # æ‰¹é‡åˆ†ç±»
            results = self.batch_classify(
                test_texts, 
                use_few_shot=use_few_shot, 
                examples=few_shot_examples,
                batch_size=4  # å‡å°æ‰¹æ¬¡å¤§å°
            )
            
            # æå–é¢„æµ‹ç»“æœ
            predictions = [r['predicted_label'] for r in results]
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            accuracy = accuracy_score(test_labels, predictions)
            f1_macro = f1_score(test_labels, predictions, average='macro')
            f1_weighted = f1_score(test_labels, predictions, average='weighted')
            
            # åˆ†ç±»æŠ¥å‘Š
            report = classification_report(
                test_labels, predictions,
                target_names=list(self.label_mapping.values()),
                output_dict=True
            )
            
            evaluation_result = {
                'accuracy': accuracy,
                'f1_macro': f1_macro,
                'f1_weighted': f1_weighted,
                'classification_report': report,
                'num_samples': len(test_texts),
                'use_few_shot': use_few_shot,
                'model_name': self.model_name,
                'predictions': predictions,
                'true_labels': test_labels,
                'detailed_results': results
            }
            
            print(f"âœ… è¯„ä¼°å®Œæˆ:")
            print(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
            print(f"   F1åˆ†æ•°(macro): {f1_macro:.4f}")
            print(f"   F1åˆ†æ•°(weighted): {f1_weighted:.4f}")
            
            return evaluation_result
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def save_model(self, save_path: Optional[str] = None):
        """ä¿å­˜æ¨¡å‹å’Œé…ç½®"""
        if save_path is None:
            save_path = self.output_dir / f"qwen_rumor_classifier"
        
        save_path = Path(save_path)
        save_path.mkdir(parents=True, exist_ok=True)
        
        try:
            # ä¿å­˜åˆ†è¯å™¨
            self.tokenizer.save_pretrained(save_path / "tokenizer")
            
            # ä¿å­˜LoRAæ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if self.peft_model is not None:
                self.peft_model.save_pretrained(save_path / "lora_model")
                print(f"âœ… LoRAæ¨¡å‹å·²ä¿å­˜åˆ°: {save_path / 'lora_model'}")
            else:
                # ä¿å­˜å®Œæ•´æ¨¡å‹
                self.model.save_pretrained(save_path / "model")
                print(f"âœ… å®Œæ•´æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path / 'model'}")
            
            # ä¿å­˜é…ç½®
            config = {
                'model_name': self.model_name,
                'max_length': self.max_length,
                'use_lora': self.use_lora,
                'label_mapping': self.label_mapping,
                'device': str(self.device)
            }
            
            with open(save_path / "config.json", 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… åˆ†è¯å™¨å·²ä¿å­˜åˆ°: {save_path / 'tokenizer'}")
            print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {save_path / 'config.json'}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            raise


def create_qwen_classifier(use_lora: bool = True, 
                          load_in_4bit: bool = False) -> QwenRumorClassifier:
    """åˆ›å»ºQwenè°£è¨€åˆ†ç±»å™¨çš„ä¾¿æ·å‡½æ•°"""
    print("ğŸš€ åˆ›å»ºQwenè°£è¨€åˆ†ç±»å™¨...")
    
    classifier = QwenRumorClassifier(
        model_name="Qwen/Qwen3-0.6B",
        use_lora=use_lora,
        load_in_4bit=load_in_4bit,
        max_length=512
    )
    
    if use_lora:
        classifier.setup_lora(r=16, lora_alpha=32, lora_dropout=0.1)
    
    return classifier


def demo_qwen_classification():
    """æ¼”ç¤ºQwenåˆ†ç±»åŠŸèƒ½"""
    print("ğŸ¯ Qwenè°£è¨€æ£€æµ‹æ¼”ç¤º")
    print("=" * 50)
    
    try:
        # åˆ›å»ºåˆ†ç±»å™¨
        classifier = create_qwen_classifier(use_lora=True, load_in_4bit=False)
        
        # æµ‹è¯•å•ä¸ªæ–‡æœ¬åˆ†ç±»
        test_texts = [
            "ç§‘å­¦å®¶åœ¨å®éªŒå®¤å‘ç°äº†æ–°çš„æ²»ç–—æ–¹æ³•ï¼Œç»è¿‡ä¸¥æ ¼çš„ä¸´åºŠè¯•éªŒè¯å®æœ‰æ•ˆ",
            "ç½‘ä¼ æŸåœ°å‘ç”Ÿé‡å¤§åœ°éœ‡ï¼Œä½†å®˜æ–¹æ°”è±¡å±€å°šæœªå‘å¸ƒç›¸å…³ä¿¡æ¯",
            "è°£ä¼ æ–°å† ç–«è‹—å«æœ‰å¾®èŠ¯ç‰‡ï¼Œè¿™ä¸€è¯´æ³•å·²è¢«å¤šé¡¹ç§‘å­¦ç ”ç©¶è¯æ˜ä¸ºè™šå‡ä¿¡æ¯"
        ]
        
        print("\nğŸ” å•ä¸ªæ–‡æœ¬åˆ†ç±»æµ‹è¯•:")
        for i, text in enumerate(test_texts, 1):
            print(f"\næ–‡æœ¬ {i}: {text}")
            result = classifier.classify_text(text)
            print(f"é¢„æµ‹: {result['predicted_class']} (ç½®ä¿¡åº¦: {result['confidence']:.2f})")
            print(f"åŸå§‹å“åº”: {result['raw_response']}")
        
        # å°‘æ ·æœ¬å­¦ä¹ æµ‹è¯•
        print(f"\nğŸ¯ å°‘æ ·æœ¬å­¦ä¹ æµ‹è¯•:")
        few_shot_examples = [
            {'text': 'æ”¿åºœå®˜æ–¹å‘å¸ƒçš„æƒå¨å£°æ˜', 'label': 'Non-rumor'},
            {'text': 'ç½‘ç»œä¸Šæµä¼ çš„æœªç»è¯å®çš„ä¼ è¨€', 'label': 'Rumor'}
        ]
        
        test_text = "ä¸“å®¶å­¦è€…åœ¨å­¦æœ¯æœŸåˆŠä¸Šå‘è¡¨çš„ç ”ç©¶æˆæœ"
        result = classifier.classify_text(test_text, use_few_shot=True, examples=few_shot_examples)
        print(f"æ–‡æœ¬: {test_text}")
        print(f"å°‘æ ·æœ¬é¢„æµ‹: {result['predicted_class']} (ç½®ä¿¡åº¦: {result['confidence']:.2f})")
        
        # æ•°æ®é›†è¯„ä¼°
        print(f"\nğŸ“Š æ•°æ®é›†è¯„ä¼°:")
        eval_result = classifier.evaluate_on_dataset(use_few_shot=False)
        print(f"æ ‡å‡†æç¤ºå‡†ç¡®ç‡: {eval_result['accuracy']:.4f}")
        
        eval_result_few_shot = classifier.evaluate_on_dataset(use_few_shot=True)
        print(f"å°‘æ ·æœ¬æç¤ºå‡†ç¡®ç‡: {eval_result_few_shot['accuracy']:.4f}")
        
        # ä¿å­˜æ¨¡å‹
        print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
        classifier.save_model()
        
        print(f"\nâœ… Qwenåˆ†ç±»æ¼”ç¤ºå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    demo_qwen_classification()