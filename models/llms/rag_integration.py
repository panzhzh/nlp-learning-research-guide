#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# models/llms/rag_integration.py

"""
RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) é›†æˆæ¨¡å—
ç»“åˆæ£€ç´¢å’Œç”Ÿæˆï¼Œæå‡è°£è¨€æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§
åŸºäºQwen3-0.6Bæ¨¡å‹å®ç°ï¼Œæ”¯æŒåŠ¨æ€çŸ¥è¯†æ£€ç´¢
"""

import torch
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union
from pathlib import Path
import sys
import json
import warnings
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import logging

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å°è¯•å¯¼å…¥sentence_transformerså’Œfaissï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
HAS_SENTENCE_TRANSFORMERS = True

try:
    import faiss
    HAS_FAISS = True
    print("âœ… æˆåŠŸå¯¼å…¥faiss")
except ImportError:
    print("âš ï¸  faisså¯¼å…¥å¤±è´¥ï¼Œå°†ä½¿ç”¨sklearn")
    HAS_FAISS = False

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from data_utils.data_loaders import create_all_dataloaders
    from utils.config_manager import get_config_manager, get_output_path
    from models.llms.open_source_llms import QwenRumorClassifier
    from models.llms.prompt_engineering import PromptManager
    USE_PROJECT_MODULES = True
    print("âœ… æˆåŠŸå¯¼å…¥é¡¹ç›®æ¨¡å—")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥é¡¹ç›®æ¨¡å—å¤±è´¥: {e}")
    USE_PROJECT_MODULES = False


class KnowledgeBase:
    """çŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    def __init__(self, knowledge_sources: Optional[List[str]] = None):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“
        
        Args:
            knowledge_sources: çŸ¥è¯†æ¥æºåˆ—è¡¨
        """
        self.knowledge_sources = knowledge_sources or ['dataset', 'predefined']
        self.documents = []
        self.embeddings = None
        self.index = None
        self.vectorizer = None
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embedding_model = None
        print("âš ï¸  ä½¿ç”¨TF-IDFè¿›è¡Œæ–‡æ¡£æ£€ç´¢ï¼ˆé¿å…ä¾èµ–å†²çªï¼‰")
        
        # æ„å»ºçŸ¥è¯†åº“
        self._build_knowledge_base()
        
    def _build_knowledge_base(self):
        """æ„å»ºçŸ¥è¯†åº“"""
        print("ğŸ”„ æ„å»ºçŸ¥è¯†åº“...")
        
        # æ·»åŠ é¢„å®šä¹‰çš„è°£è¨€æ£€æµ‹çŸ¥è¯†
        self._add_predefined_knowledge()
        
        # ä»æ•°æ®é›†æ·»åŠ çŸ¥è¯†
        if 'dataset' in self.knowledge_sources and USE_PROJECT_MODULES:
            self._add_dataset_knowledge()
        
        # æ„å»ºç´¢å¼•
        self._build_index()
        
        print(f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆ: {len(self.documents)} ä¸ªæ–‡æ¡£")
    
    def _add_predefined_knowledge(self):
        """æ·»åŠ é¢„å®šä¹‰çš„è°£è¨€æ£€æµ‹çŸ¥è¯†"""
        predefined_docs = [
            {
                'content': 'æƒå¨æœºæ„å‘å¸ƒçš„å®˜æ–¹ä¿¡æ¯é€šå¸¸å…·æœ‰é«˜å¯ä¿¡åº¦ï¼Œå¦‚æ”¿åºœéƒ¨é—¨ã€ç§‘ç ”æœºæ„ã€çŸ¥ååª’ä½“çš„æ­£å¼å£°æ˜ã€‚',
                'type': 'guideline',
                'category': 'credibility',
                'label': 'Non-rumor'
            },
            {
                'content': 'åŒ…å«"æ®ä¸å®Œå…¨ç»Ÿè®¡"ã€"æœ‰æ¶ˆæ¯ç§°"ã€"ç½‘ä¼ "ç­‰æ¨¡ç³Šè¡¨è¿°çš„ä¿¡æ¯éœ€è¦è°¨æ…å¯¹å¾…ï¼Œå¯èƒ½ç¼ºä¹äº‹å®ä¾æ®ã€‚',
                'type': 'guideline', 
                'category': 'language_pattern',
                'label': 'Unverified'
            },
            {
                'content': 'æ˜æ˜¾å¤¸å¤§äº‹å®ã€ä½¿ç”¨æç«¯è¯æ±‡ã€ç¼ºä¹å…·ä½“æ—¶é—´åœ°ç‚¹çš„ä¿¡æ¯å¾€å¾€æ˜¯è°£è¨€çš„ç‰¹å¾ã€‚',
                'type': 'guideline',
                'category': 'rumor_pattern',
                'label': 'Rumor'
            },
            {
                'content': 'ç§‘å­¦ç ”ç©¶éœ€è¦åŒè¡Œè¯„è®®å’Œå¤šæ¬¡éªŒè¯ï¼Œå•ä¸€ç ”ç©¶ç»“æœä¸è¶³ä»¥å¾—å‡ºç»å¯¹ç»“è®ºã€‚',
                'type': 'guideline',
                'category': 'scientific_method',
                'label': 'Non-rumor'
            },
            {
                'content': 'ç¤¾äº¤åª’ä½“ä¸Šæµä¼ çš„æœªç»è¯å®çš„æ¶ˆæ¯ï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠå¥åº·ã€å®‰å…¨ç­‰æ•æ„Ÿè¯é¢˜çš„ï¼Œéœ€è¦å®˜æ–¹ç¡®è®¤ã€‚',
                'type': 'guideline',
                'category': 'social_media',
                'label': 'Unverified'
            },
            {
                'content': 'è°£è¨€å¾€å¾€åˆ©ç”¨äººä»¬çš„ææƒ§å¿ƒç†ï¼Œä½¿ç”¨"ç´§æ€¥"ã€"å±é™©"ã€"ç«‹å³"ç­‰è¯æ±‡åˆ¶é€ ç´§è¿«æ„Ÿã€‚',
                'type': 'guideline',
                'category': 'psychological_manipulation',
                'label': 'Rumor'
            },
            {
                'content': 'å¯ä»¥é€šè¿‡æŸ¥è¯å®˜æ–¹ç½‘ç«™ã€æƒå¨åª’ä½“æŠ¥é“ã€ä¸“å®¶æ„è§ç­‰å¤šä¸ªæ¸ é“æ¥éªŒè¯ä¿¡æ¯çœŸå®æ€§ã€‚',
                'type': 'verification_method',
                'category': 'fact_checking',
                'label': 'Non-rumor'
            },
            {
                'content': 'åŒ»å­¦å¥åº·ä¿¡æ¯åº”è¯¥æ¥æºäºæ­£è§„åŒ»ç–—æœºæ„ã€åŒ»å­¦æœŸåˆŠæˆ–æ‰§ä¸šåŒ»å¸ˆï¼Œé¿å…ä¼ æ’­æœªç»éªŒè¯çš„åæ–¹ã€‚',
                'type': 'domain_specific',
                'category': 'health',
                'label': 'Non-rumor'
            },
            {
                'content': 'è‡ªç„¶ç¾å®³é¢„è­¦ä¿¡æ¯åº”ä»¥æ°”è±¡å±€ã€åœ°éœ‡å±€ç­‰å®˜æ–¹æœºæ„å‘å¸ƒä¸ºå‡†ï¼Œéå®˜æ–¹é¢„æµ‹ä¸å¯ä¿¡ã€‚',
                'type': 'domain_specific',
                'category': 'disaster',
                'label': 'Rumor'
            },
            {
                'content': 'ç»æµé‡‘èä¿¡æ¯åº”å…³æ³¨å‘å¸ƒæœºæ„çš„æƒå¨æ€§ï¼Œé¿å…è¢«è™šå‡æŠ•èµ„ä¿¡æ¯è¯¯å¯¼ã€‚',
                'type': 'domain_specific',
                'category': 'finance',
                'label': 'Unverified'
            }
        ]
        
        self.documents.extend(predefined_docs)
        print(f"ğŸ“š æ·»åŠ é¢„å®šä¹‰çŸ¥è¯†: {len(predefined_docs)} ä¸ªæ–‡æ¡£")
    
    def _add_dataset_knowledge(self):
        """ä»è®­ç»ƒé›†æ·»åŠ çŸ¥è¯† - åªä½¿ç”¨è®­ç»ƒé›†æ„å»ºçŸ¥è¯†åº“"""
        try:
            # åªåŠ è½½è®­ç»ƒé›†æ•°æ®ç”¨äºæ„å»ºçŸ¥è¯†åº“
            print("ğŸ“Š ä»è®­ç»ƒé›†æ„å»ºçŸ¥è¯†åº“...")
            dataloaders = create_all_dataloaders(
                batch_sizes={'train': 32, 'val': 32, 'test': 32}
            )
            
            # åªä»è®­ç»ƒé›†æå–æ ·æœ¬ä½œä¸ºçŸ¥è¯†åº“
            train_loader = dataloaders['train']
            sample_count = 0
            
            for batch in train_loader:
                # ä½¿ç”¨captionå­—æ®µï¼ˆè¿™æ˜¯å®é™…çš„æ–‡æœ¬å†…å®¹ï¼‰
                captions = batch.get('caption', batch.get('text', []))
                labels = batch.get('labels', batch.get('label', []))
                
                if hasattr(labels, 'tolist'):
                    labels = labels.tolist()
                
                for caption, label in zip(captions, labels):
                    if caption and len(caption.strip()) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬
                        label_map = {0: 'Non-rumor', 1: 'Rumor', 2: 'Unverified'}
                        doc = {
                            'content': caption.strip(),
                            'type': 'train_example',
                            'category': 'dataset_sample',
                            'label': label_map.get(label, 'Unknown'),
                            'source': 'train_set'
                        }
                        self.documents.append(doc)
                        sample_count += 1
                        
                        # é™åˆ¶æ•°é‡é¿å…çŸ¥è¯†åº“è¿‡å¤§ï¼Œä½†ä¿æŒè¶³å¤Ÿçš„æ ·æœ¬
                        if sample_count >= 200:
                            break
                
                if sample_count >= 200:
                    break
            
            print(f"ğŸ“Š ä»è®­ç»ƒé›†æ·»åŠ çŸ¥è¯†: {sample_count} ä¸ªæ ·æœ¬")
            
            # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒ
            train_labels = [doc['label'] for doc in self.documents if doc.get('source') == 'train_set']
            from collections import Counter
            label_dist = Counter(train_labels)
            print(f"ğŸ“Š è®­ç»ƒé›†çŸ¥è¯†åº“æ ‡ç­¾åˆ†å¸ƒ: {dict(label_dist)}")
            
        except Exception as e:
            logger.warning(f"ä»è®­ç»ƒé›†æ·»åŠ çŸ¥è¯†å¤±è´¥: {e}")
            print(f"âš ï¸  ä»è®­ç»ƒé›†æ„å»ºçŸ¥è¯†åº“å¤±è´¥: {e}")
            print("    å°†ç»§ç»­ä½¿ç”¨é¢„å®šä¹‰çŸ¥è¯†")
    
    def _build_index(self):
        """æ„å»ºæ–‡æ¡£ç´¢å¼•"""
        if not self.documents:
            logger.warning("æ²¡æœ‰æ–‡æ¡£å¯ä»¥ç´¢å¼•")
            return
        
        # æå–æ–‡æ¡£å†…å®¹
        doc_contents = [doc['content'] for doc in self.documents]
        
        # ä½¿ç”¨TF-IDF
        print("ğŸ”„ è®¡ç®—TF-IDFå‘é‡...")
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 2),
            stop_words=None,  # ä¸ä½¿ç”¨è‹±æ–‡åœç”¨è¯ï¼Œå› ä¸ºæœ‰ä¸­æ–‡å†…å®¹
            min_df=1,         # é™ä½æœ€å°æ–‡æ¡£é¢‘ç‡
            max_df=0.95,      # è®¾ç½®æœ€å¤§æ–‡æ¡£é¢‘ç‡
            token_pattern=r'(?u)\b\w+\b',  # æ”¯æŒä¸­æ–‡å­—ç¬¦
            lowercase=True,
            analyzer='word'
        )
        self.embeddings = self.vectorizer.fit_transform(doc_contents)
        print(f"âœ… TF-IDFç´¢å¼•æ„å»ºå®Œæˆï¼Œç‰¹å¾ç»´åº¦: {self.embeddings.shape}")
    
    def retrieve(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªç»“æœ
            
        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        if not self.documents:
            print("âš ï¸  çŸ¥è¯†åº“ä¸ºç©ºï¼Œæ— æ³•æ£€ç´¢")
            return []
        
        try:
            # TF-IDFæ£€ç´¢
            if self.vectorizer is None:
                print("âš ï¸  TF-IDFå‘é‡åŒ–å™¨æœªåˆå§‹åŒ–")
                return []
            
            try:
                query_vector = self.vectorizer.transform([query])
                similarities = cosine_similarity(query_vector, self.embeddings).flatten()
                
                # è°ƒè¯•ä¿¡æ¯
                print(f"ğŸ” è°ƒè¯•ä¿¡æ¯: æŸ¥è¯¢='{query[:30]}...', ç›¸ä¼¼åº¦èŒƒå›´=[{similarities.min():.4f}, {similarities.max():.4f}]")
                print(f"ğŸ” æŸ¥è¯¢å‘é‡éé›¶å…ƒç´ : {query_vector.nnz}, æ–‡æ¡£çŸ©é˜µå½¢çŠ¶: {self.embeddings.shape}")
                
                # å¦‚æœæ‰€æœ‰ç›¸ä¼¼åº¦éƒ½æ˜¯0ï¼Œå°è¯•æŸ¥çœ‹è¯æ±‡è¡¨
                if similarities.max() == 0.0:
                    query_terms = self.vectorizer.get_feature_names_out()
                    query_tokens = query.split()
                    print(f"ğŸ” æŸ¥è¯¢è¯æ±‡: {query_tokens[:5]}")
                    print(f"ğŸ” TF-IDFè¯æ±‡è¡¨å¤§å°: {len(query_terms)}")
                    
                    # æ£€æŸ¥æ–‡æ¡£å†…å®¹
                    if len(self.documents) > 0:
                        print(f"ğŸ” ç¬¬ä¸€ä¸ªæ–‡æ¡£: {self.documents[0]['content'][:50]}...")
                
                # è·å–top_kä¸ªæœ€ç›¸ä¼¼çš„æ–‡æ¡£
                top_indices = np.argsort(similarities)[-top_k:][::-1]
                
                results = []
                for i, idx in enumerate(top_indices):
                    if idx < len(self.documents):  # ç§»é™¤ç›¸ä¼¼åº¦>0çš„é™åˆ¶
                        doc = self.documents[idx].copy()
                        doc['score'] = float(similarities[idx])
                        doc['rank'] = i + 1
                        results.append(doc)
                
                print(f"ğŸ” æ‰¾åˆ° {len(results)} ä¸ªæ–‡æ¡£ï¼Œæœ€é«˜ç›¸ä¼¼åº¦: {similarities[top_indices[0]] if len(top_indices) > 0 else 0:.4f}")
                
                return results
                
            except Exception as e:
                print(f"âš ï¸  TF-IDFæ£€ç´¢å¤±è´¥: {e}")
                # è¿”å›å‰å‡ ä¸ªæ–‡æ¡£ä½œä¸ºå¤‡ç”¨
                results = []
                for i, doc in enumerate(self.documents[:min(top_k, 3)]):
                    doc_copy = doc.copy()
                    doc_copy['score'] = 0.1  # ç»™ä¸€ä¸ªè¾ƒä½çš„é»˜è®¤åˆ†æ•°
                    doc_copy['rank'] = i + 1
                    results.append(doc_copy)
                return results
                
        except Exception as e:
            logger.error(f"æ£€ç´¢å¤±è´¥: {e}")
            print(f"âš ï¸  æ£€ç´¢è¿‡ç¨‹å‡ºé”™: {e}")
            # è¿”å›å‰å‡ ä¸ªæ–‡æ¡£ä½œä¸ºå¤‡ç”¨
            results = []
            for i, doc in enumerate(self.documents[:min(top_k, 3)]):
                doc_copy = doc.copy()
                doc_copy['score'] = 0.1  # ç»™ä¸€ä¸ªè¾ƒä½çš„é»˜è®¤åˆ†æ•°
                doc_copy['rank'] = i + 1
                results.append(doc_copy)
            return results


class RAGRumorDetector:
    """åŸºäºRAGçš„è°£è¨€æ£€æµ‹å™¨"""
    
    def __init__(self, 
                 llm_model: Optional[QwenRumorClassifier] = None,
                 knowledge_base: Optional[KnowledgeBase] = None):
        """
        åˆå§‹åŒ–RAGè°£è¨€æ£€æµ‹å™¨
        
        Args:
            llm_model: LLMæ¨¡å‹å®ä¾‹
            knowledge_base: çŸ¥è¯†åº“å®ä¾‹
        """
        # åˆå§‹åŒ–LLMæ¨¡å‹
        if llm_model is None:
            try:
                from models.llms.open_source_llms import create_qwen_classifier
                self.llm_model = create_qwen_classifier(use_lora=True, load_in_4bit=False)
                print("âœ… æˆåŠŸåˆ›å»ºQwenåˆ†ç±»å™¨")
            except Exception as e:
                print(f"âš ï¸  LLMæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
                self.llm_model = None
        else:
            self.llm_model = llm_model
        
        # åˆå§‹åŒ–çŸ¥è¯†åº“
        if knowledge_base is None:
            self.knowledge_base = KnowledgeBase()
        else:
            self.knowledge_base = knowledge_base
        
        # åˆå§‹åŒ–æç¤ºç®¡ç†å™¨
        if USE_PROJECT_MODULES:
            self.prompt_manager = PromptManager()
        else:
            self.prompt_manager = None
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if USE_PROJECT_MODULES:
            config_manager = get_config_manager()
            self.output_dir = get_output_path('models', 'llms')
        else:
            self.output_dir = Path('outputs/models/llms')
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print("ğŸ¤– RAGè°£è¨€æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def create_rag_prompt(self, query_text: str, retrieved_docs: List[Dict]) -> str:
        """
        åˆ›å»ºRAGæç¤º
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            retrieved_docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£
            
        Returns:
            RAGæç¤ºå­—ç¬¦ä¸²
        """
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        context_parts = []
        for i, doc in enumerate(retrieved_docs[:3], 1):  # æœ€å¤šä½¿ç”¨å‰3ä¸ªæ–‡æ¡£
            context_parts.append(
                f"å‚è€ƒ{i}ï¼š{doc['content']} "
                f"(ç±»å‹: {doc.get('type', 'æœªçŸ¥')}, "
                f"æ ‡ç­¾: {doc.get('label', 'æœªçŸ¥')}, "
                f"ç›¸å…³åº¦: {doc.get('score', 0):.3f})"
            )
        
        context = "\n".join(context_parts)
        
        # åˆ›å»ºRAGæç¤º
        prompt = f"""åŸºäºä»¥ä¸‹ç›¸å…³ä¿¡æ¯å’Œä½ çš„çŸ¥è¯†ï¼Œè¯·åˆ†æè¿™æ®µæ–‡æœ¬æ˜¯å¦ä¸ºè°£è¨€ã€‚

ç›¸å…³å‚è€ƒä¿¡æ¯ï¼š
{context}

å¾…åˆ†ææ–‡æœ¬ï¼š{query_text}

è¯·æ ¹æ®å‚è€ƒä¿¡æ¯å’Œè°£è¨€æ£€æµ‹çŸ¥è¯†ï¼Œä»ä»¥ä¸‹ä¸‰ä¸ªç±»åˆ«ä¸­é€‰æ‹©ï¼š
1. Non-rumor (éè°£è¨€): å†…å®¹çœŸå®å¯ä¿¡ï¼Œæœ‰å¯é ä¾æ®
2. Rumor (è°£è¨€): å†…å®¹è™šå‡æˆ–è¯¯å¯¼ï¼Œç¼ºä¹äº‹å®æ”¯æ’‘  
3. Unverified (æœªéªŒè¯): æ— æ³•ç¡®å®šçœŸä¼ªï¼Œéœ€è¦è¿›ä¸€æ­¥æ ¸å®

è¯·è¯´æ˜ä½ çš„åˆ†æç†ç”±ï¼Œå¹¶ç»™å‡ºæœ€ç»ˆåˆ†ç±»ã€‚

åˆ†æï¼š"""

        return prompt
    
    def retrieve_and_generate(self, query_text: str, 
                            retrieve_top_k: int = 5,
                            use_context: bool = True) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ£€ç´¢å¢å¼ºç”Ÿæˆ
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            retrieve_top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            use_context: æ˜¯å¦ä½¿ç”¨æ£€ç´¢ä¸Šä¸‹æ–‡
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        try:
            # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
            retrieved_docs = self.knowledge_base.retrieve(query_text, retrieve_top_k)
            
            # 2. ç”Ÿæˆå¢å¼ºæç¤º
            if use_context and retrieved_docs:
                prompt = self.create_rag_prompt(query_text, retrieved_docs)
                generation_type = "rag_enhanced"
            else:
                # ä¸ä½¿ç”¨ä¸Šä¸‹æ–‡çš„æ ‡å‡†æç¤º
                if self.prompt_manager:
                    prompt = self.prompt_manager.create_classification_prompt(query_text)
                else:
                    prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬æ˜¯å¦ä¸ºè°£è¨€ï¼š\n\n{query_text}\n\nåˆ†ç±»ï¼š"
                generation_type = "standard"
            
            # 3. LLMç”Ÿæˆ
            if self.llm_model:
                response = self.llm_model.generate_response(
                    prompt, 
                    max_new_tokens=200,
                    temperature=0.3
                )
                
                # è§£æå“åº”
                predicted_label = self._parse_rag_response(response)
                confidence = self._calculate_rag_confidence(response, retrieved_docs)
            else:
                response = "æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆå“åº”"
                predicted_label = 0
                confidence = 0.0
            
            # 4. æ„å»ºç»“æœ
            result = {
                'query_text': query_text,
                'retrieved_docs': retrieved_docs,
                'retrieved_count': len(retrieved_docs),
                'prompt': prompt,
                'raw_response': response,
                'predicted_label': predicted_label,
                'predicted_class': {0: 'Non-rumor', 1: 'Rumor', 2: 'Unverified'}.get(predicted_label, 'Unknown'),
                'confidence': confidence,
                'generation_type': generation_type,
                'context_used': use_context and len(retrieved_docs) > 0
            }
            
            return result
            
        except Exception as e:
            logger.error(f"RAGå¤„ç†å¤±è´¥: {e}")
            return {
                'query_text': query_text,
                'error': str(e),
                'predicted_label': 0,
                'predicted_class': 'Non-rumor',
                'confidence': 0.0
            }
    
    def _parse_rag_response(self, response: str) -> int:
        """è§£æRAGå“åº”"""
        response_lower = response.lower()
        
        # æ£€æŸ¥æ˜ç¡®çš„åˆ†ç±»æ ‡å¿—
        if 'rumor' in response_lower and 'non-rumor' not in response_lower:
            return 1  # Rumor
        elif 'non-rumor' in response_lower:
            return 0  # Non-rumor
        elif 'unverified' in response_lower:
            return 2  # Unverified
        
        # æ£€æŸ¥ä¸­æ–‡æ ‡å¿—
        if 'è°£è¨€' in response_lower and 'éè°£è¨€' not in response_lower:
            return 1
        elif 'éè°£è¨€' in response_lower or 'çœŸå®' in response_lower:
            return 0
        elif 'æœªéªŒè¯' in response_lower or 'ä¸ç¡®å®š' in response_lower:
            return 2
        
        # æ£€æŸ¥å…³é”®è¯
        rumor_keywords = ['è™šå‡', 'è¯¯å¯¼', 'ä¸å®', 'é”™è¯¯']
        non_rumor_keywords = ['å¯ä¿¡', 'çœŸå®', 'æ­£ç¡®', 'å®˜æ–¹']
        unverified_keywords = ['éœ€è¦', 'æ ¸å®', 'ç¡®è®¤', 'è¯å®']
        
        for keyword in rumor_keywords:
            if keyword in response_lower:
                return 1
        
        for keyword in non_rumor_keywords:
            if keyword in response_lower:
                return 0
        
        for keyword in unverified_keywords:
            if keyword in response_lower:
                return 2
        
        # é»˜è®¤è¿”å›Non-rumor
        return 0
    
    def _calculate_rag_confidence(self, response: str, retrieved_docs: List[Dict]) -> float:
        """è®¡ç®—RAGç½®ä¿¡åº¦"""
        base_confidence = 0.6
        
        # å¦‚æœä½¿ç”¨äº†æ£€ç´¢ä¸Šä¸‹æ–‡
        if retrieved_docs:
            # æ£€ç´¢è´¨é‡åŠ åˆ†
            avg_score = np.mean([doc.get('score', 0) for doc in retrieved_docs])
            base_confidence += avg_score * 0.2
            
            # ä¸€è‡´æ€§æ£€æŸ¥
            response_lower = response.lower()
            consistent_docs = 0
            
            for doc in retrieved_docs[:3]:  # æ£€æŸ¥å‰3ä¸ªæ–‡æ¡£
                doc_label = doc.get('label', '').lower()
                if (('non-rumor' in doc_label and 'non-rumor' in response_lower) or
                    ('rumor' in doc_label and 'rumor' in response_lower and 'non-rumor' not in response_lower) or
                    ('unverified' in doc_label and 'unverified' in response_lower)):
                    consistent_docs += 1
            
            if len(retrieved_docs) > 0:
                consistency_ratio = consistent_docs / min(3, len(retrieved_docs))
                base_confidence += consistency_ratio * 0.15
        
        # å“åº”è´¨é‡è¯„ä¼°
        if len(response) > 50:  # è¯¦ç»†çš„åˆ†æ
            base_confidence += 0.05
        
        if any(keyword in response.lower() for keyword in ['å› ä¸º', 'ç”±äº', 'æ ¹æ®', 'because', 'since']):
            base_confidence += 0.05  # åŒ…å«è§£é‡Š
        
        return min(base_confidence, 1.0)
    
    def batch_analyze(self, texts: List[str], **kwargs) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡åˆ†ææ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        results = []
        
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡RAGåˆ†æ {len(texts)} ä¸ªæ–‡æœ¬...")
        
        for i, text in enumerate(texts):
            print(f"  å¤„ç† {i+1}/{len(texts)}: {text[:50]}...")
            result = self.retrieve_and_generate(text, **kwargs)
            results.append(result)
        
        return results
    
    def evaluate_rag_performance(self, test_data: Optional[List[Tuple[str, int]]] = None) -> Dict[str, Any]:
        """
        è¯„ä¼°RAGæ€§èƒ½
        
        Args:
            test_data: æµ‹è¯•æ•°æ® [(text, true_label), ...]
            
        Returns:
            è¯„ä¼°ç»“æœ
        """
        if test_data is None:
            # ä½¿ç”¨é»˜è®¤æµ‹è¯•æ•°æ®
            test_data = [
                ("ç§‘å­¦å®¶åœ¨å®éªŒå®¤å‘ç°æ–°çš„æ²»ç–—æ–¹æ³•ï¼Œå·²é€šè¿‡åŒè¡Œè¯„è®®å‘è¡¨", 0),  # Non-rumor
                ("ç½‘ä¼ æŸåœ°æ˜å¤©å‘ç”Ÿå¤§åœ°éœ‡ï¼Œè¯·å¤§å®¶æå‰æ’¤ç¦»", 1),  # Rumor
                ("æ®æ¶ˆæ¯äººå£«é€éœ²ï¼ŒæŸå…¬å¸å¯èƒ½è¿›è¡Œé‡ç»„", 2),  # Unverified
                ("æƒå¨åŒ»å­¦æœŸåˆŠå‘è¡¨ç ”ç©¶æ˜¾ç¤ºæ–°è¯ç‰©ç–—æ•ˆæ˜¾è‘—", 0),  # Non-rumor
                ("æœ‹å‹åœˆæµä¼ çš„å…»ç”Ÿåæ–¹èƒ½æ²»æ„ˆæ‰€æœ‰ç–¾ç—…", 1)   # Rumor
            ]
        
        print(f"ğŸ“Š å¼€å§‹RAGæ€§èƒ½è¯„ä¼°ï¼Œæµ‹è¯•æ ·æœ¬: {len(test_data)}")
        
        # æµ‹è¯•æ ‡å‡†æ¨¡å¼
        print("ğŸ” æµ‹è¯•æ ‡å‡†æ¨¡å¼...")
        standard_results = []
        for text, true_label in test_data:
            result = self.retrieve_and_generate(text, use_context=False)
            standard_results.append({
                'text': text,
                'true_label': true_label,
                'predicted_label': result['predicted_label'],
                'confidence': result['confidence'],
                'correct': result['predicted_label'] == true_label
            })
        
        # æµ‹è¯•RAGæ¨¡å¼
        print("ğŸ” æµ‹è¯•RAGå¢å¼ºæ¨¡å¼...")
        rag_results = []
        for text, true_label in test_data:
            result = self.retrieve_and_generate(text, use_context=True)
            rag_results.append({
                'text': text,
                'true_label': true_label,
                'predicted_label': result['predicted_label'],
                'confidence': result['confidence'],
                'retrieved_count': result['retrieved_count'],
                'context_used': result['context_used'],
                'correct': result['predicted_label'] == true_label
            })
        
        # è®¡ç®—æŒ‡æ ‡
        standard_accuracy = np.mean([r['correct'] for r in standard_results])
        rag_accuracy = np.mean([r['correct'] for r in rag_results])
        
        standard_confidence = np.mean([r['confidence'] for r in standard_results])
        rag_confidence = np.mean([r['confidence'] for r in rag_results])
        
        evaluation = {
            'test_samples': len(test_data),
            'standard_mode': {
                'accuracy': standard_accuracy,
                'avg_confidence': standard_confidence,
                'results': standard_results
            },
            'rag_mode': {
                'accuracy': rag_accuracy,
                'avg_confidence': rag_confidence,
                'avg_retrieved_docs': np.mean([r['retrieved_count'] for r in rag_results]),
                'context_usage_rate': np.mean([r['context_used'] for r in rag_results]),
                'results': rag_results
            },
            'improvement': {
                'accuracy_gain': rag_accuracy - standard_accuracy,
                'confidence_gain': rag_confidence - standard_confidence
            }
        }
        
        print(f"âœ… RAGè¯„ä¼°å®Œæˆ:")
        print(f"   æ ‡å‡†æ¨¡å¼å‡†ç¡®ç‡: {standard_accuracy:.4f}")
        print(f"   RAGæ¨¡å¼å‡†ç¡®ç‡: {rag_accuracy:.4f}")
        print(f"   å‡†ç¡®ç‡æå‡: {evaluation['improvement']['accuracy_gain']:+.4f}")
        
        return evaluation
    
    def save_knowledge_base(self, save_path: Optional[str] = None):
        """ä¿å­˜çŸ¥è¯†åº“"""
        if save_path is None:
            save_path = self.output_dir / "rag_knowledge_base.json"
        
        # ä¿å­˜æ–‡æ¡£æ•°æ®ï¼ˆä¸åŒ…å«åµŒå…¥å‘é‡ï¼‰
        knowledge_data = {
            'documents': self.knowledge_base.documents,
            'knowledge_sources': self.knowledge_base.knowledge_sources,
            'total_documents': len(self.knowledge_base.documents),
            'embedding_model': 'all-MiniLM-L6-v2' if self.knowledge_base.embedding_model else 'TF-IDF'
        }
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(knowledge_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ çŸ¥è¯†åº“å·²ä¿å­˜åˆ°: {save_path}")


def demo_rag_integration():
    """æ¼”ç¤ºRAGé›†æˆåŠŸèƒ½"""
    print("ğŸ¤– RAGé›†æˆåŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # åˆ›å»ºRAGæ£€æµ‹å™¨
        print("ğŸ”„ åˆå§‹åŒ–RAGæ£€æµ‹å™¨...")
        rag_detector = RAGRumorDetector()
        
        # æµ‹è¯•å•ä¸ªæ–‡æœ¬åˆ†æ
        test_texts = [
            "ä¸­å›½ç§‘å­¦é™¢å‘å¸ƒæœ€æ–°ç ”ç©¶æˆæœï¼Œåœ¨é‡å­è®¡ç®—é¢†åŸŸå–å¾—é‡å¤§çªç ´",
            "ç½‘ä¼ æŸå¸‚æ˜å¤©å°†å‘ç”Ÿ8çº§å¤§åœ°éœ‡ï¼Œè¯·å¤§å®¶åšå¥½æ’¤ç¦»å‡†å¤‡",
            "æ®ä¸šå†…äººå£«é€éœ²ï¼ŒæŸäº’è”ç½‘å…¬å¸å¯èƒ½è¿›è¡Œå¤§è§„æ¨¡è£å‘˜",
            "ä¸–ç•Œå«ç”Ÿç»„ç»‡ç¡®è®¤æ–°å† ç–«è‹—å¯¹å˜å¼‚æ ªå…·æœ‰è‰¯å¥½ä¿æŠ¤æ•ˆæœ",
            "æœ‹å‹åœˆçƒ­ä¼ çš„åæ–¹èƒ½å¤Ÿå®Œå…¨æ²»æ„ˆç³–å°¿ç—…ï¼Œæ— éœ€è¯ç‰©æ²»ç–—"
        ]
        
        print(f"\nğŸ” å•æ–‡æœ¬RAGåˆ†ææµ‹è¯•:")
        for i, text in enumerate(test_texts[:3], 1):  # æµ‹è¯•å‰3ä¸ª
            print(f"\n--- æµ‹è¯• {i} ---")
            print(f"æ–‡æœ¬: {text}")
            
            # RAGåˆ†æ
            result = rag_detector.retrieve_and_generate(text)
            print(f"é¢„æµ‹: {result['predicted_class']} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")
            print(f"æ£€ç´¢åˆ° {result['retrieved_count']} ä¸ªç›¸å…³æ–‡æ¡£")
            print(f"ä½¿ç”¨ä¸Šä¸‹æ–‡: {result['context_used']}")
            
            # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£
            if result.get('retrieved_docs'):
                print("ç›¸å…³å‚è€ƒ:")
                for j, doc in enumerate(result['retrieved_docs'][:2], 1):
                    print(f"  {j}. {doc['content'][:80]}... (ç›¸å…³åº¦: {doc.get('score', 0):.3f})")
        
        # æ€§èƒ½è¯„ä¼°
        print(f"\nğŸ“Š RAGæ€§èƒ½è¯„ä¼°:")
        evaluation = rag_detector.evaluate_rag_performance()
        
        print(f"âœ… è¯„ä¼°ç»“æœ:")
        print(f"   æµ‹è¯•æ ·æœ¬æ•°: {evaluation['test_samples']}")
        print(f"   æ ‡å‡†æ¨¡å¼å‡†ç¡®ç‡: {evaluation['standard_mode']['accuracy']:.4f}")
        print(f"   RAGæ¨¡å¼å‡†ç¡®ç‡: {evaluation['rag_mode']['accuracy']:.4f}")
        print(f"   å‡†ç¡®ç‡æå‡: {evaluation['improvement']['accuracy_gain']:+.4f}")
        print(f"   å¹³å‡æ£€ç´¢æ–‡æ¡£æ•°: {evaluation['rag_mode']['avg_retrieved_docs']:.1f}")
        
        # ä¿å­˜çŸ¥è¯†åº“
        print(f"\nğŸ’¾ ä¿å­˜çŸ¥è¯†åº“...")
        rag_detector.save_knowledge_base()
        
        # å¯¹æ¯”åˆ†æï¼šæ ‡å‡†æ¨¡å¼ vs RAGæ¨¡å¼
        print(f"\nğŸ”¬ å¯¹æ¯”åˆ†æç¤ºä¾‹:")
        comparison_text = "ä¸“å®¶ç§°æŸåœ°åŒºå¯èƒ½å‘ç”Ÿåœ°è´¨ç¾å®³ï¼Œå»ºè®®å±…æ°‘æ³¨æ„é˜²èŒƒ"
        
        # æ ‡å‡†æ¨¡å¼
        standard_result = rag_detector.retrieve_and_generate(comparison_text, use_context=False)
        print(f"æ ‡å‡†æ¨¡å¼:")
        print(f"  é¢„æµ‹: {standard_result['predicted_class']} (ç½®ä¿¡åº¦: {standard_result['confidence']:.3f})")
        
        # RAGæ¨¡å¼
        rag_result = rag_detector.retrieve_and_generate(comparison_text, use_context=True)
        print(f"RAGæ¨¡å¼:")
        print(f"  é¢„æµ‹: {rag_result['predicted_class']} (ç½®ä¿¡åº¦: {rag_result['confidence']:.3f})")
        print(f"  æ£€ç´¢æ–‡æ¡£: {rag_result['retrieved_count']} ä¸ª")
        
        # çŸ¥è¯†åº“ç»Ÿè®¡
        print(f"\nğŸ“š çŸ¥è¯†åº“ç»Ÿè®¡:")
        kb = rag_detector.knowledge_base
        doc_types = {}
        doc_labels = {}
        
        for doc in kb.documents:
            doc_type = doc.get('type', 'unknown')
            doc_label = doc.get('label', 'unknown')
            doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
            doc_labels[doc_label] = doc_labels.get(doc_label, 0) + 1
        
        print(f"  æ€»æ–‡æ¡£æ•°: {len(kb.documents)}")
        print(f"  æ–‡æ¡£ç±»å‹åˆ†å¸ƒ: {doc_types}")
        print(f"  æ ‡ç­¾åˆ†å¸ƒ: {doc_labels}")
        
        print(f"\nâœ… RAGé›†æˆåŠŸèƒ½æ¼”ç¤ºå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ RAGæ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def create_rag_detector(use_existing_llm: bool = False) -> RAGRumorDetector:
    """
    åˆ›å»ºRAGæ£€æµ‹å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        use_existing_llm: æ˜¯å¦ä½¿ç”¨å·²å­˜åœ¨çš„LLMæ¨¡å‹
        
    Returns:
        RAGæ£€æµ‹å™¨å®ä¾‹
    """
    print("ğŸš€ åˆ›å»ºRAGè°£è¨€æ£€æµ‹å™¨...")
    
    llm_model = None
    if use_existing_llm:
        try:
            from models.llms.open_source_llms import create_qwen_classifier
            llm_model = create_qwen_classifier(use_lora=True, load_in_4bit=False)
        except Exception as e:
            print(f"âš ï¸  LLMæ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
    
    detector = RAGRumorDetector(llm_model=llm_model)
    return detector


class AdvancedRAGFeatures:
    """é«˜çº§RAGåŠŸèƒ½"""
    
    def __init__(self, rag_detector: RAGRumorDetector):
        """
        åˆå§‹åŒ–é«˜çº§RAGåŠŸèƒ½
        
        Args:
            rag_detector: RAGæ£€æµ‹å™¨å®ä¾‹
        """
        self.rag_detector = rag_detector
        self.query_history = []
        self.feedback_data = []
    
    def multi_query_rag(self, query_text: str, query_variations: int = 3) -> Dict[str, Any]:
        """
        å¤šæŸ¥è¯¢RAGï¼šç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“æ¥è·å–æ›´å…¨é¢çš„æ£€ç´¢ç»“æœ
        
        Args:
            query_text: åŸå§‹æŸ¥è¯¢æ–‡æœ¬
            query_variations: æŸ¥è¯¢å˜ä½“æ•°é‡
            
        Returns:
            ç»¼åˆåˆ†æç»“æœ
        """
        print(f"ğŸ”„ æ‰§è¡Œå¤šæŸ¥è¯¢RAGåˆ†æ...")
        
        # ç”ŸæˆæŸ¥è¯¢å˜ä½“
        query_variants = self._generate_query_variants(query_text, query_variations)
        
        # å¯¹æ¯ä¸ªå˜ä½“è¿›è¡Œæ£€ç´¢
        all_results = []
        all_retrieved_docs = []
        
        for i, variant in enumerate(query_variants):
            print(f"  å¤„ç†æŸ¥è¯¢å˜ä½“ {i+1}: {variant[:50]}...")
            result = self.rag_detector.retrieve_and_generate(variant)
            all_results.append(result)
            all_retrieved_docs.extend(result.get('retrieved_docs', []))
        
        # å»é‡å’Œé‡æ–°æ’åºæ–‡æ¡£
        unique_docs = self._deduplicate_documents(all_retrieved_docs)
        
        # ç»¼åˆåˆ†æ
        final_prediction = self._ensemble_predictions([r['predicted_label'] for r in all_results])
        avg_confidence = np.mean([r['confidence'] for r in all_results])
        
        return {
            'original_query': query_text,
            'query_variants': query_variants,
            'individual_results': all_results,
            'unique_retrieved_docs': unique_docs,
            'ensemble_prediction': final_prediction,
            'ensemble_class': {0: 'Non-rumor', 1: 'Rumor', 2: 'Unverified'}.get(final_prediction, 'Unknown'),
            'average_confidence': avg_confidence,
            'total_unique_docs': len(unique_docs)
        }
    
    def _generate_query_variants(self, query_text: str, num_variants: int) -> List[str]:
        """ç”ŸæˆæŸ¥è¯¢å˜ä½“"""
        variants = [query_text]  # åŸå§‹æŸ¥è¯¢
        
        # åŸºäºå…³é”®è¯çš„å˜ä½“
        keywords = query_text.split()[:5]  # å–å‰5ä¸ªè¯
        if len(keywords) > 2:
            variants.append(" ".join(keywords[:3]))  # å‰3ä¸ªå…³é”®è¯
            variants.append(" ".join(keywords[-3:]))  # å3ä¸ªå…³é”®è¯
        
        # åŸºäºé—®é¢˜ç±»å‹çš„å˜ä½“
        if 'ç½‘ä¼ ' in query_text or 'rumor' in query_text.lower():
            variants.append(f"è¿™ä¸ªä¿¡æ¯æ˜¯å¦å¯ä¿¡ï¼š{query_text}")
        
        if 'ä¸“å®¶' in query_text or 'expert' in query_text.lower():
            variants.append(f"æƒå¨æ€§åˆ†æï¼š{query_text}")
        
        # è¿”å›æ‰€éœ€æ•°é‡çš„å˜ä½“
        return variants[:num_variants]
    
    def _deduplicate_documents(self, docs: List[Dict]) -> List[Dict]:
        """å»é‡æ–‡æ¡£"""
        seen_contents = set()
        unique_docs = []
        
        for doc in docs:
            content = doc['content']
            if content not in seen_contents:
                seen_contents.add(content)
                unique_docs.append(doc)
        
        # æŒ‰ç›¸å…³åº¦æ’åº
        unique_docs.sort(key=lambda x: x.get('score', 0), reverse=True)
        return unique_docs[:10]  # è¿”å›å‰10ä¸ªæœ€ç›¸å…³çš„
    
    def _ensemble_predictions(self, predictions: List[int]) -> int:
        """é›†æˆé¢„æµ‹ç»“æœ"""
        if not predictions:
            return 0
        
        # æŠ•ç¥¨æ³•
        from collections import Counter
        vote_counts = Counter(predictions)
        return vote_counts.most_common(1)[0][0]
    
    def iterative_rag(self, query_text: str, max_iterations: int = 3) -> Dict[str, Any]:
        """
        è¿­ä»£å¼RAGï¼šåŸºäºåˆå§‹ç»“æœè¿›è¡Œè¿­ä»£ä¼˜åŒ–
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            
        Returns:
            è¿­ä»£åˆ†æç»“æœ
        """
        print(f"ğŸ”„ æ‰§è¡Œè¿­ä»£å¼RAGåˆ†æ...")
        
        iteration_results = []
        current_query = query_text
        
        for i in range(max_iterations):
            print(f"  è¿­ä»£ {i+1}/{max_iterations}")
            
            # å½“å‰è¿­ä»£çš„RAGåˆ†æ
            result = self.rag_detector.retrieve_and_generate(current_query)
            iteration_results.append({
                'iteration': i + 1,
                'query': current_query,
                'result': result
            })
            
            # å¦‚æœç½®ä¿¡åº¦è¶³å¤Ÿé«˜ï¼Œåœæ­¢è¿­ä»£
            if result['confidence'] > 0.8:
                print(f"  é«˜ç½®ä¿¡åº¦è¾¾æˆï¼Œåœæ­¢è¿­ä»£")
                break
            
            # åŸºäºå½“å‰ç»“æœä¼˜åŒ–ä¸‹ä¸€æ¬¡æŸ¥è¯¢
            if i < max_iterations - 1:
                current_query = self._refine_query_from_result(query_text, result)
        
        # é€‰æ‹©æœ€ä½³ç»“æœ
        best_result = max(iteration_results, key=lambda x: x['result']['confidence'])
        
        return {
            'original_query': query_text,
            'iterations': iteration_results,
            'best_iteration': best_result['iteration'],
            'best_result': best_result['result'],
            'confidence_progression': [ir['result']['confidence'] for ir in iteration_results]
        }
    
    def _refine_query_from_result(self, original_query: str, result: Dict) -> str:
        """åŸºäºç»“æœä¼˜åŒ–æŸ¥è¯¢"""
        # å¦‚æœæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæå–å…³é”®æ¦‚å¿µ
        if result.get('retrieved_docs'):
            doc_contents = [doc['content'] for doc in result['retrieved_docs'][:2]]
            combined_content = " ".join(doc_contents)
            
            # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæŠ€æœ¯ï¼‰
            important_words = []
            for word in combined_content.split():
                if len(word) > 3 and word not in ['è¿™ä¸ª', 'é‚£ä¸ª', 'å¯ä»¥', 'åº”è¯¥']:
                    important_words.append(word)
            
            if important_words:
                refined_query = f"{original_query} {' '.join(important_words[:3])}"
                return refined_query
        
        return original_query
    
    def add_user_feedback(self, query: str, predicted_label: int, true_label: int, 
                         feedback_type: str = "correction"):
        """
        æ·»åŠ ç”¨æˆ·åé¦ˆç”¨äºæ”¹è¿›RAGç³»ç»Ÿ
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            predicted_label: é¢„æµ‹æ ‡ç­¾
            true_label: çœŸå®æ ‡ç­¾
            feedback_type: åé¦ˆç±»å‹
        """
        feedback = {
            'query': query,
            'predicted_label': predicted_label,
            'true_label': true_label,
            'feedback_type': feedback_type,
            'timestamp': np.datetime64('now').astype(str),
            'is_correct': predicted_label == true_label
        }
        
        self.feedback_data.append(feedback)
        print(f"ğŸ“ æ·»åŠ ç”¨æˆ·åé¦ˆ: {'æ­£ç¡®' if feedback['is_correct'] else 'é”™è¯¯'}")
    
    def analyze_feedback(self) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ·åé¦ˆ"""
        if not self.feedback_data:
            return {'message': 'æš‚æ— åé¦ˆæ•°æ®'}
        
        total_feedback = len(self.feedback_data)
        correct_predictions = sum(1 for f in self.feedback_data if f['is_correct'])
        accuracy = correct_predictions / total_feedback
        
        # æŒ‰æ ‡ç­¾åˆ†æ
        label_analysis = {}
        for label in [0, 1, 2]:
            label_feedback = [f for f in self.feedback_data if f['true_label'] == label]
            if label_feedback:
                label_correct = sum(1 for f in label_feedback if f['is_correct'])
                label_analysis[label] = {
                    'total': len(label_feedback),
                    'correct': label_correct,
                    'accuracy': label_correct / len(label_feedback)
                }
        
        return {
            'total_feedback': total_feedback,
            'overall_accuracy': accuracy,
            'label_analysis': label_analysis,
            'recent_feedback': self.feedback_data[-5:] if len(self.feedback_data) >= 5 else self.feedback_data
        }


def demo_advanced_rag_features():
    """æ¼”ç¤ºé«˜çº§RAGåŠŸèƒ½"""
    print("ğŸ”¬ é«˜çº§RAGåŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # åˆ›å»ºRAGæ£€æµ‹å™¨
        rag_detector = create_rag_detector(use_existing_llm=False)
        advanced_rag = AdvancedRAGFeatures(rag_detector)
        
        # æµ‹è¯•å¤šæŸ¥è¯¢RAG
        print("\nğŸ” å¤šæŸ¥è¯¢RAGæµ‹è¯•:")
        test_query = "ç½‘ä¼ æŸåœ°å‘ç”Ÿé‡å¤§åœ°è´¨ç¾å®³ï¼Œä¸“å®¶å»ºè®®æ’¤ç¦»"
        multi_result = advanced_rag.multi_query_rag(test_query, query_variations=3)
        
        print(f"åŸå§‹æŸ¥è¯¢: {multi_result['original_query']}")
        print(f"æŸ¥è¯¢å˜ä½“: {multi_result['query_variants']}")
        print(f"é›†æˆé¢„æµ‹: {multi_result['ensemble_class']} (ç½®ä¿¡åº¦: {multi_result['average_confidence']:.3f})")
        print(f"æ£€ç´¢åˆ°å”¯ä¸€æ–‡æ¡£: {multi_result['total_unique_docs']} ä¸ª")
        
        # æµ‹è¯•è¿­ä»£RAG
        print("\nğŸ”„ è¿­ä»£RAGæµ‹è¯•:")
        iterative_result = advanced_rag.iterative_rag(test_query, max_iterations=2)
        
        print(f"è¿­ä»£æ¬¡æ•°: {len(iterative_result['iterations'])}")
        print(f"æœ€ä½³è¿­ä»£: ç¬¬{iterative_result['best_iteration']}æ¬¡")
        print(f"ç½®ä¿¡åº¦å˜åŒ–: {iterative_result['confidence_progression']}")
        print(f"æœ€ç»ˆé¢„æµ‹: {iterative_result['best_result']['predicted_class']}")
        
        # æ¨¡æ‹Ÿç”¨æˆ·åé¦ˆ
        print("\nğŸ“ ç”¨æˆ·åé¦ˆæµ‹è¯•:")
        test_cases = [
            ("å®˜æ–¹å‘å¸ƒçš„æƒå¨å£°æ˜", 0, 0),  # æ­£ç¡®é¢„æµ‹
            ("ç½‘ä¸Šæµä¼ çš„æœªè¯å®æ¶ˆæ¯", 1, 2),  # é”™è¯¯é¢„æµ‹
            ("ä¸“å®¶å­¦è€…çš„ç ”ç©¶æˆæœ", 0, 0)   # æ­£ç¡®é¢„æµ‹
        ]
        
        for query, predicted, true in test_cases:
            advanced_rag.add_user_feedback(query, predicted, true)
        
        feedback_analysis = advanced_rag.analyze_feedback()
        print(f"åé¦ˆåˆ†æ:")
        print(f"  æ€»åé¦ˆæ•°: {feedback_analysis['total_feedback']}")
        print(f"  æ•´ä½“å‡†ç¡®ç‡: {feedback_analysis['overall_accuracy']:.3f}")
        
        print(f"\nâœ… é«˜çº§RAGåŠŸèƒ½æ¼”ç¤ºå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ é«˜çº§RAGæ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


# ä¸»æ‰§è¡Œä»£ç 
if __name__ == "__main__":
    print("ğŸš€ RAGé›†æˆæ¨¡å—æµ‹è¯•")
    print("=" * 60)
    
    try:
        # åŸºç¡€RAGåŠŸèƒ½æ¼”ç¤º
        demo_rag_integration()
        
        print("\n" + "=" * 60)
        
        # é«˜çº§RAGåŠŸèƒ½æ¼”ç¤º
        demo_advanced_rag_features()
        
        print("\nâœ… RAGé›†æˆæ¨¡å—æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)