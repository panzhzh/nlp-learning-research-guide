#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# models/multimodal/vision_language_models.py

"""
å¤šæ¨¡æ€è§†è§‰-è¯­è¨€æ¨¡å‹å®ç° - ä¿®å¤å›¾åƒè·¯å¾„é—®é¢˜
è§£å†³å›¾åƒåŠ è½½å¤±è´¥çš„é—®é¢˜
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from torchvision import transforms
from PIL import Image
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, classification_report
from tqdm import tqdm
import json
from typing import Dict, List, Tuple, Any, Optional, Union
from pathlib import Path
import sys
import warnings
warnings.filterwarnings('ignore')

# å¿«é€Ÿè·¯å¾„è®¾ç½®
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from data_utils.data_loaders import create_all_dataloaders
    from utils.config_manager import get_config_manager, get_output_path
    from preprocessing.text_processing import TextProcessor
    from preprocessing.image_processing import ImageProcessor
    USE_PROJECT_MODULES = True
    print("âœ… æˆåŠŸå¯¼å…¥é¡¹ç›®æ¨¡å—")
except ImportError as e:
    print(f"âš ï¸  å¯¼å…¥é¡¹ç›®æ¨¡å—å¤±è´¥: {e}")
    USE_PROJECT_MODULES = False

# å°è¯•å¯¼å…¥CLIP
try:
    import clip
    HAS_CLIP = True
    print("âœ… CLIPå¯ç”¨")
except ImportError:
    print("âš ï¸  CLIPæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–å®ç°")
    HAS_CLIP = False

# å°è¯•å¯¼å…¥transformersç”¨äºBLIP
try:
    from transformers import BlipProcessor, BlipForImageTextRetrieval
    HAS_BLIP = True
    print("âœ… BLIPå¯ç”¨")
except ImportError:
    print("âš ï¸  BLIPä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CLIPæˆ–ç®€åŒ–å®ç°")
    HAS_BLIP = False

import logging
logger = logging.getLogger(__name__)


class MultiModalDataset(Dataset):
    """å¤šæ¨¡æ€æ•°æ®é›†ç±» - ä¿®å¤å›¾åƒè·¯å¾„å¤„ç†"""
    
    def __init__(self, texts: List[str], image_paths: List[str], labels: List[int], 
                 data_dir: str, text_processor, image_processor, max_text_length: int = 77):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€æ•°æ®é›†
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨
            data_dir: æ•°æ®æ ¹ç›®å½•è·¯å¾„
            text_processor: æ–‡æœ¬å¤„ç†å™¨
            image_processor: å›¾åƒå¤„ç†å™¨
            max_text_length: æœ€å¤§æ–‡æœ¬é•¿åº¦
        """
        self.texts = texts
        self.image_paths = image_paths
        self.labels = labels
        self.data_dir = Path(data_dir)  # ä¿å­˜æ•°æ®æ ¹ç›®å½•
        self.text_processor = text_processor
        self.image_processor = image_processor
        self.max_text_length = max_text_length
        
        # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´
        min_length = min(len(texts), len(image_paths), len(labels))
        self.texts = texts[:min_length]
        self.image_paths = image_paths[:min_length]
        self.labels = labels[:min_length]
        
        # éªŒè¯å¹¶ä¿®å¤å›¾åƒè·¯å¾„
        self._fix_image_paths()
        
        print(f"ğŸ“Š å¤šæ¨¡æ€æ•°æ®é›†åˆå§‹åŒ–: {len(self.texts)} æ ·æœ¬")
    
    def _fix_image_paths(self):
        """ä¿®å¤å’ŒéªŒè¯å›¾åƒè·¯å¾„"""
        print(f"ğŸ”§ ä¿®å¤å›¾åƒè·¯å¾„...")
        
        fixed_paths = []
        valid_count = 0
        
        for i, img_path in enumerate(self.image_paths):
            if not img_path or img_path == "":
                # ç©ºè·¯å¾„ï¼Œä¿æŒä¸ºç©º
                fixed_paths.append("")
                continue
                
            # æ„å»ºå¯èƒ½çš„å›¾åƒè·¯å¾„
            # æ ¹æ®ä½ çš„å®é™…è·¯å¾„ç»“æ„: data/split/img/index.jpg
            possible_paths = [
                self.data_dir / img_path,  # ç›´æ¥ç›¸å¯¹äºæ•°æ®ç›®å½•
                Path(img_path),  # åŸå§‹è·¯å¾„ï¼ˆå¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼‰
            ]
            
            # å¦‚æœè·¯å¾„ä¸­ä¸åŒ…å«ç›®å½•åˆ†éš”ç¬¦ï¼Œå¯èƒ½åªæ˜¯æ–‡ä»¶å
            if "/" not in str(img_path) and "\\" not in str(img_path):
                # å°è¯•åœ¨å„ä¸ªsplitçš„imgç›®å½•ä¸­æŸ¥æ‰¾
                for split in ['train', 'val', 'test']:
                    possible_paths.append(self.data_dir / split / 'img' / img_path)
            
            # æŸ¥æ‰¾å­˜åœ¨çš„è·¯å¾„
            found_path = None
            for test_path in possible_paths:
                if test_path.exists() and test_path.is_file():
                    # å°†ç»å¯¹è·¯å¾„è½¬æ¢ä¸ºç›¸å¯¹äºdata_dirçš„è·¯å¾„
                    try:
                        relative_path = test_path.relative_to(self.data_dir)
                        found_path = str(relative_path)
                        break
                    except ValueError:
                        # å¦‚æœæ— æ³•è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„
                        found_path = str(test_path)
                        break
            
            if found_path:
                fixed_paths.append(found_path)
                valid_count += 1
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œè®°å½•è°ƒè¯•ä¿¡æ¯
                if i < 5:  # åªæ‰“å°å‰5ä¸ªå¤±è´¥æ¡ˆä¾‹
                    print(f"âš ï¸  æ‰¾ä¸åˆ°å›¾åƒæ–‡ä»¶ {i}: {img_path}")
                    print(f"   å°è¯•çš„è·¯å¾„:")
                    for j, p in enumerate(possible_paths[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªå°è¯•
                        print(f"     {p} - å­˜åœ¨: {p.exists()}")
                fixed_paths.append("")
        
        self.image_paths = fixed_paths
        print(f"âœ… å›¾åƒè·¯å¾„ä¿®å¤å®Œæˆ: {valid_count}/{len(self.image_paths)} ä¸ªæœ‰æ•ˆå›¾åƒ")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªæœ‰æ•ˆè·¯å¾„ä½œä¸ºç¤ºä¾‹
        valid_examples = [path for path in fixed_paths[:10] if path]
        if valid_examples:
            print(f"   å›¾åƒè·¯å¾„ç¤ºä¾‹: {valid_examples[:3]}")
        
        return valid_count > 0
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        image_path = self.image_paths[idx]
        label = self.labels[idx]
        
        # å¤„ç†æ–‡æœ¬
        if hasattr(self.text_processor, 'tokenize') and callable(self.text_processor.tokenize):
            # ä½¿ç”¨CLIP tokenizer
            try:
                text_tokens = self.text_processor.tokenize(text)
            except:
                # CLIP tokenizerå¤±è´¥ï¼Œä½¿ç”¨ç®€å•å¤„ç†
                words = text.split()[:self.max_text_length]
                text_tokens = torch.zeros(self.max_text_length, dtype=torch.long)
                for i, word in enumerate(words):
                    text_tokens[i] = hash(word) % 30000
        else:
            # ç®€å•æ–‡æœ¬å¤„ç†
            words = text.split()[:self.max_text_length]
            text_tokens = torch.zeros(self.max_text_length, dtype=torch.long)
            for i, word in enumerate(words):
                text_tokens[i] = hash(word) % 30000  # ç®€å•hashç¼–ç 
        
        # å¤„ç†å›¾åƒ - ä¿®å¤å›¾åƒåŠ è½½é€»è¾‘
        try:
            if image_path and Path(image_path).exists():
                # ä½¿ç”¨é¡¹ç›®çš„å›¾åƒå¤„ç†å™¨
                if hasattr(self.image_processor, 'process_single_image'):
                    image_tensor = self.image_processor.process_single_image(
                        image_path, transform_type='val'
                    )
                    if image_tensor is None:
                        raise ValueError("å›¾åƒå¤„ç†å™¨è¿”å›None")
                else:
                    # ä½¿ç”¨PILç›´æ¥å¤„ç†
                    image = Image.open(image_path).convert('RGB')
                    transform = transforms.Compose([
                        transforms.Resize((224, 224)),
                        transforms.ToTensor(),
                        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                           std=[0.229, 0.224, 0.225])
                    ])
                    image_tensor = transform(image)
            else:
                # æ²¡æœ‰å›¾åƒæ–‡ä»¶æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºtensor
                image_tensor = torch.zeros(3, 224, 224)
                
        except Exception as e:
            # å¤„ç†ä»»ä½•å›¾åƒåŠ è½½é”™è¯¯ï¼Œåˆ›å»ºç©ºtensor
            image_tensor = torch.zeros(3, 224, 224)
        
        return {
            'text': text_tokens,
            'image': image_tensor,
            'labels': torch.tensor(label, dtype=torch.long),
            'text_raw': text,
            'image_path': str(image_path) if image_path else ""
        }


class SimpleCLIPModel(nn.Module):
    """ç®€åŒ–çš„CLIPé£æ ¼æ¨¡å‹"""
    
    def __init__(self, vocab_size: int = 30000, text_embed_dim: int = 512, 
                 image_embed_dim: int = 512, projection_dim: int = 256, 
                 num_classes: int = 3):
        """
        åˆå§‹åŒ–ç®€åŒ–CLIPæ¨¡å‹
        
        Args:
            vocab_size: è¯æ±‡è¡¨å¤§å°
            text_embed_dim: æ–‡æœ¬åµŒå…¥ç»´åº¦
            image_embed_dim: å›¾åƒåµŒå…¥ç»´åº¦
            projection_dim: æŠ•å½±ç»´åº¦
            num_classes: åˆ†ç±»æ•°é‡
        """
        super(SimpleCLIPModel, self).__init__()
        
        # æ–‡æœ¬ç¼–ç å™¨
        self.text_embedding = nn.Embedding(vocab_size, text_embed_dim)
        self.text_transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(text_embed_dim, 8, dim_feedforward=2048),
            num_layers=6
        )
        self.text_projection = nn.Linear(text_embed_dim, projection_dim)
        
        # å›¾åƒç¼–ç å™¨ï¼ˆç®€åŒ–çš„CNNï¼‰
        self.image_encoder = nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((7, 7)),
            nn.Flatten(),
            nn.Linear(256 * 7 * 7, image_embed_dim),
            nn.ReLU()
        )
        self.image_projection = nn.Linear(image_embed_dim, projection_dim)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(projection_dim * 2, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, num_classes)
        )
        
        # æ¸©åº¦å‚æ•°ï¼ˆç”¨äºå¯¹æ¯”å­¦ä¹ ï¼‰
        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
    
    def encode_text(self, text):
        """ç¼–ç æ–‡æœ¬"""
        # æ–‡æœ¬åµŒå…¥
        x = self.text_embedding(text)  # [batch, seq_len, embed_dim]
        x = x.transpose(0, 1)  # [seq_len, batch, embed_dim]
        
        # Transformerç¼–ç 
        x = self.text_transformer(x)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = x.mean(dim=0)  # [batch, embed_dim]
        
        # æŠ•å½±
        x = self.text_projection(x)
        
        return F.normalize(x, dim=-1)
    
    def encode_image(self, image):
        """ç¼–ç å›¾åƒ"""
        x = self.image_encoder(image)
        x = self.image_projection(x)
        return F.normalize(x, dim=-1)
    
    def forward(self, text, image):
        """å‰å‘ä¼ æ’­"""
        text_features = self.encode_text(text)
        image_features = self.encode_image(image)
        
        # èåˆç‰¹å¾
        fused_features = torch.cat([text_features, image_features], dim=1)
        
        # åˆ†ç±»
        logits = self.classifier(fused_features)
        
        return logits, text_features, image_features


class CLIPBasedClassifier(nn.Module):
    """åŸºäºå®˜æ–¹CLIPçš„åˆ†ç±»å™¨"""
    
    def __init__(self, clip_model_name: str = "ViT-B/32", num_classes: int = 3):
        """
        åˆå§‹åŒ–CLIPåˆ†ç±»å™¨
        
        Args:
            clip_model_name: CLIPæ¨¡å‹åç§°
            num_classes: åˆ†ç±»æ•°é‡
        """
        super(CLIPBasedClassifier, self).__init__()
        
        if HAS_CLIP:
            # åŠ è½½é¢„è®­ç»ƒCLIPæ¨¡å‹
            self.clip_model, self.clip_preprocess = clip.load(clip_model_name, device="cpu")
            
            # å†»ç»“CLIPå‚æ•°
            for param in self.clip_model.parameters():
                param.requires_grad = False
            
            # è·å–ç‰¹å¾ç»´åº¦
            with torch.no_grad():
                dummy_text = clip.tokenize(["hello world"])
                dummy_image = torch.randn(1, 3, 224, 224)
                text_features = self.clip_model.encode_text(dummy_text).float()
                image_features = self.clip_model.encode_image(dummy_image).float()
                feature_dim = text_features.shape[-1] + image_features.shape[-1]
            
            # åˆ†ç±»å¤´
            self.classifier = nn.Sequential(
                nn.Linear(feature_dim, 512),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(512, num_classes)
            )
            
            self.use_clip = True
            print("âœ… ä½¿ç”¨å®˜æ–¹CLIPæ¨¡å‹")
        else:
            # ä½¿ç”¨ç®€åŒ–å®ç°
            self.clip_model = SimpleCLIPModel(num_classes=num_classes)
            self.use_clip = False
            print("âš ï¸  ä½¿ç”¨ç®€åŒ–CLIPå®ç°")
    
    def forward(self, text, image):
        """å‰å‘ä¼ æ’­"""
        if self.use_clip and HAS_CLIP:
            # ä½¿ç”¨å®˜æ–¹CLIP
            with torch.no_grad():
                text_features = self.clip_model.encode_text(text).float()
                image_features = self.clip_model.encode_image(image).float()
            
            # èåˆç‰¹å¾
            fused_features = torch.cat([text_features, image_features], dim=1)
            logits = self.classifier(fused_features)
            
            return logits
        else:
            # ä½¿ç”¨ç®€åŒ–å®ç°
            return self.clip_model(text, image)[0]


class MultiModalTrainer:
    """å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒå™¨ - ä¿®å¤æ•°æ®åŠ è½½"""
    
    def __init__(self, data_dir: str = "data", device: str = "auto"):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
        """
        self.data_dir = Path(data_dir).resolve()  # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        
        # è®¾ç½®è®¾å¤‡
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.models = {}
        self.results = {}
        
        # åˆå§‹åŒ–å¤„ç†å™¨
        if USE_PROJECT_MODULES:
            self.text_processor = TextProcessor(language='mixed')
            self.image_processor = ImageProcessor(target_size=(224, 224))
        else:
            self.text_processor = None
            self.image_processor = None
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if USE_PROJECT_MODULES:
            config_manager = get_config_manager()
            self.output_dir = get_output_path('models', 'multimodal')
        else:
            self.output_dir = Path('outputs/models/multimodal')
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ ‡ç­¾æ˜ å°„
        self.label_mapping = {0: 'Non-rumor', 1: 'Rumor', 2: 'Unverified'}
        
        print(f"ğŸ­ å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ•°æ®ç›®å½•: {self.data_dir}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_data(self) -> Dict[str, Tuple[List[str], List[str], List[int]]]:
        """åŠ è½½MR2å¤šæ¨¡æ€æ•°æ®é›† - æ”¹è¿›ç‰ˆæœ¬ï¼Œä¼˜å…ˆä½¿ç”¨ç›´æ¥åŠ è½½"""
        print("ğŸ“š åŠ è½½MR2å¤šæ¨¡æ€æ•°æ®é›†...")
        
        # ä¼˜å…ˆå°è¯•ç›´æ¥ä»JSONæ–‡ä»¶åŠ è½½ï¼Œæ›´å¯é 
        try:
            data = self._load_data_directly()
            if data and len(data) > 0:
                print("âœ… ä½¿ç”¨ç›´æ¥æ–‡ä»¶åŠ è½½æˆåŠŸ")
                return data
        except Exception as e:
            print(f"âš ï¸  ç›´æ¥æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        
        # å¤‡ç”¨ï¼šå°è¯•ä½¿ç”¨æ•°æ®åŠ è½½å™¨
        if USE_PROJECT_MODULES:
            try:
                print("ğŸ”„ å°è¯•ä½¿ç”¨æ•°æ®åŠ è½½å™¨...")
                dataloaders = create_all_dataloaders(
                    batch_sizes={'train': 16, 'val': 16, 'test': 16}
                )
                
                data = {}
                for split, dataloader in dataloaders.items():
                    texts = []
                    image_paths = []
                    labels = []
                    
                    print(f"ğŸ“‚ å¤„ç† {split} æ•°æ®é›†...")
                    
                    for batch_idx, batch in enumerate(dataloader):
                        # æå–æ–‡æœ¬
                        if 'text' in batch:
                            batch_texts = batch['text']
                        elif 'caption' in batch:
                            batch_texts = batch['caption']
                        else:
                            print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx} æ²¡æœ‰æ–‡æœ¬å­—æ®µ")
                            continue
                        
                        texts.extend(batch_texts)
                        
                        # æå–æ ‡ç­¾
                        if 'labels' in batch:
                            batch_labels = batch['labels'].tolist()
                        elif 'label' in batch:
                            batch_labels = batch['label']
                        else:
                            print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx} æ²¡æœ‰æ ‡ç­¾å­—æ®µ")
                            batch_labels = [0] * len(batch_texts)
                        
                        labels.extend(batch_labels)
                        
                        # ä¿®å¤ï¼šæ£€æŸ¥æ•°æ®é›†æ˜¯å¦æœ‰å›¾åƒè·¯å¾„ä¿¡æ¯
                        batch_size = len(batch_texts)
                        batch_image_paths = []
                        
                        # å°è¯•ä»batchä¸­è·å–å›¾åƒè·¯å¾„
                        if 'image_path' in batch:
                            batch_image_paths = batch['image_path']
                        else:
                            # å¦‚æœbatchä¸­æ²¡æœ‰image_pathï¼Œå°è¯•æ„å»ºè·¯å¾„
                            # æ ¹æ®æ•°æ®ç›®å½•ç»“æ„ï¼šdata/split/img/index.jpg
                            start_idx = batch_idx * dataloader.batch_size
                            for i in range(batch_size):
                                img_path = f"{split}/img/{start_idx + i}.jpg"
                                batch_image_paths.append(img_path)
                        
                        image_paths.extend(batch_image_paths)
                    
                    # ç¡®ä¿ä¸‰ä¸ªåˆ—è¡¨é•¿åº¦ä¸€è‡´
                    min_length = min(len(texts), len(image_paths), len(labels))
                    if min_length < len(texts):
                        print(f"âš ï¸  {split} æ•°æ®é•¿åº¦ä¸ä¸€è‡´ï¼Œæˆªæ–­åˆ° {min_length}")
                        texts = texts[:min_length]
                        image_paths = image_paths[:min_length]
                        labels = labels[:min_length]
                    
                    data[split] = (texts, image_paths, labels)
                    print(f"âœ… åŠ è½½ {split}: {len(texts)} æ ·æœ¬")
                
                return data
                
            except Exception as e:
                print(f"âŒ ä½¿ç”¨é¡¹ç›®æ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
        
        # æœ€åå¤‡ç”¨ï¼šåˆ›å»ºæ¼”ç¤ºæ•°æ®
        print("ğŸ”„ ä½¿ç”¨æ¼”ç¤ºæ•°æ®...")
        return self._create_demo_data()

    def _load_data_directly(self) -> Dict[str, Tuple[List[str], List[str], List[int]]]:
        """ç›´æ¥ä»JSONæ–‡ä»¶åŠ è½½æ•°æ® - ä¿®å¤ç‰ˆæœ¬"""
        print("ğŸ“‚ ç›´æ¥ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®...")
        data = {}
        
        for split in ['train', 'val', 'test']:
            dataset_file = self.data_dir / f'dataset_items_{split}.json'
            
            if not dataset_file.exists():
                print(f"âš ï¸  æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {dataset_file}")
                continue
            
            try:
                with open(dataset_file, 'r', encoding='utf-8') as f:
                    dataset_items = json.load(f)
                
                texts = []
                image_paths = []
                labels = []
                
                for item_id, item_data in dataset_items.items():
                    # ä¿®å¤ï¼šä¼˜å…ˆæ£€æŸ¥captionå­—æ®µï¼Œç„¶åæ˜¯textå­—æ®µ
                    text = item_data.get('caption', '') or item_data.get('text', '')
                    if not text or not text.strip():
                        print(f"âš ï¸  è·³è¿‡æ ·æœ¬ {item_id}: æ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬å†…å®¹")
                        continue
                    
                    texts.append(text.strip())
                    
                    # æå–æ ‡ç­¾
                    label = item_data.get('label', 0)
                    labels.append(label)
                    
                    # ä½¿ç”¨æ•°æ®ä¸­æä¾›çš„å›¾åƒè·¯å¾„
                    img_path = item_data.get('image_path', '')
                    if not img_path:
                        # å¦‚æœæ²¡æœ‰image_pathï¼Œå°è¯•æ„å»ºè·¯å¾„
                        img_path = f"{split}/img/{item_id}.jpg"
                    
                    image_paths.append(img_path)
                
                data[split] = (texts, image_paths, labels)
                print(f"âœ… ç›´æ¥åŠ è½½ {split}: {len(texts)} æ ·æœ¬")
                
                # éªŒè¯å‰å‡ ä¸ªå›¾åƒè·¯å¾„æ˜¯å¦å­˜åœ¨
                if image_paths:
                    valid_images = 0
                    check_count = min(len(image_paths), 5)
                    for i, img_path in enumerate(image_paths[:check_count]):
                        full_path = self.data_dir / img_path
                        if full_path.exists():
                            valid_images += 1
                        elif i < 3:  # åªæ‰“å°å‰3ä¸ªå¤±è´¥çš„
                            print(f"   å›¾åƒè·¯å¾„ç¤ºä¾‹ {i}: {img_path} -> {full_path} (å­˜åœ¨: {full_path.exists()})")
                    
                    print(f"   å‰{check_count}ä¸ªå›¾åƒä¸­æœ‰æ•ˆçš„: {valid_images}/{check_count}")
                
            except Exception as e:
                print(f"âŒ ç›´æ¥åŠ è½½ {split} å¤±è´¥: {e}")
                continue
        
        return data

    def _create_demo_data(self) -> Dict[str, Tuple[List[str], List[str], List[int]]]:
        """åˆ›å»ºæ¼”ç¤ºæ•°æ®"""
        print("ğŸ”§ åˆ›å»ºå¤šæ¨¡æ€æ¼”ç¤ºæ•°æ®...")
        print("âš ï¸  æ³¨æ„ï¼šç”±äºæ²¡æœ‰çœŸå®æ•°æ®ï¼Œå°†ä½¿ç”¨ç©ºå›¾åƒè·¯å¾„")
        
        demo_texts = [
            "è¿™æ˜¯ä¸€ä¸ªå…³äºç§‘æŠ€è¿›æ­¥çš„çœŸå®æ–°é—»æŠ¥é“",
            "This is fake news about celebrity scandal",
            "æœªç»è¯å®çš„ä¼ è¨€éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥éªŒè¯",
            "Breaking: Major breakthrough in AI technology",
            "ç½‘ä¼ æŸåœ°å‘ç”Ÿé‡å¤§äº‹æ•…ï¼Œå®˜æ–¹å°šæœªç¡®è®¤",
            "Scientists discover new species in deep ocean",
            "è°£ä¼ æŸå…¬å¸å€’é—­ï¼Œå®é™…æƒ…å†µæœ‰å¾…æ ¸å®",
            "Weather alert: Severe storm approaching",
            "ç¤¾äº¤åª’ä½“æµä¼ çš„æœªè¯å®æ¶ˆæ¯å¼•å‘å…³æ³¨",
            "Economic indicators show positive trends"
        ]
        
        demo_labels = [0, 1, 2, 0, 2, 0, 1, 0, 2, 0]
        demo_image_paths = [""] * len(demo_texts)
        
        # æ‰©å±•æ•°æ®
        extended_texts = demo_texts * 8
        extended_image_paths = demo_image_paths * 8
        extended_labels = demo_labels * 8
        
        # æŒ‰æ¯”ä¾‹åˆ†å‰²æ•°æ®
        total_size = len(extended_texts)
        train_size = int(0.7 * total_size)
        val_size = int(0.15 * total_size)
        
        print(f"ğŸ“ åˆ›å»ºæ¼”ç¤ºæ•°æ®: {total_size} ä¸ªæ ·æœ¬ï¼ˆæ— å›¾åƒï¼‰")
        
        return {
            'train': (extended_texts[:train_size], 
                     extended_image_paths[:train_size],
                     extended_labels[:train_size]),
            'val': (extended_texts[train_size:train_size+val_size], 
                   extended_image_paths[train_size:train_size+val_size],
                   extended_labels[train_size:train_size+val_size]),
            'test': (extended_texts[train_size+val_size:], 
                    extended_image_paths[train_size+val_size:],
                    extended_labels[train_size+val_size:])
        }

    def create_models(self):
        """åˆ›å»ºå¤šæ¨¡æ€æ¨¡å‹"""
        print("ğŸ­ åˆ›å»ºå¤šæ¨¡æ€æ¨¡å‹...")
        
        # CLIPæ¨¡å‹
        self.models['clip'] = CLIPBasedClassifier(
            clip_model_name="ViT-B/32",
            num_classes=3
        ).to(self.device)
        
        # ç®€åŒ–å¤šæ¨¡æ€æ¨¡å‹
        self.models['simple_multimodal'] = SimpleCLIPModel(
            vocab_size=30000,
            text_embed_dim=256,
            image_embed_dim=256,
            projection_dim=128,
            num_classes=3
        ).to(self.device)
        
        print(f"âœ… åˆ›å»ºäº† {len(self.models)} ä¸ªå¤šæ¨¡æ€æ¨¡å‹")
        
        # æ‰“å°æ¨¡å‹å‚æ•°é‡
        for name, model in self.models.items():
            param_count = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"   {name}: {param_count:,} å‚æ•° ({trainable_params:,} å¯è®­ç»ƒ)")
    
    def train_single_model(self, model_name: str, train_loader: DataLoader, 
                          val_loader: DataLoader, epochs: int = 5, 
                          learning_rate: float = 1e-4) -> Dict[str, Any]:
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        print(f"ğŸ‹ï¸ è®­ç»ƒ {model_name} æ¨¡å‹...")
        
        model = self.models[model_name]
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        
        train_losses = []
        val_accuracies = []
        best_val_acc = 0
        best_model_state = None
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0
            
            train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]", leave=False)
            for batch in train_pbar:
                text = batch['text'].to(self.device)
                image = batch['image'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                optimizer.zero_grad()
                
                # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒç”¨ä¸åŒçš„å‰å‘ä¼ æ’­
                if isinstance(model, CLIPBasedClassifier):
                    logits = model(text, image)
                else:
                    logits = model(text, image)[0]  # SimpleCLIPModelè¿”å›å¤šä¸ªå€¼
                
                loss = criterion(logits, labels)
                
                loss.backward()
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = torch.max(logits.data, 1)
                train_total += labels.size(0)
                train_correct += (predicted == labels).sum().item()
                
                train_pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{100*train_correct/train_total:.2f}%'
                })
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_correct = 0
            val_total = 0
            all_preds = []
            all_labels = []
            
            with torch.no_grad():
                for batch in val_loader:
                    text = batch['text'].to(self.device)
                    image = batch['image'].to(self.device)
                    labels = batch['labels'].to(self.device)
                    
                    if isinstance(model, CLIPBasedClassifier):
                        logits = model(text, image)
                    else:
                        logits = model(text, image)[0]
                    
                    _, predicted = torch.max(logits.data, 1)
                    
                    val_total += labels.size(0)
                    val_correct += (predicted == labels).sum().item()
                    
                    all_preds.extend(predicted.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())
            
            train_acc = 100 * train_correct / train_total
            val_acc = 100 * val_correct / val_total
            avg_train_loss = train_loss / len(train_loader)
            
            train_losses.append(avg_train_loss)
            val_accuracies.append(val_acc)
            
            print(f"Epoch {epoch+1}/{epochs}: "
                  f"Train Loss: {avg_train_loss:.4f}, "
                  f"Train Acc: {train_acc:.2f}%, "
                  f"Val Acc: {val_acc:.2f}%")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_model_state = model.state_dict().copy()
        
        # æ¢å¤æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
        
        # è®¡ç®—æœ€ç»ˆF1åˆ†æ•°
        val_f1 = f1_score(all_labels, all_preds, average='macro')
        
        result = {
            'model_name': model_name,
            'best_val_accuracy': best_val_acc,
            'val_f1_score': val_f1,
            'train_losses': train_losses,
            'val_accuracies': val_accuracies,
            'epochs_trained': epochs
        }
        
        print(f"âœ… {model_name} è®­ç»ƒå®Œæˆ:")
        print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
        print(f"   éªŒè¯F1åˆ†æ•°: {val_f1:.4f}")
        
        return result
    
    def evaluate_model(self, model_name: str, test_loader: DataLoader) -> Dict[str, Any]:
        """è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½"""
        print(f"ğŸ“Š è¯„ä¼° {model_name} æ¨¡å‹...")
        
        model = self.models[model_name]
        model.eval()
        
        test_correct = 0
        test_total = 0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="Testing"):
                text = batch['text'].to(self.device)
                image = batch['image'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                if isinstance(model, CLIPBasedClassifier):
                    logits = model(text, image)
                else:
                    logits = model(text, image)[0]
                
                _, predicted = torch.max(logits.data, 1)
                
                test_total += labels.size(0)
                test_correct += (predicted == labels).sum().item()
                
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        test_acc = 100 * test_correct / test_total
        test_f1 = f1_score(all_labels, all_preds, average='macro')
        
        # åˆ†ç±»æŠ¥å‘Š
        report = classification_report(
            all_labels, all_preds,
            target_names=list(self.label_mapping.values()),
            output_dict=True
        )
        
        result = {
            'test_accuracy': test_acc,
            'test_f1_score': test_f1,
            'classification_report': report
        }
        
        print(f"âœ… {model_name} æµ‹è¯•ç»“æœ:")
        print(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
        print(f"   æµ‹è¯•F1åˆ†æ•°: {test_f1:.4f}")
        
        return result
    
    def train_all_models(self, epochs: int = 5, batch_size: int = 8, learning_rate: float = 1e-4):
        """è®­ç»ƒæ‰€æœ‰å¤šæ¨¡æ€æ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ‰€æœ‰å¤šæ¨¡æ€æ¨¡å‹...")
        
        # åŠ è½½æ•°æ®
        data = self.load_data()
        
        # åˆ›å»ºæ¨¡å‹
        self.create_models()
        
        # å¤„ç†æ–‡æœ¬çš„tokenizer
        if HAS_CLIP:
            text_processor = clip.tokenize
        else:
            text_processor = self.text_processor
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = MultiModalDataset(
            data['train'][0], data['train'][1], data['train'][2],
            str(self.data_dir), text_processor, self.image_processor
        )
        val_dataset = MultiModalDataset(
            data['val'][0], data['val'][1], data['val'][2],
            str(self.data_dir), text_processor, self.image_processor
        )
        test_dataset = MultiModalDataset(
            data['test'][0], data['test'][1], data['test'][2],
            str(self.data_dir), text_processor, self.image_processor
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
        for model_name in self.models.keys():
            print(f"\n{'='*60}")
            print(f"è®­ç»ƒæ¨¡å‹: {model_name.upper()}")
            print(f"{'='*60}")
            
            try:
                # è®­ç»ƒæ¨¡å‹
                train_result = self.train_single_model(
                    model_name, train_loader, val_loader, epochs, learning_rate
                )
                
                # æµ‹è¯•è¯„ä¼°
                test_result = self.evaluate_model(model_name, test_loader)
                
                # åˆå¹¶ç»“æœ
                self.results[model_name] = {**train_result, **test_result}
                
            except Exception as e:
                print(f"âŒ è®­ç»ƒæ¨¡å‹å¤±è´¥: {model_name}, é”™è¯¯: {e}")
                continue
        
        # ä¿å­˜æ¨¡å‹å’Œç»“æœ
        self.save_models_and_results()
        
        # æ˜¾ç¤ºæœ€ç»ˆå¯¹æ¯”
        self.print_model_comparison()
    
    def save_models_and_results(self):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œç»“æœ"""
        print("\nğŸ’¾ ä¿å­˜æ¨¡å‹å’Œç»“æœ...")
        
        # ä¿å­˜æ¯ä¸ªæ¨¡å‹
        for model_name, model in self.models.items():
            model_file = self.output_dir / f'{model_name}_model.pth'
            torch.save(model.state_dict(), model_file)
            print(f"âœ… ä¿å­˜æ¨¡å‹: {model_file}")
        
        # ä¿å­˜ç»“æœ
        results_file = self.output_dir / 'training_results.json'
        # è½¬æ¢numpyç±»å‹ä¸ºå¯åºåˆ—åŒ–çš„ç±»å‹
        serializable_results = {}
        for model_name, result in self.results.items():
            serializable_result = {}
            for key, value in result.items():
                if isinstance(value, (list, np.ndarray)):
                    serializable_result[key] = list(value) if hasattr(value, 'tolist') else value
                else:
                    serializable_result[key] = value
            serializable_results[model_name] = serializable_result
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        print(f"âœ… ä¿å­˜è®­ç»ƒç»“æœ: {results_file}")
    
    def print_model_comparison(self):
        """æ‰“å°æ¨¡å‹æ¯”è¾ƒç»“æœ"""
        print(f"\nğŸ“Š {'='*70}")
        print("å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
        print(f"{'='*70}")
        
        if not self.results:
            print("âš ï¸  æ²¡æœ‰è®­ç»ƒç»“æœå¯æ˜¾ç¤º")
            return
        
        print(f"{'æ¨¡å‹':<20} {'éªŒè¯å‡†ç¡®ç‡':<10} {'æµ‹è¯•å‡†ç¡®ç‡':<10} {'éªŒè¯F1':<8} {'æµ‹è¯•F1':<8}")
        print("-" * 70)
        
        # æŒ‰æµ‹è¯•F1åˆ†æ•°æ’åº
        sorted_results = sorted(self.results.items(), 
                              key=lambda x: x[1].get('test_f1_score', 0), 
                              reverse=True)
        
        for model_name, result in sorted_results:
            val_acc = result.get('best_val_accuracy', 0)
            test_acc = result.get('test_accuracy', 0)
            val_f1 = result.get('val_f1_score', 0)
            test_f1 = result.get('test_f1_score', 0)
            
            print(f"{model_name:<20} "
                  f"{val_acc:<10.2f} "
                  f"{test_acc:<10.2f} "
                  f"{val_f1:<8.4f} "
                  f"{test_f1:<8.4f}")
        
        if sorted_results:
            best_model = sorted_results[0]
            print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model[0]}")
            print(f"   æµ‹è¯•F1åˆ†æ•°: {best_model[1].get('test_f1_score', 0):.4f}")
            print(f"   æµ‹è¯•å‡†ç¡®ç‡: {best_model[1].get('test_accuracy', 0):.2f}%")


def main():
    """ä¸»å‡½æ•°ï¼Œæ¼”ç¤ºå¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒæµç¨‹"""
    print("ğŸš€ å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒæ¼”ç¤º")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MultiModalTrainer(data_dir="data")
    
    # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    trainer.train_all_models(
        epochs=5,           # è®­ç»ƒè½®æ•°
        batch_size=8,       # å°æ‰¹æ¬¡ï¼Œé€‚åº”æ˜¾å­˜é™åˆ¶
        learning_rate=1e-4  # å¤šæ¨¡æ€æ¨¡å‹å­¦ä¹ ç‡
    )
    
    print("\nâœ… å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒæ¼”ç¤ºå®Œæˆ!")
    print(f"ğŸ“ æ¨¡å‹å’Œç»“æœå·²ä¿å­˜åˆ°: {trainer.output_dir}")


if __name__ == "__main__":
    main()