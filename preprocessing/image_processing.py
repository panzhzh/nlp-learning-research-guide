#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# preprocessing/image_processing.py

"""
MR2å›¾åƒé¢„å¤„ç†æ¨¡å—
ä¸“é—¨å¤„ç†MR2æ•°æ®é›†ä¸­çš„å›¾åƒæ•°æ®ï¼ŒåŒ…æ‹¬ï¼š
- å›¾åƒåŠ è½½å’ŒéªŒè¯
- å°ºå¯¸æ ‡å‡†åŒ–å’Œç¼©æ”¾
- æ•°æ®å¢å¼º
- ç‰¹å¾æå–
- æ‰¹é‡å¤„ç†
"""

import os
import cv2
import numpy as np
from PIL import Image, ImageEnhance, ImageFilter
import torch
import torchvision.transforms as transforms
from torchvision.transforms import functional as F
from typing import List, Dict, Optional, Tuple, Union, Any
from pathlib import Path
import sys
import json
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# é…ç½®ç®¡ç†
try:
    from utils.config_manager import get_data_config, get_data_dir
    USE_CONFIG = True
except ImportError:
    USE_CONFIG = False

import logging
logger = logging.getLogger(__name__)


class ImageProcessor:
    """
    å›¾åƒé¢„å¤„ç†å™¨
    ä¸“é—¨ä¸ºMR2æ•°æ®é›†è®¾è®¡çš„å›¾åƒå¤„ç†å·¥å…·
    """
    
    def __init__(self, target_size: Tuple[int, int] = (224, 224)):
        """
        åˆå§‹åŒ–å›¾åƒå¤„ç†å™¨
        
        Args:
            target_size: ç›®æ ‡å›¾åƒå°ºå¯¸ (height, width)
        """
        self.target_size = target_size
        
        # åŠ è½½é…ç½®
        if USE_CONFIG:
            try:
                config = get_data_config()
                self.processing_config = config.get('processing', {}).get('image', {})
                self.data_dir = get_data_dir()
            except:
                self.processing_config = {}
                self.data_dir = Path('data')
        else:
            self.processing_config = {}
            self.data_dir = Path('data')
        
        # è®¾ç½®å¤„ç†å‚æ•°
        self.normalize_mean = self.processing_config.get('normalize_mean', [0.485, 0.456, 0.406])
        self.normalize_std = self.processing_config.get('normalize_std', [0.229, 0.224, 0.225])
        self.quality_threshold = self.processing_config.get('quality_threshold', 0.3)
        
        # åˆå§‹åŒ–å˜æ¢ï¼ˆæ”¾åœ¨å‚æ•°è®¾ç½®ä¹‹åï¼‰
        self.setup_transforms()
        
        print(f"ğŸ–¼ï¸  å›¾åƒå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç›®æ ‡å°ºå¯¸: {self.target_size}")
        print(f"   æ•°æ®ç›®å½•: {self.data_dir}")
    
    def setup_transforms(self):
        """è®¾ç½®å›¾åƒå˜æ¢"""
        
        # åŸºç¡€å˜æ¢ - ç”¨äºè®­ç»ƒ
        self.train_transforms = transforms.Compose([
            transforms.Resize(self.target_size),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=10),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=self.normalize_mean, std=self.normalize_std)
        ])
        
        # éªŒè¯å˜æ¢ - ç”¨äºéªŒè¯å’Œæµ‹è¯•
        self.val_transforms = transforms.Compose([
            transforms.Resize(self.target_size),
            transforms.ToTensor(),
            transforms.Normalize(mean=self.normalize_mean, std=self.normalize_std)
        ])
        
        # æ— æ ‡å‡†åŒ–å˜æ¢ - ç”¨äºå¯è§†åŒ–
        self.visual_transforms = transforms.Compose([
            transforms.Resize(self.target_size),
            transforms.ToTensor()
        ])
    
    def load_image(self, image_path: Union[str, Path], mode: str = 'RGB') -> Optional[Image.Image]:
        """
        åŠ è½½å›¾åƒæ–‡ä»¶
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            mode: å›¾åƒæ¨¡å¼ ('RGB', 'RGBA', 'L')
            
        Returns:
            PIL Imageå¯¹è±¡ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(image_path):
                logger.error(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                return None
            
            # åŠ è½½å›¾åƒ
            image = Image.open(image_path)
            
            # è½¬æ¢æ¨¡å¼
            if image.mode != mode:
                image = image.convert(mode)
            
            # éªŒè¯å›¾åƒè´¨é‡
            if not self.validate_image(image):
                logger.warning(f"å›¾åƒè´¨é‡æ£€æŸ¥æœªé€šè¿‡: {image_path}")
                return None
            
            return image
            
        except Exception as e:
            logger.error(f"åŠ è½½å›¾åƒå¤±è´¥ {image_path}: {e}")
            return None
    
    def validate_image(self, image: Image.Image) -> bool:
        """
        éªŒè¯å›¾åƒè´¨é‡
        
        Args:
            image: PIL Imageå¯¹è±¡
            
        Returns:
            æ˜¯å¦é€šè¿‡éªŒè¯
        """
        # æ£€æŸ¥å›¾åƒå°ºå¯¸
        width, height = image.size
        if width < 50 or height < 50:
            return False
        
        # æ£€æŸ¥å›¾åƒæ•°æ® - ä¿®å¤éªŒè¯æ–¹æ³•
        try:
            # ä½¿ç”¨copy()æ–¹æ³•éªŒè¯å›¾åƒæ•°æ®è€Œä¸æ˜¯verify()
            _ = image.copy()
            return True
        except Exception:
            return False
    
    def get_image_info(self, image_path: Union[str, Path]) -> Dict[str, Any]:
        """
        è·å–å›¾åƒåŸºæœ¬ä¿¡æ¯
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            
        Returns:
            å›¾åƒä¿¡æ¯å­—å…¸
        """
        try:
            with Image.open(image_path) as image:
                file_size = os.path.getsize(image_path)
                
                return {
                    'path': str(image_path),
                    'filename': os.path.basename(image_path),
                    'format': image.format,
                    'mode': image.mode,
                    'size': image.size,
                    'width': image.width,
                    'height': image.height,
                    'file_size': file_size,
                    'file_size_mb': round(file_size / (1024 * 1024), 2),
                    'aspect_ratio': round(image.width / image.height, 2)
                }
        except Exception as e:
            logger.error(f"è·å–å›¾åƒä¿¡æ¯å¤±è´¥ {image_path}: {e}")
            return {}
    
    def resize_image(self, image: Image.Image, size: Optional[Tuple[int, int]] = None, 
                     method: str = 'lanczos') -> Image.Image:
        """
        è°ƒæ•´å›¾åƒå°ºå¯¸
        
        Args:
            image: PIL Imageå¯¹è±¡
            size: ç›®æ ‡å°ºå¯¸ï¼Œé»˜è®¤ä½¿ç”¨self.target_size
            method: é‡é‡‡æ ·æ–¹æ³• ('lanczos', 'bilinear', 'bicubic')
            
        Returns:
            è°ƒæ•´å°ºå¯¸åçš„å›¾åƒ
        """
        if size is None:
            size = self.target_size
        
        # é€‰æ‹©é‡é‡‡æ ·æ–¹æ³•
        resample_methods = {
            'lanczos': Image.Resampling.LANCZOS,
            'bilinear': Image.Resampling.BILINEAR,
            'bicubic': Image.Resampling.BICUBIC
        }
        resample = resample_methods.get(method, Image.Resampling.LANCZOS)
        
        return image.resize(size, resample)
    
    def apply_augmentation(self, image: Image.Image, augment_type: str = 'light') -> Image.Image:
        """
        åº”ç”¨æ•°æ®å¢å¼º
        
        Args:
            image: PIL Imageå¯¹è±¡
            augment_type: å¢å¼ºç±»å‹ ('light', 'medium', 'heavy')
            
        Returns:
            å¢å¼ºåçš„å›¾åƒ
        """
        if augment_type == 'light':
            # è½»åº¦å¢å¼º
            if np.random.random() > 0.5:
                image = F.hflip(image)  # æ°´å¹³ç¿»è½¬
            if np.random.random() > 0.7:
                angle = np.random.uniform(-5, 5)
                image = F.rotate(image, angle)  # å°è§’åº¦æ—‹è½¬
                
        elif augment_type == 'medium':
            # ä¸­åº¦å¢å¼º
            if np.random.random() > 0.5:
                image = F.hflip(image)
            if np.random.random() > 0.6:
                angle = np.random.uniform(-10, 10)
                image = F.rotate(image, angle)
            if np.random.random() > 0.6:
                # äº®åº¦è°ƒæ•´
                enhancer = ImageEnhance.Brightness(image)
                factor = np.random.uniform(0.8, 1.2)
                image = enhancer.enhance(factor)
                
        elif augment_type == 'heavy':
            # é‡åº¦å¢å¼º
            if np.random.random() > 0.5:
                image = F.hflip(image)
            if np.random.random() > 0.5:
                angle = np.random.uniform(-15, 15)
                image = F.rotate(image, angle)
            if np.random.random() > 0.5:
                # äº®åº¦è°ƒæ•´
                enhancer = ImageEnhance.Brightness(image)
                factor = np.random.uniform(0.7, 1.3)
                image = enhancer.enhance(factor)
            if np.random.random() > 0.5:
                # å¯¹æ¯”åº¦è°ƒæ•´
                enhancer = ImageEnhance.Contrast(image)
                factor = np.random.uniform(0.8, 1.2)
                image = enhancer.enhance(factor)
            if np.random.random() > 0.3:
                # æ·»åŠ è½»å¾®æ¨¡ç³Š
                image = image.filter(ImageFilter.GaussianBlur(radius=0.5))
        
        return image
    
    def process_single_image(self, image_path: Union[str, Path], 
                           transform_type: str = 'val',
                           apply_augment: bool = False,
                           augment_type: str = 'light') -> Optional[torch.Tensor]:
        """
        å¤„ç†å•å¼ å›¾åƒ
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            transform_type: å˜æ¢ç±»å‹ ('train', 'val', 'visual')
            apply_augment: æ˜¯å¦åº”ç”¨æ•°æ®å¢å¼º
            augment_type: å¢å¼ºç±»å‹
            
        Returns:
            å¤„ç†åçš„tensorï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # åŠ è½½å›¾åƒ
            image = self.load_image(image_path)
            if image is None:
                return None
            
            # åº”ç”¨æ•°æ®å¢å¼º
            if apply_augment:
                image = self.apply_augmentation(image, augment_type)
            
            # é€‰æ‹©å˜æ¢
            if transform_type == 'train':
                transforms_fn = self.train_transforms
            elif transform_type == 'val':
                transforms_fn = self.val_transforms
            elif transform_type == 'visual':
                transforms_fn = self.visual_transforms
            else:
                transforms_fn = self.val_transforms
            
            # åº”ç”¨å˜æ¢
            tensor = transforms_fn(image)
            return tensor
            
        except Exception as e:
            logger.error(f"å¤„ç†å›¾åƒå¤±è´¥ {image_path}: {e}")
            return None
    
    def extract_image_features(self, image: Image.Image) -> Dict[str, float]:
        """
        æå–å›¾åƒç‰¹å¾ - ä¿®å¤è®¡ç®—é”™è¯¯
        
        Args:
            image: PIL Imageå¯¹è±¡
            
        Returns:
            å›¾åƒç‰¹å¾å­—å…¸
        """
        try:
            # ç¡®ä¿å›¾åƒæ˜¯PIL Imageå¯¹è±¡
            if not isinstance(image, Image.Image):
                logger.error(f"è¾“å…¥ä¸æ˜¯PIL Imageå¯¹è±¡: {type(image)}")
                return {}
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            img_array = np.array(image)
            
            # æ£€æŸ¥æ•°ç»„æ˜¯å¦æœ‰æ•ˆ
            if img_array is None or img_array.size == 0:
                logger.error("è½¬æ¢ä¸ºnumpyæ•°ç»„å¤±è´¥")
                return {}
            
            features = {}
            
            # åŸºæœ¬ç‰¹å¾
            features['width'] = float(image.width)
            features['height'] = float(image.height)
            features['aspect_ratio'] = float(image.width / image.height)
            features['total_pixels'] = float(image.width * image.height)
            
            # é¢œè‰²ç‰¹å¾
            if len(img_array.shape) == 3 and img_array.shape[2] == 3:  # å½©è‰²å›¾åƒ
                # RGBå‡å€¼
                features['mean_r'] = float(np.mean(img_array[:, :, 0]))
                features['mean_g'] = float(np.mean(img_array[:, :, 1]))
                features['mean_b'] = float(np.mean(img_array[:, :, 2]))
                
                # RGBæ ‡å‡†å·®
                features['std_r'] = float(np.std(img_array[:, :, 0]))
                features['std_g'] = float(np.std(img_array[:, :, 1]))
                features['std_b'] = float(np.std(img_array[:, :, 2]))
                
                # æ•´ä½“äº®åº¦
                features['brightness'] = float(np.mean(img_array))
                
                # å¯¹æ¯”åº¦ï¼ˆç®€å•ä¼°è®¡ï¼‰
                features['contrast'] = float(np.std(img_array))
                
                # è½¬æ¢ä¸ºç°åº¦ç”¨äºè¾¹ç¼˜æ£€æµ‹
                gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
                
            else:  # ç°åº¦å›¾åƒæˆ–å•é€šé“
                if len(img_array.shape) == 3:
                    img_array = img_array[:, :, 0]  # å–ç¬¬ä¸€ä¸ªé€šé“
                
                features['mean_gray'] = float(np.mean(img_array))
                features['std_gray'] = float(np.std(img_array))
                features['brightness'] = float(np.mean(img_array))
                features['contrast'] = float(np.std(img_array))
                
                gray = img_array.astype(np.uint8)
            
            # è¾¹ç¼˜å¯†åº¦
            try:
                edges = cv2.Canny(gray.astype(np.uint8), 50, 150)
                features['edge_density'] = float(np.sum(edges > 0) / edges.size)
            except:
                features['edge_density'] = 0.0
            
            return features
            
        except Exception as e:
            logger.error(f"æå–å›¾åƒç‰¹å¾å¤±è´¥: {e}")
            return {}
    
    def process_mr2_dataset(self, splits: List[str] = ['train', 'val', 'test'], 
                           save_features: bool = True) -> Dict[str, Dict]:
        """
        å¤„ç†MR2æ•°æ®é›†çš„æ‰€æœ‰å›¾åƒ
        
        Args:
            splits: è¦å¤„ç†çš„æ•°æ®åˆ’åˆ†
            save_features: æ˜¯å¦ä¿å­˜ç‰¹å¾åˆ°æ–‡ä»¶
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        print(f"ğŸ”„ å¼€å§‹å¤„ç†MR2æ•°æ®é›†å›¾åƒ...")
        
        results = {}
        
        for split in splits:
            print(f"\nğŸ“‚ å¤„ç† {split} æ•°æ®é›†")
            
            # åŠ è½½æ•°æ®é›†ä¿¡æ¯
            dataset_file = self.data_dir / f'dataset_items_{split}.json'
            if not dataset_file.exists():
                print(f"âš ï¸  æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_file}")
                continue
            
            with open(dataset_file, 'r', encoding='utf-8') as f:
                dataset_items = json.load(f)
            
            split_results = {
                'total_items': len(dataset_items),
                'processed_images': 0,
                'failed_images': 0,
                'image_info': {},
                'image_features': {}
            }
            
            # å¤„ç†æ¯ä¸ªæ•°æ®é¡¹
            for item_id, item_data in dataset_items.items():
                if 'image_path' not in item_data:
                    continue
                
                # æ„å»ºå®Œæ•´å›¾åƒè·¯å¾„
                image_path = self.data_dir / item_data['image_path']
                
                if not image_path.exists():
                    print(f"âš ï¸  å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                    split_results['failed_images'] += 1
                    continue
                
                try:
                    # è·å–å›¾åƒä¿¡æ¯
                    img_info = self.get_image_info(image_path)
                    split_results['image_info'][item_id] = img_info
                    
                    # åŠ è½½å›¾åƒå¹¶æå–ç‰¹å¾ - ä¿®å¤è¿™é‡Œçš„å¤„ç†é€»è¾‘
                    image = self.load_image(image_path)
                    if image is not None:
                        features = self.extract_image_features(image)
                        split_results['image_features'][item_id] = features
                        split_results['processed_images'] += 1
                        
                        if split_results['processed_images'] % 50 == 0:
                            print(f"  å·²å¤„ç† {split_results['processed_images']} å¼ å›¾åƒ")
                    else:
                        split_results['failed_images'] += 1
                        
                except Exception as e:
                    logger.error(f"å¤„ç†å›¾åƒå¤±è´¥ {image_path}: {e}")
                    split_results['failed_images'] += 1
            
            results[split] = split_results
            
            print(f"âœ… {split} æ•°æ®é›†å¤„ç†å®Œæˆ:")
            print(f"  æ€»æ•°: {split_results['total_items']}")
            print(f"  æˆåŠŸ: {split_results['processed_images']}")
            print(f"  å¤±è´¥: {split_results['failed_images']}")
            
            # ä¿å­˜ç‰¹å¾
            if save_features:
                self.save_image_features(split_results, split)
        
        return results
    
    def save_image_features(self, features_data: Dict, split: str):
        """
        ä¿å­˜å›¾åƒç‰¹å¾åˆ°æ–‡ä»¶
        
        Args:
            features_data: ç‰¹å¾æ•°æ®
            split: æ•°æ®åˆ’åˆ†åç§°
        """
        # åˆ›å»ºprocessedç›®å½•
        processed_dir = self.data_dir / 'processed'
        processed_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜å›¾åƒä¿¡æ¯
        info_file = processed_dir / f'{split}_image_info.json'
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(features_data['image_info'], f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜å›¾åƒç‰¹å¾
        features_file = processed_dir / f'{split}_image_features.json'
        with open(features_file, 'w', encoding='utf-8') as f:
            json.dump(features_data['image_features'], f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ å›¾åƒç‰¹å¾å·²ä¿å­˜åˆ°: {processed_dir}")
    
    def create_image_statistics(self, results: Dict) -> Dict[str, Any]:
        """
        åˆ›å»ºå›¾åƒç»Ÿè®¡ä¿¡æ¯
        
        Args:
            results: å¤„ç†ç»“æœ
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            'total_images': 0,
            'successful_images': 0,
            'failed_images': 0,
            'size_distribution': {
                'widths': [],
                'heights': [],
                'file_sizes': []
            },
            'format_distribution': {},
            'feature_statistics': {}
        }
        
        for split, split_data in results.items():
            stats['total_images'] += split_data['total_items']
            stats['successful_images'] += split_data['processed_images']
            stats['failed_images'] += split_data['failed_images']
            
            # æ”¶é›†å°ºå¯¸ä¿¡æ¯
            for item_id, img_info in split_data['image_info'].items():
                stats['size_distribution']['widths'].append(img_info.get('width', 0))
                stats['size_distribution']['heights'].append(img_info.get('height', 0))
                stats['size_distribution']['file_sizes'].append(img_info.get('file_size', 0))
                
                # æ ¼å¼åˆ†å¸ƒ
                img_format = img_info.get('format', 'Unknown')
                stats['format_distribution'][img_format] = stats['format_distribution'].get(img_format, 0) + 1
        
        # è®¡ç®—ç»Ÿè®¡å€¼
        if stats['size_distribution']['widths']:
            stats['avg_width'] = np.mean(stats['size_distribution']['widths'])
            stats['avg_height'] = np.mean(stats['size_distribution']['heights'])
            stats['avg_file_size'] = np.mean(stats['size_distribution']['file_sizes'])
        
        return stats
    
    def batch_process_images(self, image_paths: List[Union[str, Path]], 
                           transform_type: str = 'val',
                           batch_size: int = 32) -> List[torch.Tensor]:
        """
        æ‰¹é‡å¤„ç†å›¾åƒ
        
        Args:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            transform_type: å˜æ¢ç±»å‹
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            å¤„ç†åçš„tensoråˆ—è¡¨
        """
        processed_tensors = []
        
        for i in range(0, len(image_paths), batch_size):
            batch_paths = image_paths[i:i+batch_size]
            batch_tensors = []
            
            for path in batch_paths:
                tensor = self.process_single_image(path, transform_type)
                if tensor is not None:
                    batch_tensors.append(tensor)
            
            if batch_tensors:
                # å †å tensor
                batch_tensor = torch.stack(batch_tensors)
                processed_tensors.append(batch_tensor)
        
        return processed_tensors


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ–¼ï¸  æµ‹è¯•å›¾åƒå¤„ç†æ¨¡å—")
    
    # åˆ›å»ºå›¾åƒå¤„ç†å™¨
    processor = ImageProcessor(target_size=(224, 224))
    
    # æµ‹è¯•å•å¼ å›¾åƒå¤„ç†
    print("\nğŸ“ === å•å¼ å›¾åƒå¤„ç†æµ‹è¯• ===")
    
    # å‡è®¾çš„å›¾åƒè·¯å¾„ï¼ˆä½ éœ€è¦æ›¿æ¢ä¸ºå®é™…è·¯å¾„ï¼‰
    test_image_dir = Path("data/train/img")
    if test_image_dir.exists():
        # è·å–ç¬¬ä¸€å¼ å›¾åƒè¿›è¡Œæµ‹è¯•
        image_files = list(test_image_dir.glob("*.jpg")) + list(test_image_dir.glob("*.png"))
        if image_files:
            test_image = image_files[0]
            print(f"æµ‹è¯•å›¾åƒ: {test_image}")
            
            # è·å–å›¾åƒä¿¡æ¯
            img_info = processor.get_image_info(test_image)
            print(f"å›¾åƒä¿¡æ¯: {img_info}")
            
            # å¤„ç†å›¾åƒ
            tensor = processor.process_single_image(test_image, transform_type='val')
            if tensor is not None:
                print(f"å¤„ç†ç»“æœtensorå½¢çŠ¶: {tensor.shape}")
            
            # æå–ç‰¹å¾
            image = processor.load_image(test_image)
            if image is not None:
                features = processor.extract_image_features(image)
                print(f"å›¾åƒç‰¹å¾: {list(features.keys())}")
                print(f"äº®åº¦: {features.get('brightness', 0):.2f}")
                print(f"å¯¹æ¯”åº¦: {features.get('contrast', 0):.2f}")
        else:
            print("æœªæ‰¾åˆ°æµ‹è¯•å›¾åƒæ–‡ä»¶")
    else:
        print(f"å›¾åƒç›®å½•ä¸å­˜åœ¨: {test_image_dir}")
    
    # æµ‹è¯•æ•°æ®é›†å¤„ç†
    print("\nğŸ”„ === æ•°æ®é›†å¤„ç†æµ‹è¯• ===")
    print("å¼€å§‹å¤„ç†MR2æ•°æ®é›†ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
    
    try:
        # å¤„ç†æ•°æ®é›†
        results = processor.process_mr2_dataset(splits=['train'], save_features=True)
        
        # åˆ›å»ºç»Ÿè®¡ä¿¡æ¯
        stats = processor.create_image_statistics(results)
        
        print("\nğŸ“Š === å¤„ç†ç»Ÿè®¡ ===")
        print(f"æ€»å›¾åƒæ•°: {stats['total_images']}")
        print(f"æˆåŠŸå¤„ç†: {stats['successful_images']}")
        print(f"å¤„ç†å¤±è´¥: {stats['failed_images']}")
        
        if 'avg_width' in stats:
            print(f"å¹³å‡å®½åº¦: {stats['avg_width']:.1f}px")
            print(f"å¹³å‡é«˜åº¦: {stats['avg_height']:.1f}px")
            print(f"å¹³å‡æ–‡ä»¶å¤§å°: {stats['avg_file_size']/1024:.1f}KB")
        
        print(f"æ ¼å¼åˆ†å¸ƒ: {stats['format_distribution']}")
        
    except Exception as e:
        print(f"æ•°æ®é›†å¤„ç†å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿æ•°æ®ç›®å½•å’Œæ–‡ä»¶å­˜åœ¨")
    
    print("\nâœ… å›¾åƒå¤„ç†æ¨¡å—æµ‹è¯•å®Œæˆ")