#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: ipanzhzh
# preprocessing/text_processing.py

"""
MR2æ–‡æœ¬é¢„å¤„ç†æ¨¡å—
ä¸“é—¨å¤„ç†ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬çš„åˆ†è¯ã€æ¸…æ´—ã€æ ‡å‡†åŒ–ç­‰åŠŸèƒ½
æ”¯æŒè°£è¨€æ£€æµ‹ä»»åŠ¡çš„ç‰¹æ®Šéœ€æ±‚
"""

import re
import string
import emoji
from typing import List, Dict, Optional, Tuple, Union
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# ä¸­æ–‡åˆ†è¯
try:
    import jieba
    import jieba.posseg as pseg
    HAS_JIEBA = True
except ImportError:
    print("âš ï¸  jiebaæœªå®‰è£…ï¼Œä¸­æ–‡åˆ†è¯åŠŸèƒ½ä¸å¯ç”¨")
    HAS_JIEBA = False

# è‹±æ–‡å¤„ç†
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.stem import PorterStemmer, WordNetLemmatizer
    from nltk.tokenize import word_tokenize, sent_tokenize
    HAS_NLTK = True
except ImportError:
    print("âš ï¸  nltkæœªå®‰è£…ï¼Œè‹±æ–‡é«˜çº§å¤„ç†åŠŸèƒ½ä¸å¯ç”¨")
    HAS_NLTK = False

# é…ç½®ç®¡ç†
try:
    from utils.config_manager import get_data_config
    USE_CONFIG = True
except ImportError:
    USE_CONFIG = False

import logging
logger = logging.getLogger(__name__)


class TextProcessor:
    """
    æ–‡æœ¬é¢„å¤„ç†å™¨
    æ”¯æŒä¸­è‹±æ–‡æ··åˆæ–‡æœ¬çš„å…¨é¢å¤„ç†
    """
    
    def __init__(self, language: str = 'mixed'):
        """
        åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨
        
        Args:
            language: è¯­è¨€ç±»å‹ ('chinese', 'english', 'mixed')
        """
        self.language = language
        self.setup_processors()
        self.load_stopwords()
        
        # åŠ è½½é…ç½®
        if USE_CONFIG:
            try:
                config = get_data_config()
                self.processing_config = config.get('processing', {}).get('text', {})
            except:
                self.processing_config = {}
        else:
            self.processing_config = {}
        
        # è®¾ç½®å¤„ç†å‚æ•°
        self.max_length = self.processing_config.get('max_length', 512)
        self.remove_urls = self.processing_config.get('remove_urls', True)
        self.remove_mentions = self.processing_config.get('remove_mentions', True)
        self.remove_hashtags = self.processing_config.get('remove_hashtags', False)
        self.normalize_whitespace = self.processing_config.get('normalize_whitespace', True)
    
    def setup_processors(self):
        """è®¾ç½®å„ç§å¤„ç†å™¨"""
        # è‹±æ–‡å¤„ç†å™¨
        if HAS_NLTK:
            self.stemmer = PorterStemmer()
            self.lemmatizer = WordNetLemmatizer()
        
        # ä¸­æ–‡åœç”¨è¯
        self.chinese_stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'æœ‰', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ',
            'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'è¿™', 'é‚£', 'è¿™ä¸ª', 'é‚£ä¸ª', 'ä¸Š', 'ä¸‹',
            'ä¸­', 'å¤§', 'å°', 'å¤š', 'å°‘', 'å¥½', 'å', 'å¯¹', 'é”™', 'æ²¡', 'ä¸',
            'å°±', 'éƒ½', 'ä¼š', 'è¯´', 'æ¥', 'å»', 'ä»', 'åˆ°', 'æŠŠ', 'è¢«', 'ç»™',
            'å‘', 'å¾€', 'é‡Œ', 'å¤–', 'å‰', 'å', 'å·¦', 'å³', 'ä¸œ', 'è¥¿', 'å—', 'åŒ—'
        }
        
        # è‹±æ–‡åœç”¨è¯
        if HAS_NLTK:
            try:
                self.english_stopwords = set(stopwords.words('english'))
            except LookupError:
                print("âš ï¸  NLTKåœç”¨è¯æœªä¸‹è½½ï¼Œä½¿ç”¨é»˜è®¤åœç”¨è¯")
                self.english_stopwords = {
                    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',
                    'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him',
                    'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its',
                    'itself', 'they', 'them', 'their', 'theirs', 'themselves',
                    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',
                    'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
                    'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',
                    'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as',
                    'until', 'while', 'of', 'at', 'by', 'for', 'with', 'through',
                    'during', 'before', 'after', 'above', 'below', 'up', 'down',
                    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',
                    'then', 'once'
                }
        else:
            self.english_stopwords = set()
    
    def load_stopwords(self):
        """åŠ è½½åœç”¨è¯"""
        pass  # å·²åœ¨setup_processorsä¸­å¤„ç†
    
    def detect_language(self, text: str) -> str:
        """
        æ£€æµ‹æ–‡æœ¬è¯­è¨€
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            è¯­è¨€ç±»å‹ ('chinese', 'english', 'mixed')
        """
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        # ç»Ÿè®¡è‹±æ–‡å­—ç¬¦
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        
        total_chars = chinese_chars + english_chars
        if total_chars == 0:
            return 'unknown'
        
        chinese_ratio = chinese_chars / total_chars
        english_ratio = english_chars / total_chars
        
        if chinese_ratio > 0.7:
            return 'chinese'
        elif english_ratio > 0.7:
            return 'english'
        else:
            return 'mixed'
    
    def clean_text(self, text: str) -> str:
        """
        æ–‡æœ¬æ¸…æ´—
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æ¸…æ´—åçš„æ–‡æœ¬
        """
        if not text or not isinstance(text, str):
            return ""
        
        # 1. ç§»é™¤URL
        if self.remove_urls:
            text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
            text = re.sub(r'www\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # 2. ç§»é™¤@æåŠ
        if self.remove_mentions:
            text = re.sub(r'@[a-zA-Z0-9_\u4e00-\u9fff]+', '', text)
        
        # 3. ç§»é™¤#è¯é¢˜æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
        if self.remove_hashtags:
            text = re.sub(r'#[a-zA-Z0-9_\u4e00-\u9fff]+', '', text)
        
        # 4. ç§»é™¤emojiï¼ˆè½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼‰
        text = emoji.demojize(text, language='en')
        
        # 5. ç§»é™¤å¤šä½™çš„æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[!]{2,}', '!', text)
        text = re.sub(r'[?]{2,}', '?', text)
        text = re.sub(r'[.]{3,}', '...', text)
        
        # 6. ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        
        # 7. æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        if self.normalize_whitespace:
            text = re.sub(r'\s+', ' ', text)
            text = text.strip()
        
        return text
    
    def tokenize_chinese(self, text: str) -> List[str]:
        """
        ä¸­æ–‡åˆ†è¯
        
        Args:
            text: ä¸­æ–‡æ–‡æœ¬
            
        Returns:
            åˆ†è¯ç»“æœåˆ—è¡¨
        """
        if not HAS_JIEBA:
            # ç®€å•çš„å­—ç¬¦çº§åˆ†å‰²
            return list(text.replace(' ', ''))
        
        # ä½¿ç”¨jiebaåˆ†è¯
        tokens = list(jieba.cut(text, cut_all=False))
        
        # è¿‡æ»¤åœç”¨è¯å’Œæ— æ„ä¹‰tokens
        filtered_tokens = []
        for token in tokens:
            token = token.strip()
            if (len(token) > 1 and 
                token not in self.chinese_stopwords and
                not re.match(r'^[\s\d\W]+$', token)):
                filtered_tokens.append(token)
        
        return filtered_tokens
    
    def tokenize_english(self, text: str) -> List[str]:
        """
        è‹±æ–‡åˆ†è¯
        
        Args:
            text: è‹±æ–‡æ–‡æœ¬
            
        Returns:
            åˆ†è¯ç»“æœåˆ—è¡¨
        """
        if HAS_NLTK:
            try:
                tokens = word_tokenize(text.lower())
            except LookupError:
                # å¦‚æœnltkæ•°æ®æœªä¸‹è½½ï¼Œä½¿ç”¨ç®€å•åˆ†å‰²
                tokens = text.lower().split()
        else:
            tokens = text.lower().split()
        
        # è¿‡æ»¤åœç”¨è¯å’Œæ ‡ç‚¹ç¬¦å·
        filtered_tokens = []
        for token in tokens:
            if (token not in self.english_stopwords and
                token not in string.punctuation and
                len(token) > 2 and
                token.isalpha()):
                filtered_tokens.append(token)
        
        return filtered_tokens
    
    def tokenize_mixed(self, text: str) -> List[str]:
        """
        ä¸­è‹±æ–‡æ··åˆåˆ†è¯
        
        Args:
            text: æ··åˆæ–‡æœ¬
            
        Returns:
            åˆ†è¯ç»“æœåˆ—è¡¨
        """
        tokens = []
        
        # å°†æ–‡æœ¬æŒ‰ä¸­è‹±æ–‡åˆ†å‰²
        parts = re.split(r'([\u4e00-\u9fff]+)', text)
        
        for part in parts:
            part = part.strip()
            if not part:
                continue
                
            # åˆ¤æ–­æ˜¯ä¸­æ–‡è¿˜æ˜¯è‹±æ–‡
            if re.search(r'[\u4e00-\u9fff]', part):
                # ä¸­æ–‡éƒ¨åˆ†
                chinese_tokens = self.tokenize_chinese(part)
                tokens.extend(chinese_tokens)
            else:
                # è‹±æ–‡éƒ¨åˆ†
                english_tokens = self.tokenize_english(part)
                tokens.extend(english_tokens)
        
        return tokens
    
    def tokenize(self, text: str, language: Optional[str] = None) -> List[str]:
        """
        ç»Ÿä¸€åˆ†è¯æ¥å£
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            language: æŒ‡å®šè¯­è¨€ï¼ŒNoneåˆ™è‡ªåŠ¨æ£€æµ‹
            
        Returns:
            åˆ†è¯ç»“æœåˆ—è¡¨
        """
        if not text:
            return []
        
        # æ¸…æ´—æ–‡æœ¬
        text = self.clean_text(text)
        
        if not text:
            return []
        
        # ç¡®å®šè¯­è¨€
        if language is None:
            language = self.detect_language(text)
        
        # æ ¹æ®è¯­è¨€é€‰æ‹©åˆ†è¯æ–¹æ³•
        if language == 'chinese':
            return self.tokenize_chinese(text)
        elif language == 'english':
            return self.tokenize_english(text)
        else:  # mixed or unknown
            return self.tokenize_mixed(text)
    
    def extract_features(self, text: str) -> Dict[str, Union[int, float, List[str]]]:
        """
        æå–æ–‡æœ¬ç‰¹å¾
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            ç‰¹å¾å­—å…¸
        """
        # åŸºç¡€ç‰¹å¾
        features = {
            'text_length': len(text),
            'word_count': len(text.split()),
            'char_count': len(text),
            'language': self.detect_language(text)
        }
        
        # åˆ†è¯
        tokens = self.tokenize(text)
        features['tokens'] = tokens
        features['token_count'] = len(tokens)
        
        # æ ‡ç‚¹ç¬¦å·ç»Ÿè®¡
        features['exclamation_count'] = text.count('!')
        features['question_count'] = text.count('?')
        features['period_count'] = text.count('.')
        features['comma_count'] = text.count(',')
        
        # å¤§å†™å­—æ¯æ¯”ä¾‹ï¼ˆè‹±æ–‡ï¼‰
        if re.search(r'[a-zA-Z]', text):
            english_chars = re.findall(r'[a-zA-Z]', text)
            if english_chars:
                uppercase_ratio = sum(1 for c in english_chars if c.isupper()) / len(english_chars)
                features['uppercase_ratio'] = uppercase_ratio
            else:
                features['uppercase_ratio'] = 0.0
        else:
            features['uppercase_ratio'] = 0.0
        
        # æ•°å­—ç»Ÿè®¡
        features['digit_count'] = len(re.findall(r'\d', text))
        
        # URLå’ŒæåŠç»Ÿè®¡
        features['url_count'] = len(re.findall(r'http[s]?://\S+', text))
        features['mention_count'] = len(re.findall(r'@\w+', text))
        features['hashtag_count'] = len(re.findall(r'#\w+', text))
        
        # emojiç»Ÿè®¡
        features['emoji_count'] = len(re.findall(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]', text))
        
        return features
    
    def preprocess_batch(self, texts: List[str]) -> List[Dict[str, any]]:
        """
        æ‰¹é‡é¢„å¤„ç†æ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            é¢„å¤„ç†ç»“æœåˆ—è¡¨
        """
        results = []
        for text in texts:
            result = {
                'original_text': text,
                'cleaned_text': self.clean_text(text),
                'tokens': self.tokenize(text),
                'features': self.extract_features(text)
            }
            results.append(result)
        return results


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ”„ æµ‹è¯•æ–‡æœ¬å¤„ç†æ¨¡å—")
    
    # åˆ›å»ºæ–‡æœ¬å¤„ç†å™¨
    processor = TextProcessor(language='mixed')
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ This is a test text!",
        "ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œé€‚åˆå‡ºé—¨æ¸¸ç©ã€‚",
        "Breaking news: AI technology advances rapidly!",
        "æ··åˆè¯­è¨€æ–‡æœ¬ with English words and ä¸­æ–‡å­—ç¬¦",
        "åŒ…å«URLçš„æ–‡æœ¬ https://example.com å’Œ@usernameæåŠ",
        "å¸¦æœ‰emojiçš„æ–‡æœ¬ ğŸ˜Š å’Œ #hashtag æ ‡ç­¾"
    ]
    
    print("\nğŸ“ === æ–‡æœ¬å¤„ç†æµ‹è¯• ===")
    for i, text in enumerate(test_texts, 1):
        print(f"\næµ‹è¯• {i}: {text}")
        
        # è¯­è¨€æ£€æµ‹
        language = processor.detect_language(text)
        print(f"  è¯­è¨€: {language}")
        
        # æ–‡æœ¬æ¸…æ´—
        cleaned = processor.clean_text(text)
        print(f"  æ¸…æ´—å: {cleaned}")
        
        # åˆ†è¯
        tokens = processor.tokenize(text)
        print(f"  åˆ†è¯ç»“æœ: {tokens}")
        
        # ç‰¹å¾æå–
        features = processor.extract_features(text)
        print(f"  ç‰¹å¾: é•¿åº¦={features['text_length']}, è¯æ•°={features['token_count']}, è¯­è¨€={features['language']}")
    
    print("\nğŸ”„ æµ‹è¯•æ‰¹é‡å¤„ç†")
    batch_results = processor.preprocess_batch(test_texts[:3])
    for i, result in enumerate(batch_results):
        print(f"  æ‰¹é‡ç»“æœ {i+1}: {len(result['tokens'])} ä¸ªtoken")
    
    print("\nâœ… æ–‡æœ¬å¤„ç†æ¨¡å—æµ‹è¯•å®Œæˆ")